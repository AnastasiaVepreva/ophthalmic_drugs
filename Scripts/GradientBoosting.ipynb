{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exactmw</th>\n",
       "      <th>amw</th>\n",
       "      <th>lipinskiHBA</th>\n",
       "      <th>lipinskiHBD</th>\n",
       "      <th>NumRotatableBonds</th>\n",
       "      <th>NumHBD</th>\n",
       "      <th>NumHBA</th>\n",
       "      <th>NumHeavyAtoms</th>\n",
       "      <th>NumAtoms</th>\n",
       "      <th>NumHeteroatoms</th>\n",
       "      <th>...</th>\n",
       "      <th>chi1n</th>\n",
       "      <th>chi2n</th>\n",
       "      <th>chi3n</th>\n",
       "      <th>chi4n</th>\n",
       "      <th>hallKierAlpha</th>\n",
       "      <th>kappa1</th>\n",
       "      <th>kappa2</th>\n",
       "      <th>kappa3</th>\n",
       "      <th>Phi</th>\n",
       "      <th>logPerm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>376.204988</td>\n",
       "      <td>376.468</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.972554</td>\n",
       "      <td>8.703150</td>\n",
       "      <td>8.703150</td>\n",
       "      <td>7.183176</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>19.006790</td>\n",
       "      <td>5.383489</td>\n",
       "      <td>1.997481</td>\n",
       "      <td>3.789735</td>\n",
       "      <td>5.135798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>244.089958</td>\n",
       "      <td>244.265</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.670131</td>\n",
       "      <td>2.948361</td>\n",
       "      <td>2.948361</td>\n",
       "      <td>1.795098</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>12.300954</td>\n",
       "      <td>5.001423</td>\n",
       "      <td>2.399040</td>\n",
       "      <td>3.417904</td>\n",
       "      <td>5.347108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243.105942</td>\n",
       "      <td>243.281</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.735200</td>\n",
       "      <td>3.004712</td>\n",
       "      <td>3.004712</td>\n",
       "      <td>1.816787</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>12.300954</td>\n",
       "      <td>5.001423</td>\n",
       "      <td>2.399040</td>\n",
       "      <td>3.417904</td>\n",
       "      <td>5.393628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92.047344</td>\n",
       "      <td>92.094</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.707151</td>\n",
       "      <td>0.421498</td>\n",
       "      <td>0.421498</td>\n",
       "      <td>0.057735</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>3.084918</td>\n",
       "      <td>2.137732</td>\n",
       "      <td>3.023220</td>\n",
       "      <td>3.806662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>362.209324</td>\n",
       "      <td>362.466</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.957047</td>\n",
       "      <td>8.653267</td>\n",
       "      <td>8.653267</td>\n",
       "      <td>7.281020</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>18.329215</td>\n",
       "      <td>5.727623</td>\n",
       "      <td>2.138229</td>\n",
       "      <td>4.037801</td>\n",
       "      <td>4.442651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>255.089543</td>\n",
       "      <td>255.273</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.202262</td>\n",
       "      <td>3.579548</td>\n",
       "      <td>3.579548</td>\n",
       "      <td>2.628901</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>11.714803</td>\n",
       "      <td>4.387760</td>\n",
       "      <td>1.851818</td>\n",
       "      <td>2.705355</td>\n",
       "      <td>1.249902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>420.221306</td>\n",
       "      <td>420.528</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.114456</td>\n",
       "      <td>7.749433</td>\n",
       "      <td>7.749433</td>\n",
       "      <td>5.796964</td>\n",
       "      <td>-2.71</td>\n",
       "      <td>21.519337</td>\n",
       "      <td>8.434242</td>\n",
       "      <td>3.742717</td>\n",
       "      <td>5.854816</td>\n",
       "      <td>1.406097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>324.183778</td>\n",
       "      <td>324.424</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.682990</td>\n",
       "      <td>5.819484</td>\n",
       "      <td>5.819484</td>\n",
       "      <td>4.444306</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>15.608011</td>\n",
       "      <td>6.187608</td>\n",
       "      <td>2.502406</td>\n",
       "      <td>4.024010</td>\n",
       "      <td>2.161022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>253.018894</td>\n",
       "      <td>253.718</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.805416</td>\n",
       "      <td>2.294635</td>\n",
       "      <td>2.294635</td>\n",
       "      <td>1.545781</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>9.799739</td>\n",
       "      <td>3.876645</td>\n",
       "      <td>1.595333</td>\n",
       "      <td>2.374382</td>\n",
       "      <td>3.850998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>349.115045</td>\n",
       "      <td>349.316</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.372743</td>\n",
       "      <td>4.089727</td>\n",
       "      <td>4.089727</td>\n",
       "      <td>2.724196</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>17.027366</td>\n",
       "      <td>6.595177</td>\n",
       "      <td>3.224982</td>\n",
       "      <td>4.491940</td>\n",
       "      <td>3.455686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        exactmw      amw  lipinskiHBA  lipinskiHBD  NumRotatableBonds  NumHBD  \\\n",
       "0    376.204988  376.468          4.0          2.0                1.0     2.0   \n",
       "1    244.089958  244.265          2.0          1.0                3.0     1.0   \n",
       "2    243.105942  243.281          2.0          2.0                3.0     1.0   \n",
       "3     92.047344   92.094          3.0          3.0                2.0     3.0   \n",
       "4    362.209324  362.466          5.0          3.0                2.0     3.0   \n",
       "..          ...      ...          ...          ...                ...     ...   \n",
       "115  255.089543  255.273          4.0          1.0                3.0     1.0   \n",
       "116  420.221306  420.528          4.0          1.0                4.0     1.0   \n",
       "117  324.183778  324.424          4.0          1.0                4.0     1.0   \n",
       "118  253.018894  253.718          5.0          2.0                1.0     2.0   \n",
       "119  349.115045  349.316          6.0          1.0                5.0     1.0   \n",
       "\n",
       "     NumHBA  NumHeavyAtoms  NumAtoms  NumHeteroatoms  ...      chi1n  \\\n",
       "0       4.0           27.0      56.0             5.0  ...   9.972554   \n",
       "1       1.0           18.0      31.0             3.0  ...   5.670131   \n",
       "2       1.0           18.0      32.0             3.0  ...   5.735200   \n",
       "3       3.0            6.0      14.0             3.0  ...   1.707151   \n",
       "4       5.0           26.0      56.0             5.0  ...   9.957047   \n",
       "..      ...            ...       ...             ...  ...        ...   \n",
       "115     3.0           19.0      32.0             4.0  ...   6.202262   \n",
       "116     3.0           31.0      60.0             5.0  ...  11.114456   \n",
       "117     4.0           24.0      48.0             4.0  ...   8.682990   \n",
       "118     6.0           16.0      24.0             7.0  ...   4.805416   \n",
       "119     6.0           25.0      39.0             9.0  ...   7.372743   \n",
       "\n",
       "        chi2n     chi3n     chi4n  hallKierAlpha     kappa1    kappa2  \\\n",
       "0    8.703150  8.703150  7.183176          -1.33  19.006790  5.383489   \n",
       "1    2.948361  2.948361  1.795098          -2.16  12.300954  5.001423   \n",
       "2    3.004712  3.004712  1.816787          -2.16  12.300954  5.001423   \n",
       "3    0.421498  0.421498  0.057735          -0.12   5.880000  3.084918   \n",
       "4    8.653267  8.653267  7.281020          -1.04  18.329215  5.727623   \n",
       "..        ...       ...       ...            ...        ...       ...   \n",
       "115  3.579548  3.579548  2.628901          -2.36  11.714803  4.387760   \n",
       "116  7.749433  7.749433  5.796964          -2.71  21.519337  8.434242   \n",
       "117  5.819484  5.819484  4.444306          -1.91  15.608011  6.187608   \n",
       "118  2.294635  2.294635  1.545781          -1.40   9.799739  3.876645   \n",
       "119  4.089727  4.089727  2.724196          -2.81  17.027366  6.595177   \n",
       "\n",
       "       kappa3       Phi   logPerm  \n",
       "0    1.997481  3.789735  5.135798  \n",
       "1    2.399040  3.417904  5.347108  \n",
       "2    2.399040  3.417904  5.393628  \n",
       "3    2.137732  3.023220  3.806662  \n",
       "4    2.138229  4.037801  4.442651  \n",
       "..        ...       ...       ...  \n",
       "115  1.851818  2.705355  1.249902  \n",
       "116  3.742717  5.854816  1.406097  \n",
       "117  2.502406  4.024010  2.161022  \n",
       "118  1.595333  2.374382  3.850998  \n",
       "119  3.224982  4.491940  3.455686  \n",
       "\n",
       "[120 rows x 44 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\Anastasia\\Desktop\\ocular drugs\\Data\\descriptors_120raws.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test dataset by choosing of 30 molecules and delete features with low importance\n",
    "df2 = df.loc[[16, 54, 87, 119, 15, 25, 30, 76, 88, 110, 0, 27, 72, 91, 73, 8, 23, 105, 92, 37, 51, 36, 13, 115, 104, 82, 5, 85, 66, 7]]\n",
    "#df3 = df2.iloc [:, [3,5,16,26,38, 43]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('corneal_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train dataset and delete features with low importance\n",
    "df4 = df.drop (index=[16, 54, 87, 119, 15, 25, 30, 76, 88, 110, 0, 27, 72, 91, 73, 8, 23, 105, 92, 37, 51, 36, 13, 115, 104, 82, 5, 85, 66, 7])\n",
    "#df4 = df4.iloc [:, [3,5,16,26,38, 43]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv('corneal_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df2[df2.columns[:-1]]\n",
    "y_test = df2[df2.columns[-1]]\n",
    "X_train = df4[df4.columns[:-1]]\n",
    "y_train = df4[df4.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(random_state=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(random_state=50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = GradientBoostingRegressor(random_state=50)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7450650563930239"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7450650563930239"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anastasia\\miniconda3\\envs\\drops\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-32 {color: black;}#sk-container-id-32 pre{padding: 0;}#sk-container-id-32 div.sk-toggleable {background-color: white;}#sk-container-id-32 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-32 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-32 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-32 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-32 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-32 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-32 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-32 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-32 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-32 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-32 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-32 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-32 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-32 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-32 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-32 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-32 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-32 div.sk-item {position: relative;z-index: 1;}#sk-container-id-32 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-32 div.sk-item::before, #sk-container-id-32 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-32 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-32 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-32 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-32 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-32 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-32 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-32 div.sk-label-container {text-align: center;}#sk-container-id-32 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-32 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-32\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, alpha=1, hidden_layer_sizes=(10,),\n",
       "             random_state=7, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" checked><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, alpha=1, hidden_layer_sizes=(10,),\n",
       "             random_state=7, solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(activation='logistic', alpha=1, hidden_layer_sizes=(10,),\n",
       "             random_state=7, solver='lbfgs')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(solver='lbfgs', alpha=1, hidden_layer_sizes=(10,), random_state=7, activation='logistic')\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4557325780901441"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5089262029708954"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[0;32m      3\u001b[0m legend \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlightblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(y_test, regr\u001b[38;5;241m.\u001b[39mpredict, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarkblue\u001b[39m\u001b[38;5;124m'\u001b[39m, s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(legend, loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\miniconda3\\envs\\drops\\lib\\site-packages\\matplotlib\\pyplot.py:3687\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   3668\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mscatter)\n\u001b[0;32m   3669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[0;32m   3670\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3685\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3686\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PathCollection:\n\u001b[1;32m-> 3687\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[0;32m   3688\u001b[0m         x,\n\u001b[0;32m   3689\u001b[0m         y,\n\u001b[0;32m   3690\u001b[0m         s\u001b[38;5;241m=\u001b[39ms,\n\u001b[0;32m   3691\u001b[0m         c\u001b[38;5;241m=\u001b[39mc,\n\u001b[0;32m   3692\u001b[0m         marker\u001b[38;5;241m=\u001b[39mmarker,\n\u001b[0;32m   3693\u001b[0m         cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[0;32m   3694\u001b[0m         norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   3695\u001b[0m         vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[0;32m   3696\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax,\n\u001b[0;32m   3697\u001b[0m         alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[0;32m   3698\u001b[0m         linewidths\u001b[38;5;241m=\u001b[39mlinewidths,\n\u001b[0;32m   3699\u001b[0m         edgecolors\u001b[38;5;241m=\u001b[39medgecolors,\n\u001b[0;32m   3700\u001b[0m         plotnonfinite\u001b[38;5;241m=\u001b[39mplotnonfinite,\n\u001b[0;32m   3701\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3702\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3703\u001b[0m     )\n\u001b[0;32m   3704\u001b[0m     sci(__ret)\n\u001b[0;32m   3705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\miniconda3\\envs\\drops\\lib\\site-packages\\matplotlib\\__init__.py:1478\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1478\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1480\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1481\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1482\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\Anastasia\\miniconda3\\envs\\drops\\lib\\site-packages\\matplotlib\\axes\\_axes.py:4652\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4650\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[0;32m   4651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m-> 4652\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4655\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal.classic_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m   4656\u001b[0m          mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "legend = ['Train', 'Test']\n",
    "\n",
    "plt.scatter(y_train, regr.predict, c = 'lightblue', s = 20)\n",
    "plt.scatter(y_test, regr.predict, c = 'darkblue', s = 20)\n",
    "plt.legend(legend, loc = 'best')\n",
    "plt.xlabel(\"Y True\")\n",
    "plt.ylabel(\"Y Predicted\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(random_state=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(random_state=50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = GradientBoostingRegressor(random_state=50, learning_rate=0.1, loss='squared_error', max_depth=3, n_estimators=100, subsample=1.0)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9700341932735869"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7450650563930239"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;, &#x27;huber&#x27;,\n",
       "                                  &#x27;quantile&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [3, 7, 9],\n",
       "                         &#x27;n_estimators&#x27;: [10, 50, 100, 500],\n",
       "                         &#x27;random_state&#x27;: [50], &#x27;subsample&#x27;: [0.5, 0.7, 1.0]},\n",
       "             scoring=&#x27;r2&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;, &#x27;huber&#x27;,\n",
       "                                  &#x27;quantile&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [3, 7, 9],\n",
       "                         &#x27;n_estimators&#x27;: [10, 50, 100, 500],\n",
       "                         &#x27;random_state&#x27;: [50], &#x27;subsample&#x27;: [0.5, 0.7, 1.0]},\n",
       "             scoring=&#x27;r2&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
       "                         'loss': ['squared_error', 'absolute_error', 'huber',\n",
       "                                  'quantile'],\n",
       "                         'max_depth': [3, 7, 9],\n",
       "                         'n_estimators': [10, 50, 100, 500],\n",
       "                         'random_state': [50], 'subsample': [0.5, 0.7, 1.0]},\n",
       "             scoring='r2', verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'n_estimators':[10, 50, 100, 500], 'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'], 'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1.0], 'subsample':[0.5, 0.7, 1.0], 'max_depth': [3, 7, 9], 'random_state':[50]}\n",
    "gradboost = GradientBoostingRegressor()\n",
    "regr = GridSearchCV(gradboost, parameters, scoring='r2', verbose=1)\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00639973, 0.00520129, 0.00539813, 0.01960006, 0.02180381,\n",
       "        0.01980047, 0.0377327 , 0.03960304, 0.03913078, 0.18205585,\n",
       "        0.19336705, 0.19126115, 0.00620022, 0.00679989, 0.00759883,\n",
       "        0.02540002, 0.02820091, 0.03040013, 0.04860148, 0.05480165,\n",
       "        0.05920014, 0.23878012, 0.27314162, 0.29273658, 0.00619831,\n",
       "        0.00700059, 0.0079957 , 0.02560143, 0.02939744, 0.03330755,\n",
       "        0.05037451, 0.05778732, 0.06544514, 0.24266295, 0.28125958,\n",
       "        0.31803389, 0.01254411, 0.01260071, 0.01319866, 0.05053287,\n",
       "        0.05459962, 0.06164889, 0.10244412, 0.11300197, 0.11667938,\n",
       "        0.49213071, 0.53506465, 0.58906951, 0.01552854, 0.01800017,\n",
       "        0.02120171, 0.06753559, 0.0829978 , 0.09939637, 0.13413515,\n",
       "        0.1652041 , 0.22040167, 0.73054633, 0.83335357, 1.0546989 ,\n",
       "        0.0158    , 0.01919966, 0.02279921, 0.06960011, 0.08693976,\n",
       "        0.10772552, 0.13703465, 0.17066703, 0.21602521, 0.67978396,\n",
       "        0.85385785, 1.11920185, 0.01698861, 0.01659989, 0.01613126,\n",
       "        0.07359991, 0.07639999, 0.07620015, 0.1455996 , 0.14967194,\n",
       "        0.14839988, 0.71821008, 0.73852906, 0.72558222, 0.0409256 ,\n",
       "        0.0486022 , 0.05360088, 0.19572635, 0.22993274, 0.26119261,\n",
       "        0.38765736, 0.46541448, 0.51799111, 1.91725097, 2.3339519 ,\n",
       "        2.61244359, 0.04459591, 0.05619574, 0.06806026, 0.21455851,\n",
       "        0.27269387, 0.34473996, 0.43309903, 0.55038109, 0.67572789,\n",
       "        2.15400009, 2.72147422, 3.34305282, 0.01080031, 0.01140027,\n",
       "        0.01219993, 0.04707685, 0.05099916, 0.05260134, 0.09160028,\n",
       "        0.1001668 , 0.10393457, 0.45073624, 0.5007514 , 0.5145822 ,\n",
       "        0.01160154, 0.01400251, 0.01840253, 0.05340066, 0.06619849,\n",
       "        0.08487487, 0.10940099, 0.13334308, 0.16912823, 0.53221488,\n",
       "        0.66590214, 0.83933377, 0.01199918, 0.01440058, 0.01839967,\n",
       "        0.05360045, 0.06740084, 0.08796268, 0.10860186, 0.13419666,\n",
       "        0.17337384, 0.53829436, 0.67663941, 0.84811463, 0.0049993 ,\n",
       "        0.00540609, 0.00500016, 0.01939621, 0.02040057, 0.02020011,\n",
       "        0.03742633, 0.03873634, 0.03859968, 0.18199987, 0.18959069,\n",
       "        0.18872762, 0.00599456, 0.00679898, 0.00720053, 0.02419691,\n",
       "        0.02800221, 0.03020177, 0.0475987 , 0.05439849, 0.05919819,\n",
       "        0.23365865, 0.26786013, 0.28893828, 0.00640001, 0.00719972,\n",
       "        0.00779943, 0.02559991, 0.02999988, 0.03304524, 0.04979978,\n",
       "        0.05823746, 0.06519966, 0.24504623, 0.28438635, 0.32202506,\n",
       "        0.01179986, 0.01220102, 0.01339808, 0.05120173, 0.05542183,\n",
       "        0.06204014, 0.09999814, 0.10879941, 0.12113762, 0.50203714,\n",
       "        0.54869647, 0.59517832, 0.01560192, 0.01859856, 0.02179866,\n",
       "        0.0691968 , 0.08500423, 0.10619993, 0.13932772, 0.17180004,\n",
       "        0.21873155, 0.71856885, 0.89879165, 1.13971748, 0.01559935,\n",
       "        0.0185998 , 0.02259917, 0.07060022, 0.08813586, 0.11452875,\n",
       "        0.1413332 , 0.1791995 , 0.23800187, 0.72866507, 0.94248762,\n",
       "        1.24658732, 0.01600056, 0.01606889, 0.01659775, 0.07280121,\n",
       "        0.07520061, 0.07394471, 0.14359851, 0.14913335, 0.14713802,\n",
       "        0.71157508, 0.7309792 , 0.7250463 , 0.04072299, 0.04899955,\n",
       "        0.05599933, 0.19625769, 0.23006392, 0.26139474, 0.38433285,\n",
       "        0.46133022, 0.52794309, 1.91858153, 2.32810798, 2.64526191,\n",
       "        0.04573445, 0.05819693, 0.06872959, 0.21718688, 0.27526035,\n",
       "        0.33846831, 0.43299632, 0.55191202, 0.67125649, 2.15632596,\n",
       "        2.73556938, 3.38071041, 0.01100025, 0.01179738, 0.01199999,\n",
       "        0.04700327, 0.05159984, 0.05313249, 0.09273086, 0.10112996,\n",
       "        0.10480018, 0.45547185, 0.49066267, 0.48658199, 0.01239982,\n",
       "        0.01439986, 0.01859989, 0.05479932, 0.06719995, 0.08472834,\n",
       "        0.10743384, 0.132725  , 0.17179384, 0.55805702, 0.68084855,\n",
       "        0.8108355 , 0.01219683, 0.01540165, 0.02000341, 0.05679789,\n",
       "        0.06713877, 0.08619981, 0.10925694, 0.13419967, 0.17333789,\n",
       "        0.56462169, 0.68402462, 0.83619566, 0.00500283, 0.00540028,\n",
       "        0.00499887, 0.01919909, 0.02040033, 0.02020168, 0.03859849,\n",
       "        0.03933592, 0.03920221, 0.17800045, 0.19134192, 0.18770862,\n",
       "        0.00600014, 0.00679998, 0.00699964, 0.02487888, 0.0279983 ,\n",
       "        0.03040538, 0.04880114, 0.05573392, 0.05880032, 0.22943697,\n",
       "        0.26279607, 0.29320035, 0.00640044, 0.00699873, 0.00799775,\n",
       "        0.02500157, 0.02919683, 0.03360028, 0.04980206, 0.05770969,\n",
       "        0.06653471, 0.24415393, 0.28113365, 0.32277293, 0.01180062,\n",
       "        0.01240163, 0.01360016, 0.05179977, 0.055932  , 0.06093287,\n",
       "        0.10340014, 0.11180105, 0.11980128, 0.4992857 , 0.53186517,\n",
       "        0.53400626, 0.01599908, 0.01899977, 0.0229321 , 0.07419991,\n",
       "        0.09192977, 0.11500015, 0.15325918, 0.19413238, 0.24380116,\n",
       "        0.8444283 , 1.02581358, 1.14538231, 0.01619973, 0.01906824,\n",
       "        0.02503886, 0.07539926, 0.09547033, 0.12525964, 0.15660138,\n",
       "        0.20196953, 0.26503973, 0.90158348, 1.15687575, 1.42182679,\n",
       "        0.01599774, 0.01620216, 0.01599836, 0.07200103, 0.07432237,\n",
       "        0.0737988 , 0.14160085, 0.14601016, 0.14533772, 0.67074509,\n",
       "        0.69825411, 0.67952113, 0.04079881, 0.05019956, 0.05479932,\n",
       "        0.19339442, 0.23020015, 0.26384754, 0.37813396, 0.45977979,\n",
       "        0.53333645, 1.66046777, 2.0747767 , 2.74842558, 0.04480243,\n",
       "        0.05619297, 0.06879749, 0.21548352, 0.27385573, 0.3363256 ,\n",
       "        0.43052721, 0.55096607, 0.67922626, 1.99810443, 2.54158425,\n",
       "        3.68779883, 0.01119986, 0.01240177, 0.01339946, 0.05026512,\n",
       "        0.0534564 , 0.05539761, 0.09946876, 0.09979987, 0.09980054,\n",
       "        0.47210197, 0.4913096 , 0.48005261, 0.01279621, 0.01523652,\n",
       "        0.01939931, 0.0586    , 0.07019954, 0.08539968, 0.11900001,\n",
       "        0.14922223, 0.17699966, 0.68099999, 0.85279975, 0.97282386,\n",
       "        0.01339593, 0.01540322, 0.01879821, 0.05802317, 0.07159476,\n",
       "        0.08999758, 0.12545114, 0.14540024, 0.18299041, 0.72583404,\n",
       "        0.82710419, 0.99155388, 0.00500154, 0.00520363, 0.00499687,\n",
       "        0.01914015, 0.02059946, 0.02000003, 0.03740005, 0.03898816,\n",
       "        0.03813787, 0.1791235 , 0.18839912, 0.18475947, 0.00599895,\n",
       "        0.00639653, 0.00720143, 0.02459908, 0.02720237, 0.03079758,\n",
       "        0.04768062, 0.05372109, 0.05919843, 0.22933493, 0.25772791,\n",
       "        0.26420722, 0.00639997, 0.0072031 , 0.00779872, 0.02600079,\n",
       "        0.02999763, 0.03399944, 0.05099998, 0.05860248, 0.06640401,\n",
       "        0.24463825, 0.28087811, 0.27174196, 0.01180201, 0.01260023,\n",
       "        0.01360278, 0.05077543, 0.05440083, 0.05473232, 0.0997735 ,\n",
       "        0.11105509, 0.10359869, 0.49522285, 0.50696478, 0.45430288,\n",
       "        0.01718845, 0.01999774, 0.02593307, 0.08485231, 0.1033958 ,\n",
       "        0.12277794, 0.17204437, 0.20996499, 0.23490238, 0.87910209,\n",
       "        1.05783348, 1.08677006, 0.01699691, 0.02099977, 0.02800236,\n",
       "        0.08960028, 0.11382265, 0.14745584, 0.18427262, 0.23253794,\n",
       "        0.28533707, 0.93227482, 1.18909378, 1.39669356, 0.01600156,\n",
       "        0.01679697, 0.01579814, 0.06819835, 0.07149377, 0.06919808,\n",
       "        0.13459759, 0.14139819, 0.13720002, 0.693643  , 0.70252557,\n",
       "        0.67939668, 0.04073334, 0.04859772, 0.0550878 , 0.17211428,\n",
       "        0.21043296, 0.27522259, 0.33660855, 0.40913858, 0.53342624,\n",
       "        1.59045706, 1.76480865, 1.4447897 , 0.04440336, 0.05599813,\n",
       "        0.06920176, 0.20120754, 0.25199671, 0.35900178, 0.3960855 ,\n",
       "        0.49114051, 0.7471868 , 1.93163934, 2.20735025, 1.48735218,\n",
       "        0.01020098, 0.011203  , 0.01119976, 0.0476017 , 0.04913216,\n",
       "        0.04940033, 0.09199834, 0.09839668, 0.09899621, 0.46652927,\n",
       "        0.48977098, 0.48354969, 0.01265054, 0.01480012, 0.01779947,\n",
       "        0.06559987, 0.08027477, 0.09239926, 0.1446002 , 0.16840253,\n",
       "        0.1873353 , 0.73327012, 0.84720435, 0.96369166, 0.01279945,\n",
       "        0.01519909, 0.01799979, 0.06760154, 0.08154249, 0.10093875,\n",
       "        0.15013695, 0.17559986, 0.22171254, 0.7837163 , 0.91674771,\n",
       "        1.18562965, 0.00500021, 0.00539985, 0.00499969, 0.01959825,\n",
       "        0.02080007, 0.02020073, 0.03717313, 0.03953195, 0.03904309,\n",
       "        0.18055911, 0.1901792 , 0.17538824, 0.0060112 , 0.00642428,\n",
       "        0.00620103, 0.02179899, 0.020999  , 0.02059932, 0.03819847,\n",
       "        0.0386807 , 0.04020085, 0.17566485, 0.18092966, 0.18960252,\n",
       "        0.0059381 , 0.00580177, 0.00599771, 0.02020035, 0.02059855,\n",
       "        0.02099953, 0.03740034, 0.03860011, 0.04020052, 0.1747323 ,\n",
       "        0.17959795, 0.19053626, 0.01139603, 0.01219897, 0.01239939,\n",
       "        0.05093293, 0.05279918, 0.04604502, 0.10000033, 0.10352993,\n",
       "        0.08880005, 0.49550252, 0.50399947, 0.42857938, 0.01779723,\n",
       "        0.02020197, 0.02000232, 0.07780166, 0.04120131, 0.06039801,\n",
       "        0.12285738, 0.06260071, 0.11179852, 0.31181884, 0.23126574,\n",
       "        0.52306681, 0.01839957, 0.01999993, 0.01972685, 0.07427626,\n",
       "        0.03840003, 0.06159835, 0.13012853, 0.06039987, 0.11340017,\n",
       "        0.37053056, 0.23270259, 0.52485232, 0.014821  , 0.01480188,\n",
       "        0.01580205, 0.06859989, 0.06980019, 0.07000003, 0.13660116,\n",
       "        0.13845444, 0.1370667 , 0.66548066, 0.58849683, 0.36335816,\n",
       "        0.03120012, 0.03100004, 0.02259922, 0.1198482 , 0.05780025,\n",
       "        0.04468307, 0.17620077, 0.0860014 , 0.07000136, 0.41405115,\n",
       "        0.32158275, 0.28667374, 0.02940106, 0.02860031, 0.01800003,\n",
       "        0.08630629, 0.05278664, 0.03904004, 0.11710086, 0.08093009,\n",
       "        0.06440372, 0.34788618, 0.31634297, 0.27823119, 0.01060004,\n",
       "        0.01020002, 0.00820031, 0.02980003, 0.02914405, 0.02399821,\n",
       "        0.05234494, 0.0512001 , 0.04400024, 0.23065858, 0.22893629,\n",
       "        0.20453053, 0.01040039, 0.00960059, 0.00799518, 0.02859864,\n",
       "        0.02739973, 0.02419987, 0.05100365, 0.04900279, 0.04352908,\n",
       "        0.22913537, 0.22417245, 0.2013989 , 0.01059923, 0.00959735,\n",
       "        0.00819988, 0.02860036, 0.02760162, 0.02399879, 0.05040007,\n",
       "        0.04927297, 0.04335699, 0.22455196, 0.22390542, 0.20314641]),\n",
       " 'std_fit_time': array([1.02008566e-03, 3.99146010e-04, 4.91176382e-04, 4.89706761e-04,\n",
       "        1.17146687e-03, 4.00229098e-04, 5.31749690e-04, 4.89160451e-04,\n",
       "        2.59406628e-04, 1.14038094e-03, 9.70317401e-04, 9.56823770e-04,\n",
       "        4.00114102e-04, 3.99209749e-04, 4.89266280e-04, 4.88260285e-04,\n",
       "        3.99569292e-04, 4.89128638e-04, 4.92707707e-04, 4.01490641e-04,\n",
       "        7.50293027e-04, 4.54938596e-03, 3.57299007e-03, 3.01609972e-03,\n",
       "        3.99639569e-04, 1.62124634e-06, 2.48504910e-06, 4.89630627e-04,\n",
       "        4.90318584e-04, 7.43447516e-04, 5.35356016e-04, 7.30302294e-04,\n",
       "        1.22645375e-03, 2.27678358e-03, 1.25855943e-03, 5.82178691e-03,\n",
       "        2.07040908e-03, 5.13164927e-04, 1.16781674e-03, 2.62354091e-03,\n",
       "        1.49841074e-03, 5.06646971e-03, 7.59179945e-03, 3.74329866e-03,\n",
       "        8.17727850e-03, 1.70825550e-02, 1.47351223e-02, 2.16955278e-02,\n",
       "        1.84339137e-03, 2.00121816e-03, 4.02519311e-03, 7.68843304e-03,\n",
       "        1.01402132e-02, 1.93166853e-02, 1.41345582e-02, 1.90747019e-02,\n",
       "        4.04690682e-02, 6.84118409e-02, 8.98761109e-02, 1.50504834e-01,\n",
       "        2.13554672e-03, 2.63778398e-03, 4.57790453e-03, 8.16270710e-03,\n",
       "        1.24439734e-02, 2.26140040e-02, 1.52181441e-02, 2.43090289e-02,\n",
       "        4.49010161e-02, 7.79019961e-02, 1.10185773e-01, 1.90356040e-01,\n",
       "        1.52532957e-03, 4.89881991e-04, 4.54053111e-04, 4.89765167e-04,\n",
       "        4.89726234e-04, 1.60015824e-03, 4.90563895e-04, 5.63979007e-04,\n",
       "        1.01967401e-03, 3.64286596e-03, 6.42335172e-03, 3.88836785e-03,\n",
       "        9.85768210e-04, 1.62601564e-03, 1.19955857e-03, 5.31743582e-03,\n",
       "        4.30604336e-03, 6.59474132e-03, 7.46501760e-03, 1.08707026e-02,\n",
       "        1.07533757e-02, 4.42064641e-02, 5.15640455e-02, 5.21369372e-02,\n",
       "        1.02014593e-03, 1.17101094e-03, 3.19131762e-03, 2.17124229e-03,\n",
       "        5.40881882e-03, 2.85634301e-02, 7.15323345e-03, 7.36541477e-03,\n",
       "        3.32017803e-02, 4.43079346e-02, 6.41703176e-02, 1.68980440e-01,\n",
       "        4.00281134e-04, 4.89842988e-04, 9.79656823e-04, 1.53806356e-03,\n",
       "        1.67459891e-03, 4.45350855e-03, 2.05714767e-03, 2.97681026e-03,\n",
       "        8.24509865e-03, 9.72408138e-03, 1.51157418e-02, 4.07051130e-02,\n",
       "        4.88407325e-04, 6.32735301e-04, 1.35614806e-03, 3.20228749e-03,\n",
       "        4.53675531e-03, 7.19601967e-03, 6.18226206e-03, 8.77857610e-03,\n",
       "        1.49023230e-02, 2.42577413e-02, 4.28304892e-02, 7.01844192e-02,\n",
       "        1.09627996e-03, 1.02134177e-03, 1.62755720e-03, 3.26079367e-03,\n",
       "        5.12235390e-03, 9.22420872e-03, 5.46510792e-03, 9.04322195e-03,\n",
       "        1.75223919e-02, 2.47254012e-02, 4.14881370e-02, 8.19871142e-02,\n",
       "        5.32821519e-06, 4.87393904e-04, 2.80240731e-06, 4.93179088e-04,\n",
       "        4.89358310e-04, 4.00039794e-04, 4.70507588e-04, 3.86927690e-04,\n",
       "        4.83718274e-04, 2.45044132e-03, 8.22753396e-04, 1.41825359e-03,\n",
       "        7.05009826e-06, 4.01497510e-04, 4.00638779e-04, 3.98983057e-04,\n",
       "        4.44099005e-06, 4.02905379e-04, 4.89127220e-04, 4.91090661e-04,\n",
       "        1.46656221e-03, 1.45813995e-03, 5.50481366e-03, 2.13180865e-03,\n",
       "        4.89862557e-04, 3.99851867e-04, 4.00090285e-04, 4.89843034e-04,\n",
       "        2.43140197e-07, 5.62873302e-04, 3.99995259e-04, 7.91656857e-04,\n",
       "        9.79997444e-04, 1.92219175e-03, 1.62073941e-03, 5.36963265e-03,\n",
       "        7.43308369e-04, 3.97850802e-04, 4.91814907e-04, 1.60394901e-03,\n",
       "        8.17445113e-04, 2.28266553e-03, 1.89931641e-03, 1.16960845e-03,\n",
       "        4.29017303e-03, 8.08761925e-03, 9.22725611e-03, 1.58943615e-02,\n",
       "        2.05994204e-03, 2.24434866e-03, 3.97142536e-03, 6.46231624e-03,\n",
       "        8.43543378e-03, 1.48354614e-02, 1.11073582e-02, 1.51970997e-02,\n",
       "        2.38737589e-02, 3.96370693e-02, 4.78696402e-02, 7.84145057e-02,\n",
       "        2.41594083e-03, 2.41624857e-03, 4.31727927e-03, 7.17214591e-03,\n",
       "        1.03739066e-02, 1.98774888e-02, 1.17833121e-02, 2.04875811e-02,\n",
       "        3.34233303e-02, 4.52122372e-02, 6.71139425e-02, 1.12634743e-01,\n",
       "        1.15232906e-06, 1.26180375e-04, 8.09794943e-04, 4.00354295e-04,\n",
       "        9.79375227e-04, 1.08477320e-04, 1.35130173e-03, 2.48753980e-03,\n",
       "        4.47742134e-04, 5.18252915e-03, 3.59646664e-03, 1.61640380e-03,\n",
       "        1.41455992e-03, 1.09540926e-03, 1.78896464e-03, 6.35628234e-03,\n",
       "        5.08153334e-03, 5.48048830e-03, 7.78297011e-03, 1.08035320e-02,\n",
       "        1.04621198e-02, 3.78080835e-02, 5.83385823e-02, 9.07580391e-02,\n",
       "        5.27526381e-04, 1.82973029e-03, 2.55663404e-03, 3.01155942e-03,\n",
       "        4.81125950e-03, 1.83972433e-02, 6.04569505e-03, 9.30691620e-03,\n",
       "        3.52088623e-02, 2.90188149e-02, 4.20440758e-02, 2.00503006e-01,\n",
       "        6.32560510e-04, 3.98640581e-04, 6.32485129e-04, 1.41436229e-03,\n",
       "        1.74367941e-03, 3.80391967e-03, 3.10532151e-03, 3.71291927e-03,\n",
       "        7.62632789e-03, 7.46088063e-03, 1.46024129e-02, 4.04926314e-02,\n",
       "        7.99942201e-04, 1.01970198e-03, 1.74347152e-03, 2.92527214e-03,\n",
       "        4.44512311e-03, 7.53033950e-03, 5.53840118e-03, 8.45571556e-03,\n",
       "        1.36021120e-02, 1.78906006e-02, 2.78871938e-02, 5.59462477e-02,\n",
       "        7.51188432e-04, 1.19887403e-03, 1.78926684e-03, 3.66038157e-03,\n",
       "        5.13391016e-03, 8.56502907e-03, 6.33585106e-03, 8.70388754e-03,\n",
       "        1.49003786e-02, 1.80824174e-02, 3.05819572e-02, 4.82103907e-02,\n",
       "        7.42029711e-06, 4.89804139e-04, 7.43529659e-06, 3.96069606e-04,\n",
       "        4.89356614e-04, 2.46133993e-04, 1.20055853e-03, 4.21345180e-04,\n",
       "        3.98928590e-04, 8.96557310e-04, 1.80811384e-03, 1.83966809e-03,\n",
       "        3.56832255e-07, 4.00090257e-04, 4.15696997e-07, 2.15115538e-04,\n",
       "        3.07402829e-06, 4.84668101e-04, 4.02938893e-04, 1.68125748e-03,\n",
       "        7.48484493e-04, 1.87382061e-03, 3.36575434e-03, 2.04101503e-03,\n",
       "        4.89221507e-04, 3.32215566e-06, 2.69739830e-06, 3.94480093e-06,\n",
       "        3.99687443e-04, 4.90232417e-04, 3.95683399e-04, 3.98662129e-04,\n",
       "        1.91543606e-03, 8.97218429e-04, 2.67364195e-03, 4.53674683e-03,\n",
       "        4.00124018e-04, 4.88291923e-04, 4.89143252e-04, 9.80220949e-04,\n",
       "        1.34688511e-04, 7.40964961e-04, 1.20015147e-03, 1.16546897e-03,\n",
       "        2.56526932e-03, 5.70741557e-03, 6.24176142e-03, 1.22248418e-02,\n",
       "        1.41394337e-03, 1.26497019e-03, 2.45335198e-03, 4.11834494e-03,\n",
       "        4.46284466e-03, 7.87386121e-03, 6.67395793e-03, 7.61031981e-03,\n",
       "        1.37625576e-02, 1.34853424e-02, 1.47425656e-02, 5.21765472e-02,\n",
       "        1.72073395e-03, 1.89906487e-03, 3.75286850e-03, 4.58801331e-03,\n",
       "        6.20002861e-03, 1.06621420e-02, 8.33580359e-03, 1.12827971e-02,\n",
       "        1.84834105e-02, 1.39379882e-02, 9.19659913e-03, 2.26955849e-02,\n",
       "        3.97122628e-06, 3.98829387e-04, 3.27944734e-06, 1.09536962e-03,\n",
       "        6.92644678e-04, 3.99315935e-04, 2.06236335e-03, 1.10924435e-03,\n",
       "        4.24908535e-04, 1.12347907e-02, 1.08862121e-02, 1.77292271e-02,\n",
       "        1.47054153e-03, 9.79851438e-04, 1.72061197e-03, 4.53352144e-03,\n",
       "        5.19234260e-03, 7.96933493e-03, 7.21087348e-03, 1.06439111e-02,\n",
       "        1.36246625e-02, 6.87378071e-02, 6.06606090e-02, 3.48050560e-02,\n",
       "        7.53115285e-04, 1.72426954e-03, 3.19126448e-03, 2.88925799e-03,\n",
       "        5.61289693e-03, 1.83886721e-02, 6.98149145e-03, 1.00769067e-02,\n",
       "        3.91257448e-02, 4.81861331e-02, 5.53289842e-02, 1.92519474e-01,\n",
       "        7.45329267e-04, 8.01835780e-04, 8.00561983e-04, 1.17255203e-03,\n",
       "        1.81960609e-03, 6.52990642e-03, 7.33146403e-03, 3.24914249e-03,\n",
       "        8.30520802e-03, 1.14871882e-02, 9.08994232e-03, 1.41970153e-02,\n",
       "        7.51742126e-04, 6.91118779e-04, 1.35640285e-03, 2.87059114e-03,\n",
       "        3.05971369e-03, 5.60751260e-03, 3.16206354e-03, 6.09292521e-03,\n",
       "        1.37838246e-02, 2.23786800e-02, 6.26269127e-02, 7.10872430e-02,\n",
       "        1.01968371e-03, 8.01591677e-04, 1.60391751e-03, 2.86325086e-03,\n",
       "        3.77474219e-03, 7.29071378e-03, 5.88098553e-03, 9.38974457e-03,\n",
       "        1.41107236e-02, 1.25327927e-02, 3.93664552e-02, 6.17951834e-02,\n",
       "        2.98166241e-06, 4.02032734e-04, 2.25478943e-06, 4.48579437e-04,\n",
       "        4.89010597e-04, 4.62310777e-07, 4.89842988e-04, 6.32842558e-04,\n",
       "        4.43315827e-04, 9.30572553e-04, 1.01855260e-03, 7.94168566e-04,\n",
       "        3.08878608e-06, 4.85407135e-04, 4.00664731e-04, 4.87954479e-04,\n",
       "        3.98659221e-04, 4.00482883e-04, 4.08647136e-04, 6.31915436e-04,\n",
       "        4.02557932e-04, 2.69352570e-03, 4.16182643e-03, 3.01607189e-03,\n",
       "        4.95360156e-04, 4.02106176e-04, 3.99383968e-04, 6.34092513e-04,\n",
       "        3.02481680e-06, 6.26982715e-04, 6.34609649e-04, 8.00644770e-04,\n",
       "        1.25454632e-03, 1.21363912e-03, 3.75029733e-03, 5.20505143e-03,\n",
       "        3.96633463e-04, 4.91674686e-04, 4.90033403e-04, 2.97643404e-04,\n",
       "        4.90057511e-04, 1.63700565e-03, 7.95209244e-04, 6.02067112e-03,\n",
       "        1.35584492e-03, 4.95232741e-03, 3.59149994e-03, 7.62121503e-03,\n",
       "        1.19995464e-03, 1.09524292e-03, 1.55780714e-03, 2.25087329e-03,\n",
       "        4.89914795e-04, 1.41740342e-03, 1.44320420e-03, 2.44640975e-03,\n",
       "        2.76364124e-03, 4.04772130e-03, 1.07209410e-02, 4.17692146e-02,\n",
       "        8.92087805e-04, 1.41239180e-03, 2.18611763e-03, 4.89940594e-04,\n",
       "        7.61167038e-04, 9.12973364e-03, 3.60013332e-03, 2.61802738e-03,\n",
       "        4.00371796e-03, 6.14927287e-03, 5.34208605e-03, 1.22649773e-02,\n",
       "        2.74585343e-06, 4.01578190e-04, 3.99977148e-04, 1.46670664e-03,\n",
       "        8.82427682e-04, 9.81591214e-04, 1.85602102e-03, 3.00617656e-03,\n",
       "        2.03972183e-03, 6.14547346e-03, 8.73363616e-03, 1.43817826e-02,\n",
       "        1.37188956e-03, 8.01289093e-04, 1.80186644e-03, 6.33647667e-03,\n",
       "        6.99048479e-03, 5.14226303e-03, 1.10473875e-02, 1.07971021e-02,\n",
       "        3.09438115e-02, 7.75713305e-02, 9.50545403e-02, 1.93599350e-02,\n",
       "        4.90831638e-04, 1.26678645e-03, 3.81727559e-03, 4.45900106e-03,\n",
       "        5.85137838e-03, 1.52300124e-02, 7.84420988e-03, 1.56968962e-02,\n",
       "        2.73337184e-02, 6.76449875e-02, 1.09668618e-01, 2.52084771e-02,\n",
       "        3.99246957e-04, 3.98688775e-04, 7.46598945e-04, 1.21685679e-03,\n",
       "        1.06918456e-03, 1.35912101e-03, 8.96460465e-04, 2.41669325e-03,\n",
       "        2.76017551e-03, 6.57510308e-03, 9.51110866e-03, 4.07382760e-02,\n",
       "        4.37736418e-04, 4.00090285e-04, 7.48481349e-04, 2.15375046e-03,\n",
       "        4.28588151e-03, 4.71602343e-03, 2.41624857e-03, 5.42696035e-03,\n",
       "        7.06700593e-03, 1.26828199e-02, 8.74216099e-03, 4.31907082e-02,\n",
       "        3.99805434e-04, 7.49245922e-04, 8.94469065e-04, 4.22158557e-03,\n",
       "        5.04660592e-03, 7.51847658e-03, 4.61757374e-03, 6.85891266e-03,\n",
       "        1.70217766e-02, 8.68799492e-03, 1.99891924e-02, 5.60912432e-02,\n",
       "        4.15696997e-07, 4.90446630e-04, 1.78416128e-07, 4.88428214e-04,\n",
       "        3.99804154e-04, 3.99356315e-04, 2.62216850e-04, 5.83127688e-04,\n",
       "        8.61182429e-05, 1.17364303e-03, 1.61477282e-03, 1.86981527e-03,\n",
       "        2.23175073e-05, 5.49750326e-04, 3.99389109e-04, 1.32179177e-03,\n",
       "        1.09885319e-03, 1.19939307e-03, 1.60038633e-03, 1.05932071e-03,\n",
       "        2.48064545e-03, 7.68255553e-03, 6.67462656e-03, 1.33958699e-02,\n",
       "        1.28190147e-04, 3.99407713e-04, 2.72423875e-06, 7.48557624e-04,\n",
       "        8.00983241e-04, 1.67334143e-03, 1.20024698e-03, 1.01970202e-03,\n",
       "        3.31041544e-03, 6.80870487e-03, 5.57045541e-03, 1.67192382e-02,\n",
       "        4.82924365e-04, 7.45918188e-04, 4.87516254e-04, 1.29647051e-04,\n",
       "        7.48953438e-04, 1.09822857e-03, 6.30236006e-04, 1.07118978e-03,\n",
       "        3.86725614e-03, 1.99965746e-03, 1.13470928e-02, 2.40363516e-02,\n",
       "        7.45591659e-04, 7.47880106e-04, 1.54826342e-03, 4.79305966e-03,\n",
       "        1.46618942e-03, 7.03046239e-03, 1.27906398e-02, 1.62281574e-03,\n",
       "        1.49543058e-02, 1.79298166e-02, 1.45938059e-03, 7.93483639e-02,\n",
       "        4.89901939e-04, 6.32560542e-04, 1.76018748e-03, 1.41426456e-02,\n",
       "        1.62490710e-03, 8.06437054e-03, 3.24671110e-02, 1.62473107e-03,\n",
       "        1.58819458e-02, 9.21906339e-02, 2.31711259e-03, 7.95300361e-02,\n",
       "        4.13698554e-04, 4.00405236e-04, 3.99192126e-04, 7.99632078e-04,\n",
       "        4.00090342e-04, 6.32711281e-04, 2.88973581e-03, 1.04976894e-03,\n",
       "        7.94671243e-04, 5.68459368e-03, 8.11363969e-02, 3.46368467e-03,\n",
       "        7.48596333e-04, 2.60629512e-03, 1.85686061e-03, 2.18089299e-02,\n",
       "        6.91073686e-03, 1.88020764e-03, 5.26481577e-02, 7.10303290e-03,\n",
       "        1.89670188e-03, 6.85142281e-02, 7.51642053e-03, 3.70039611e-03,\n",
       "        1.35697562e-03, 3.26170902e-03, 1.09567047e-03, 2.06548860e-02,\n",
       "        5.61857837e-03, 1.09830200e-03, 2.54158934e-02, 5.17334952e-03,\n",
       "        1.01784775e-03, 2.86680489e-02, 5.85728437e-03, 8.78642763e-04,\n",
       "        4.89551003e-04, 9.79958507e-04, 4.00114130e-04, 1.83311074e-03,\n",
       "        1.88969616e-03, 6.30150911e-04, 2.18363038e-03, 2.31504079e-03,\n",
       "        6.32560564e-04, 1.97489718e-03, 1.75752267e-03, 1.43669050e-03,\n",
       "        1.62555307e-03, 1.49626189e-03, 6.35128325e-04, 1.49566776e-03,\n",
       "        1.74189531e-03, 3.96791915e-04, 1.41496784e-03, 1.67117769e-03,\n",
       "        8.61325366e-04, 2.01738218e-03, 1.08958901e-03, 7.96413493e-04,\n",
       "        1.49786765e-03, 1.35776563e-03, 7.51030353e-04, 8.00942065e-04,\n",
       "        1.74669872e-03, 6.32340287e-04, 7.97798719e-04, 2.50136958e-03,\n",
       "        7.13183925e-04, 7.89147538e-04, 1.97407685e-03, 9.28696396e-04]),\n",
       " 'mean_score_time': array([0.00119896, 0.00120029, 0.00100179, 0.00113139, 0.0012013 ,\n",
       "        0.00139952, 0.0014029 , 0.00119824, 0.00139623, 0.00160193,\n",
       "        0.00180011, 0.00160017, 0.00099959, 0.00120029, 0.0012012 ,\n",
       "        0.00120006, 0.00119743, 0.00119996, 0.00139947, 0.00160255,\n",
       "        0.00119948, 0.0019958 , 0.00199957, 0.00200005, 0.00140147,\n",
       "        0.00100007, 0.00120363, 0.00139852, 0.00180092, 0.00140576,\n",
       "        0.00100384, 0.00160074, 0.00160136, 0.00199914, 0.00220017,\n",
       "        0.00179982, 0.00099988, 0.00100107, 0.00120153, 0.00099854,\n",
       "        0.00160155, 0.00119557, 0.00199838, 0.00139484, 0.00119772,\n",
       "        0.00140042, 0.00139966, 0.0018024 , 0.00100203, 0.00120358,\n",
       "        0.00099831, 0.00139675, 0.00140033, 0.00140357, 0.00139933,\n",
       "        0.00160074, 0.00139995, 0.00159369, 0.00159855, 0.00185008,\n",
       "        0.00119987, 0.00120029, 0.00120077, 0.00119977, 0.00140038,\n",
       "        0.00121026, 0.001998  , 0.00140038, 0.00150614, 0.00200291,\n",
       "        0.00159926, 0.00180154, 0.00119939, 0.00100007, 0.00099983,\n",
       "        0.0012001 , 0.00119987, 0.00120001, 0.00119996, 0.00159998,\n",
       "        0.00100026, 0.00179706, 0.00159941, 0.00180154, 0.00099955,\n",
       "        0.00119691, 0.00100121, 0.00140262, 0.00159941, 0.00119958,\n",
       "        0.00120029, 0.00159793, 0.00100102, 0.0020009 , 0.00220008,\n",
       "        0.00180049, 0.00160375, 0.00160422, 0.00120249, 0.00139904,\n",
       "        0.00159917, 0.00179787, 0.0015976 , 0.00181618, 0.00159764,\n",
       "        0.00220051, 0.00199633, 0.00219994, 0.00100021, 0.00099978,\n",
       "        0.00099998, 0.00139966, 0.00139799, 0.00100188, 0.00119934,\n",
       "        0.00101175, 0.00120063, 0.0010015 , 0.00179963, 0.00159855,\n",
       "        0.00119934, 0.00119581, 0.00100284, 0.00099812, 0.0012001 ,\n",
       "        0.00120158, 0.00119905, 0.00100985, 0.00120087, 0.00160127,\n",
       "        0.0014009 , 0.00200148, 0.00099978, 0.00120139, 0.00120001,\n",
       "        0.00140009, 0.00099936, 0.00119886, 0.00159822, 0.00140214,\n",
       "        0.00120044, 0.00159998, 0.00179973, 0.00159979, 0.00099778,\n",
       "        0.00079885, 0.00099974, 0.00100355, 0.00140061, 0.00099869,\n",
       "        0.00099812, 0.00159912, 0.00120187, 0.00180011, 0.0018012 ,\n",
       "        0.00140333, 0.00100651, 0.00100031, 0.00099955, 0.00140257,\n",
       "        0.00119781, 0.00120034, 0.00120082, 0.00160012, 0.00120196,\n",
       "        0.00199594, 0.00200405, 0.00200119, 0.00100007, 0.00100017,\n",
       "        0.00120049, 0.00100007, 0.00099993, 0.00140052, 0.00119982,\n",
       "        0.00120015, 0.00100012, 0.00219998, 0.00219688, 0.00199986,\n",
       "        0.00099859, 0.00099821, 0.00100036, 0.00119991, 0.00170856,\n",
       "        0.00119772, 0.00160155, 0.00160165, 0.00100169, 0.00140133,\n",
       "        0.00160007, 0.00179687, 0.00099964, 0.00120144, 0.00120144,\n",
       "        0.00102715, 0.00100079, 0.00119839, 0.00159993, 0.00159993,\n",
       "        0.0012002 , 0.00159845, 0.00200062, 0.00180073, 0.00120063,\n",
       "        0.00100017, 0.00120072, 0.00159974, 0.00140014, 0.00120115,\n",
       "        0.00119987, 0.00120044, 0.00160217, 0.00160007, 0.00200033,\n",
       "        0.00160031, 0.00100088, 0.00119834, 0.00119543, 0.00119925,\n",
       "        0.00100169, 0.00120091, 0.00120168, 0.00159898, 0.00119724,\n",
       "        0.00160041, 0.00140152, 0.00139976, 0.00100036, 0.00100031,\n",
       "        0.00120134, 0.00132446, 0.00179696, 0.00120463, 0.00120311,\n",
       "        0.00139985, 0.00139894, 0.00200009, 0.00219913, 0.00200219,\n",
       "        0.00100079, 0.00140185, 0.00120502, 0.00100093, 0.00140138,\n",
       "        0.0013999 , 0.00180044, 0.00140047, 0.00180287, 0.00199919,\n",
       "        0.00200076, 0.00200028, 0.00099978, 0.00100265, 0.00099993,\n",
       "        0.00119681, 0.00099988, 0.00139995, 0.00119987, 0.00139976,\n",
       "        0.00140285, 0.0015996 , 0.00159869, 0.00159936, 0.00120025,\n",
       "        0.00100002, 0.00140085, 0.00099983, 0.00139999, 0.0012002 ,\n",
       "        0.00110536, 0.00159883, 0.0012032 , 0.00140128, 0.00120087,\n",
       "        0.0012002 , 0.0010016 , 0.00119829, 0.0009985 , 0.00119991,\n",
       "        0.00139709, 0.00159988, 0.00119987, 0.00160108, 0.00120029,\n",
       "        0.0018301 , 0.002002  , 0.00119791, 0.00139704, 0.00099988,\n",
       "        0.00100093, 0.00100093, 0.00119963, 0.00100617, 0.00100126,\n",
       "        0.00100269, 0.0009985 , 0.00199933, 0.0016006 , 0.00159993,\n",
       "        0.00099978, 0.0012001 , 0.0010005 , 0.00120416, 0.00140171,\n",
       "        0.00119729, 0.00159659, 0.00139976, 0.00139999, 0.00200005,\n",
       "        0.0020021 , 0.00220027, 0.00099797, 0.00099964, 0.00100312,\n",
       "        0.0013958 , 0.00140324, 0.00099964, 0.00119944, 0.00119896,\n",
       "        0.00119901, 0.00220418, 0.00240121, 0.00180116, 0.0010004 ,\n",
       "        0.00119867, 0.00099955, 0.00140181, 0.00140047, 0.00099812,\n",
       "        0.00120201, 0.00119939, 0.00139999, 0.00180058, 0.00199995,\n",
       "        0.00159907, 0.00100098, 0.00120034, 0.00100079, 0.00120001,\n",
       "        0.00140033, 0.00119967, 0.00119963, 0.00160518, 0.00160012,\n",
       "        0.00179663, 0.00199866, 0.00160246, 0.00120029, 0.00142441,\n",
       "        0.00120082, 0.00119977, 0.00160007, 0.00159998, 0.00139999,\n",
       "        0.00160017, 0.0015986 , 0.00159702, 0.00199766, 0.00199652,\n",
       "        0.00120306, 0.0009995 , 0.00120177, 0.00119929, 0.00159574,\n",
       "        0.00140114, 0.00099988, 0.00139971, 0.00100121, 0.00139966,\n",
       "        0.00180292, 0.00160193, 0.00100045, 0.00140052, 0.00140071,\n",
       "        0.00120063, 0.00119848, 0.00119953, 0.0016016 , 0.00132766,\n",
       "        0.00139971, 0.00199695, 0.00200038, 0.00200114, 0.00099921,\n",
       "        0.00140095, 0.00140233, 0.00119896, 0.00119796, 0.00120454,\n",
       "        0.00119834, 0.00139971, 0.00139585, 0.00179992, 0.00199928,\n",
       "        0.00220032, 0.00120058, 0.00139785, 0.00160027, 0.00119762,\n",
       "        0.00146713, 0.00140095, 0.00139108, 0.00140004, 0.00139995,\n",
       "        0.00160022, 0.00160127, 0.00140023, 0.00100031, 0.00099983,\n",
       "        0.00100074, 0.00120001, 0.00140066, 0.00120029, 0.0013999 ,\n",
       "        0.00160069, 0.00160012, 0.00140004, 0.00200276, 0.00179996,\n",
       "        0.00140591, 0.00099645, 0.00100045, 0.00157814, 0.00120077,\n",
       "        0.00140123, 0.00160184, 0.0011991 , 0.00121045, 0.00200462,\n",
       "        0.00139732, 0.00140185, 0.00100007, 0.00119653, 0.00120368,\n",
       "        0.00119691, 0.00100188, 0.00099998, 0.00119982, 0.00140328,\n",
       "        0.00120039, 0.00180483, 0.00180264, 0.00159898, 0.00100112,\n",
       "        0.00120344, 0.00099721, 0.00099945, 0.00099916, 0.00100021,\n",
       "        0.00120206, 0.00100665, 0.00160208, 0.00200119, 0.00180168,\n",
       "        0.00199919, 0.0009985 , 0.00119853, 0.00100193, 0.00139799,\n",
       "        0.00140004, 0.00100245, 0.00180006, 0.00119934, 0.00120006,\n",
       "        0.00200043, 0.00200171, 0.00200062, 0.00119762, 0.00099936,\n",
       "        0.00099721, 0.00139956, 0.00140061, 0.00099998, 0.00100307,\n",
       "        0.00139761, 0.00100155, 0.00179987, 0.00160398, 0.00200324,\n",
       "        0.00101199, 0.00099864, 0.00100193, 0.00139923, 0.00100441,\n",
       "        0.00160003, 0.00139723, 0.001401  , 0.00180078, 0.00199885,\n",
       "        0.00180159, 0.00159974, 0.00100312, 0.00099945, 0.00099678,\n",
       "        0.0011982 , 0.00140157, 0.00180626, 0.00139661, 0.00140176,\n",
       "        0.00119977, 0.00200028, 0.00160079, 0.00199933, 0.00099959,\n",
       "        0.00140309, 0.00120296, 0.00140066, 0.00100064, 0.00100188,\n",
       "        0.00100279, 0.00120025, 0.00139985, 0.00139976, 0.00139771,\n",
       "        0.00160031, 0.00119853, 0.00120177, 0.00120072, 0.00139861,\n",
       "        0.00139976, 0.00139995, 0.00140228, 0.00099888, 0.00139813,\n",
       "        0.00199823, 0.00199933, 0.00179715, 0.00120044, 0.00100131,\n",
       "        0.00099974, 0.00139809, 0.00140028, 0.00120173, 0.0012012 ,\n",
       "        0.00160303, 0.00120187, 0.00220013, 0.00200443, 0.00159822,\n",
       "        0.00160055, 0.00099645, 0.00100021, 0.00119781, 0.00099964,\n",
       "        0.00099983, 0.00159974, 0.00139947, 0.0010047 , 0.00180001,\n",
       "        0.00119977, 0.00159888, 0.00119848, 0.00099978, 0.0010004 ,\n",
       "        0.00120015, 0.00120068, 0.00140028, 0.00140014, 0.00139704,\n",
       "        0.00119834, 0.00200152, 0.00200133, 0.00200067, 0.00100002,\n",
       "        0.00100083, 0.00120015, 0.0009984 , 0.00120058, 0.00120072,\n",
       "        0.00120044, 0.00160022, 0.0016017 , 0.0018002 , 0.00200076,\n",
       "        0.00180058, 0.00099993, 0.00100017, 0.00100031, 0.00120177,\n",
       "        0.00099983, 0.00099916, 0.00120034, 0.00120006, 0.00100021,\n",
       "        0.00179987, 0.00180006, 0.00179996, 0.00099993, 0.00100703,\n",
       "        0.00100107, 0.00119934, 0.00120063, 0.00119967, 0.00140328,\n",
       "        0.00120368, 0.00139918, 0.0015996 , 0.00179901, 0.00179877,\n",
       "        0.00099859, 0.00099831, 0.00120134, 0.0013998 , 0.00120144,\n",
       "        0.00160046, 0.00099936, 0.00119996, 0.00119953, 0.0015985 ,\n",
       "        0.00180368, 0.00120091, 0.00120564, 0.00100102, 0.00100069,\n",
       "        0.00139918, 0.00099721, 0.00120158, 0.00119958, 0.00159864,\n",
       "        0.0012013 , 0.00159726, 0.00179973, 0.00159998, 0.00100312,\n",
       "        0.0011981 , 0.00099745, 0.00119991, 0.00119877, 0.00120249,\n",
       "        0.00100269, 0.00140176, 0.00100107, 0.00160146, 0.00159502,\n",
       "        0.00160108, 0.00100021, 0.00099998, 0.00120149, 0.0012001 ,\n",
       "        0.00120144, 0.00119991, 0.00139985, 0.00120015, 0.00119977,\n",
       "        0.00159979, 0.00119963, 0.00140166, 0.00110288, 0.00139847,\n",
       "        0.00099869, 0.00139995, 0.00139961, 0.00119972, 0.0014008 ,\n",
       "        0.00159822, 0.00119829, 0.00139871, 0.00145555, 0.00159817,\n",
       "        0.00099792, 0.00139875, 0.00119948, 0.00140009, 0.00119967,\n",
       "        0.00139999, 0.00119925, 0.00100112, 0.00139618, 0.00140343,\n",
       "        0.0013032 , 0.00160041, 0.00099874, 0.00100069, 0.00119987,\n",
       "        0.00119996, 0.00099983, 0.00139976, 0.0013998 , 0.00119858,\n",
       "        0.00139832, 0.00139952, 0.00120101, 0.00160065, 0.00119996,\n",
       "        0.00099993, 0.00099964, 0.00119996, 0.00099974, 0.00100026,\n",
       "        0.00140033, 0.00120001, 0.00119967, 0.0015996 , 0.00119987,\n",
       "        0.00179958, 0.00099955, 0.00099916, 0.00100493, 0.00140138,\n",
       "        0.00100155, 0.00100002, 0.00119791, 0.00139556, 0.00159864,\n",
       "        0.00140104, 0.00159941, 0.00180149, 0.0009995 , 0.00100207,\n",
       "        0.00100036, 0.00119591, 0.0009995 , 0.00100031, 0.00140123,\n",
       "        0.00100031, 0.00139956, 0.00140114, 0.00160036, 0.00159998]),\n",
       " 'std_score_time': array([4.00845675e-04, 4.00614948e-04, 3.39325845e-06, 2.62928009e-04,\n",
       "        3.98600117e-04, 4.90563547e-04, 4.87947056e-04, 4.00931913e-04,\n",
       "        4.87912852e-04, 4.89116142e-04, 4.01521476e-04, 4.90096367e-04,\n",
       "        7.77697870e-07, 4.01568763e-04, 3.99457914e-04, 4.00038884e-04,\n",
       "        4.01117804e-04, 3.99114581e-04, 4.91404195e-04, 4.83916225e-04,\n",
       "        4.00305093e-04, 3.49427623e-06, 2.80240731e-06, 4.64126944e-06,\n",
       "        4.90234411e-04, 5.19643125e-06, 3.98383523e-04, 4.89435782e-04,\n",
       "        3.96238383e-04, 4.88230942e-04, 3.68617021e-06, 4.88343831e-04,\n",
       "        4.89126011e-04, 2.03425684e-06, 4.00581284e-04, 3.99992741e-04,\n",
       "        3.50402318e-07, 3.19302922e-06, 3.98144682e-04, 3.25299414e-06,\n",
       "        4.91238478e-04, 3.98332666e-04, 5.53295506e-06, 4.90788837e-04,\n",
       "        4.01118342e-04, 4.91968830e-04, 4.92986176e-04, 4.01156824e-04,\n",
       "        3.28083370e-06, 3.94693800e-04, 2.10241391e-06, 4.89042399e-04,\n",
       "        4.91661903e-04, 4.91647738e-04, 4.84619093e-04, 4.90564150e-04,\n",
       "        4.89630047e-04, 4.88315745e-04, 4.91893051e-04, 4.36123706e-04,\n",
       "        3.99875669e-04, 3.99661331e-04, 4.00257764e-04, 3.99565963e-04,\n",
       "        4.89667804e-04, 3.99625709e-04, 4.22586423e-06, 4.89668245e-04,\n",
       "        4.47199191e-04, 3.68555333e-06, 4.92185471e-04, 4.00457981e-04,\n",
       "        3.98206754e-04, 4.15696997e-07, 5.76164530e-07, 4.00114187e-04,\n",
       "        3.99994975e-04, 3.99684934e-04, 4.00066461e-04, 4.89940339e-04,\n",
       "        3.56832255e-07, 3.98341114e-04, 4.89182418e-04, 4.01324107e-04,\n",
       "        5.62101823e-06, 3.98384345e-04, 2.57492065e-06, 4.90280595e-04,\n",
       "        4.90078042e-04, 4.00616111e-04, 4.00018706e-04, 4.92855883e-04,\n",
       "        1.85047225e-06, 1.31972814e-06, 4.00121257e-04, 3.99830855e-04,\n",
       "        4.86503906e-04, 4.89239446e-04, 3.94627983e-04, 4.91738351e-04,\n",
       "        4.87372495e-04, 3.99218349e-04, 4.91824149e-04, 4.09499845e-04,\n",
       "        4.94489812e-04, 3.99756613e-04, 3.32899278e-06, 7.52092840e-04,\n",
       "        1.29186794e-06, 2.43140197e-07, 4.62310777e-07, 4.87351361e-04,\n",
       "        4.91536422e-04, 2.43327156e-06, 4.00378517e-04, 1.97352986e-05,\n",
       "        4.03128844e-04, 2.03760725e-06, 3.99732831e-04, 4.91303107e-04,\n",
       "        4.00510929e-04, 3.98936586e-04, 4.90563246e-06, 4.59498852e-06,\n",
       "        3.99757039e-04, 3.99268125e-04, 3.98850708e-04, 1.88935856e-05,\n",
       "        3.99508798e-04, 4.91291445e-04, 4.88955165e-04, 2.75329590e-06,\n",
       "        3.87384339e-07, 3.99125320e-04, 3.99807879e-04, 4.88734910e-04,\n",
       "        5.00928726e-06, 3.98231020e-04, 4.88801270e-04, 4.92521105e-04,\n",
       "        3.99828000e-04, 4.89745664e-04, 3.99780359e-04, 4.88913256e-04,\n",
       "        2.36983750e-06, 3.99440758e-04, 2.11857413e-06, 4.15532874e-06,\n",
       "        4.92107836e-04, 2.45652139e-06, 3.05548083e-06, 4.87704337e-04,\n",
       "        4.03187374e-04, 4.00099406e-04, 4.01393618e-04, 4.97098907e-04,\n",
       "        6.13732214e-06, 2.38704510e-06, 5.13702139e-06, 4.91099689e-04,\n",
       "        4.01149246e-04, 4.00975811e-04, 3.99521125e-04, 4.88708508e-04,\n",
       "        3.98360864e-04, 3.78357590e-06, 7.60641213e-06, 8.77769011e-06,\n",
       "        2.86102295e-07, 3.98950589e-07, 4.00161772e-04, 3.23406696e-07,\n",
       "        2.13248060e-07, 4.90427154e-04, 4.00018763e-04, 4.00090313e-04,\n",
       "        3.50402318e-07, 3.99780359e-04, 4.01102385e-04, 4.45275022e-06,\n",
       "        5.19861857e-06, 4.83090406e-06, 2.52048021e-06, 4.04154399e-04,\n",
       "        3.99477921e-04, 3.99318839e-04, 4.89884790e-04, 4.91314446e-04,\n",
       "        3.23968654e-06, 4.92695570e-04, 4.89823562e-04, 4.02561835e-04,\n",
       "        5.91739352e-07, 4.02546861e-04, 3.99337623e-04, 5.03787879e-05,\n",
       "        1.39510525e-06, 4.01105616e-04, 4.89998815e-04, 4.89713540e-04,\n",
       "        3.99828028e-04, 4.88216579e-04, 1.05982355e-06, 3.99923481e-04,\n",
       "        3.99733087e-04, 5.84003864e-07, 4.00161857e-04, 4.89648320e-04,\n",
       "        4.89765144e-04, 3.99947632e-04, 4.00114074e-04, 3.99828711e-04,\n",
       "        4.86685055e-04, 4.89920940e-04, 1.03375833e-06, 4.91293574e-04,\n",
       "        3.79078043e-06, 4.00771785e-04, 4.02006841e-04, 4.00306768e-04,\n",
       "        8.70877507e-06, 4.02097977e-04, 3.95286972e-04, 4.85730929e-04,\n",
       "        3.95466288e-04, 4.83679099e-04, 4.88472427e-04, 4.89590169e-04,\n",
       "        3.50402318e-07, 2.43140197e-07, 3.99734339e-04, 4.15213233e-04,\n",
       "        3.98634495e-04, 4.03701687e-04, 3.98650163e-04, 4.87187621e-04,\n",
       "        4.90844799e-04, 7.97901179e-07, 4.01173601e-04, 3.84260933e-06,\n",
       "        1.15430054e-06, 4.87791417e-04, 4.01248691e-04, 2.68641777e-06,\n",
       "        4.90224648e-04, 4.89279599e-04, 3.99542097e-04, 4.89881921e-04,\n",
       "        4.02270871e-04, 1.69528855e-06, 3.80694078e-06, 6.97552626e-07,\n",
       "        1.90734863e-07, 4.96368930e-06, 3.37174788e-07, 4.02050175e-04,\n",
       "        9.53674316e-08, 4.89726187e-04, 3.99994861e-04, 4.89784628e-04,\n",
       "        4.93603461e-04, 4.89823538e-04, 4.88894551e-04, 4.89568908e-04,\n",
       "        4.00042914e-04, 3.56832255e-07, 4.89086892e-04, 1.16800773e-07,\n",
       "        4.89784651e-04, 4.03051301e-04, 2.11990314e-04, 4.88429494e-04,\n",
       "        4.00955482e-04, 4.88838947e-04, 3.99299388e-04, 3.96620553e-04,\n",
       "        3.00445252e-06, 4.00906041e-04, 7.81343918e-06, 4.00353381e-04,\n",
       "        4.92296192e-04, 4.89765214e-04, 3.99875669e-04, 4.90934657e-04,\n",
       "        3.99899778e-04, 4.19101534e-04, 6.26274373e-06, 3.98022737e-04,\n",
       "        4.85034895e-04, 3.50402318e-07, 6.45123845e-06, 5.47096615e-06,\n",
       "        3.99279605e-04, 2.64067924e-05, 5.03644788e-06, 2.85545443e-06,\n",
       "        7.46825030e-06, 5.50617888e-06, 4.90934333e-04, 4.89414974e-04,\n",
       "        3.23406696e-07, 3.99995117e-04, 3.23406696e-07, 3.98058982e-04,\n",
       "        4.91992238e-04, 3.94849252e-04, 4.87571499e-04, 4.89784605e-04,\n",
       "        4.90177304e-04, 4.69968923e-06, 3.33990307e-06, 4.01801438e-04,\n",
       "        4.14985327e-06, 4.91257996e-06, 5.30254926e-06, 4.89434370e-04,\n",
       "        4.89091982e-04, 3.50402318e-07, 4.00477909e-04, 3.97133374e-04,\n",
       "        3.98789124e-04, 4.01507562e-04, 4.94113624e-04, 3.99903928e-04,\n",
       "        1.55246931e-06, 3.97253967e-04, 8.99694551e-07, 4.87667043e-04,\n",
       "        4.90855323e-04, 1.87488587e-06, 3.94890515e-04, 4.01799146e-04,\n",
       "        4.87065548e-04, 4.00350456e-04, 1.83194854e-06, 4.88911233e-04,\n",
       "        1.45102730e-06, 4.00114102e-04, 9.60800251e-07, 4.00161885e-04,\n",
       "        4.89706807e-04, 4.00093297e-04, 3.98925301e-04, 4.90956123e-04,\n",
       "        4.87339254e-04, 4.01455051e-04, 3.38856468e-06, 4.91490551e-04,\n",
       "        3.99661160e-04, 4.71939797e-04, 3.99637293e-04, 3.99446502e-04,\n",
       "        4.90018415e-04, 4.89649992e-04, 4.89784744e-04, 4.90125432e-04,\n",
       "        4.88349441e-04, 4.87549674e-04, 3.16082257e-06, 3.49102120e-06,\n",
       "        3.98413012e-04, 5.19161588e-06, 4.03053292e-04, 4.01124855e-04,\n",
       "        4.86184275e-04, 4.93043336e-04, 4.26869141e-06, 4.89434296e-04,\n",
       "        3.19017957e-06, 4.88417265e-04, 4.01661651e-04, 4.90471535e-04,\n",
       "        3.16297988e-07, 4.89843034e-04, 4.89687269e-04, 3.99614472e-04,\n",
       "        4.00695925e-04, 3.99804410e-04, 4.91078326e-04, 4.16500174e-04,\n",
       "        4.89731225e-04, 4.70935540e-06, 6.64157308e-07, 1.13443158e-06,\n",
       "        1.80317596e-06, 4.85714952e-04, 4.92407858e-04, 4.01204970e-04,\n",
       "        3.97145002e-04, 3.97813310e-04, 3.97087282e-04, 4.88949050e-04,\n",
       "        4.93098354e-04, 4.00114074e-04, 4.70017301e-06, 3.99737439e-04,\n",
       "        3.96445998e-04, 4.88704492e-04, 4.89882130e-04, 4.00660515e-04,\n",
       "        4.53503314e-04, 4.86495596e-04, 4.94892046e-04, 4.89648413e-04,\n",
       "        4.89921335e-04, 4.89162078e-04, 4.87010586e-04, 4.89590680e-04,\n",
       "        3.87384339e-07, 3.56832255e-07, 5.76164530e-07, 4.00042545e-04,\n",
       "        4.89045045e-04, 3.99780700e-04, 4.89765167e-04, 4.89648552e-04,\n",
       "        4.89862488e-04, 4.89648413e-04, 5.11217471e-06, 3.99900972e-04,\n",
       "        4.91117213e-04, 3.67999675e-06, 1.30063836e-06, 4.72886730e-04,\n",
       "        4.00535680e-04, 4.90135889e-04, 4.90697377e-04, 3.99544032e-04,\n",
       "        3.95253913e-04, 6.30185957e-04, 4.85640173e-04, 4.91977426e-04,\n",
       "        4.42200589e-07, 3.97735376e-04, 4.00001426e-04, 3.97907520e-04,\n",
       "        2.68641777e-06, 2.78041453e-07, 4.00018763e-04, 4.90899568e-04,\n",
       "        3.99755925e-04, 4.02242796e-04, 4.01606296e-04, 4.91483847e-04,\n",
       "        5.79509205e-06, 3.99797876e-04, 2.57050170e-06, 4.72526136e-06,\n",
       "        1.54806929e-06, 4.98152215e-06, 4.03458296e-04, 4.36872365e-06,\n",
       "        4.89709918e-04, 7.67189348e-06, 4.01248011e-04, 4.78360603e-06,\n",
       "        3.77575548e-06, 3.97086183e-04, 2.91066026e-06, 4.89611267e-04,\n",
       "        4.92875628e-04, 4.31162054e-06, 3.99973669e-04, 4.04437233e-04,\n",
       "        3.99899579e-04, 3.87384339e-07, 3.62333493e-06, 4.39725566e-06,\n",
       "        3.95870237e-04, 9.95665217e-07, 6.24565671e-06, 4.91896079e-04,\n",
       "        4.91721187e-04, 1.78416128e-07, 2.73672959e-06, 4.88240317e-04,\n",
       "        4.52970203e-06, 3.99971037e-04, 4.93320722e-04, 5.36817843e-06,\n",
       "        2.41293737e-05, 2.83146531e-06, 3.73519059e-06, 4.90411854e-04,\n",
       "        6.56305333e-06, 4.89882014e-04, 4.88844296e-04, 4.90135146e-04,\n",
       "        4.00066575e-04, 2.03090090e-06, 4.01553928e-04, 4.90037694e-04,\n",
       "        7.91118688e-06, 1.08735602e-06, 3.66389708e-06, 4.00853338e-04,\n",
       "        4.95642869e-04, 4.02331589e-04, 4.94896085e-04, 4.88278586e-04,\n",
       "        4.00044449e-04, 5.51978917e-07, 4.90115778e-04, 2.87845388e-06,\n",
       "        3.25369303e-06, 4.90770269e-04, 3.98696565e-04, 4.89240813e-04,\n",
       "        9.65521657e-07, 3.56002931e-06, 4.13778174e-06, 3.99804183e-04,\n",
       "        4.89804070e-04, 4.90076589e-04, 4.91755935e-04, 4.90993862e-04,\n",
       "        4.00667994e-04, 4.03332918e-04, 3.99449974e-04, 4.87419188e-04,\n",
       "        4.88812141e-04, 4.90141595e-04, 4.88465096e-04, 6.83991283e-06,\n",
       "        4.91710593e-04, 4.09747144e-06, 2.73672959e-06, 3.98631626e-04,\n",
       "        3.99833488e-04, 3.52149972e-06, 6.46813391e-07, 4.91353055e-04,\n",
       "        4.92869124e-04, 4.03255171e-04, 4.00052974e-04, 4.88350829e-04,\n",
       "        3.99718628e-04, 4.00186476e-04, 8.42046358e-06, 4.88507127e-04,\n",
       "        4.89337610e-04, 4.45428186e-06, 3.66389708e-06, 3.97344716e-04,\n",
       "        5.89892086e-06, 1.32831462e-06, 4.86834705e-04, 4.91676350e-04,\n",
       "        6.37429806e-06, 4.00161800e-04, 3.99923509e-04, 4.88852896e-04,\n",
       "        4.00694251e-04, 1.16800773e-07, 5.43678010e-07, 3.99613590e-04,\n",
       "        3.99708901e-04, 4.89745733e-04, 4.89862488e-04, 4.89196269e-04,\n",
       "        4.00650867e-04, 5.24087206e-06, 4.37132516e-06, 4.81015023e-06,\n",
       "        1.16800773e-07, 1.40160927e-06, 3.99971150e-04, 3.30568886e-06,\n",
       "        4.00114187e-04, 3.99565793e-04, 4.00185709e-04, 4.89648343e-04,\n",
       "        4.90468614e-04, 3.99780302e-04, 6.10649513e-07, 3.99851895e-04,\n",
       "        3.01578299e-07, 3.98950589e-07, 2.43140197e-07, 3.99051471e-04,\n",
       "        1.16800773e-07, 2.13461202e-06, 4.00114102e-04, 3.99899579e-04,\n",
       "        3.81469727e-07, 3.99732632e-04, 3.99828028e-04, 4.00018763e-04,\n",
       "        3.98950589e-07, 7.67159711e-06, 6.53701832e-06, 4.04082077e-04,\n",
       "        3.99257475e-04, 3.96403692e-04, 4.90616276e-04, 4.03223510e-04,\n",
       "        4.89863091e-04, 4.89629304e-04, 3.99583410e-04, 3.99436318e-04,\n",
       "        1.16279848e-05, 1.40969710e-06, 4.02832059e-04, 4.89940432e-04,\n",
       "        3.99338506e-04, 4.90232532e-04, 4.67203091e-07, 3.99947291e-04,\n",
       "        3.99804780e-04, 4.91950971e-04, 4.02040731e-04, 3.98052453e-04,\n",
       "        4.01321246e-04, 7.37419068e-06, 9.46494734e-07, 4.90458313e-04,\n",
       "        3.86385242e-06, 4.03071310e-04, 4.00167971e-04, 4.88857455e-04,\n",
       "        3.99169001e-04, 4.88114738e-04, 3.99899948e-04, 4.89844288e-04,\n",
       "        3.92225737e-06, 4.01015299e-04, 4.23446432e-06, 3.99973566e-04,\n",
       "        3.98761356e-04, 3.98812147e-04, 1.02742641e-05, 4.88061422e-04,\n",
       "        3.08067804e-06, 4.90962222e-04, 4.91572557e-04, 4.87630441e-04,\n",
       "        6.81059687e-07, 3.81469727e-07, 3.99200044e-04, 3.99756784e-04,\n",
       "        4.03261255e-04, 3.99971037e-04, 4.90193437e-04, 4.00567292e-04,\n",
       "        4.00042573e-04, 4.89881968e-04, 3.99995145e-04, 4.91731485e-04,\n",
       "        4.85984080e-04, 4.88214297e-04, 2.98318716e-06, 4.89726396e-04,\n",
       "        4.89998769e-04, 4.00185709e-04, 4.88931988e-04, 4.88215788e-04,\n",
       "        3.96847764e-04, 4.91718320e-04, 4.58031889e-04, 4.88467861e-04,\n",
       "        3.02180853e-06, 4.93570003e-04, 4.00067853e-04, 4.89804209e-04,\n",
       "        3.99851838e-04, 4.89784721e-04, 3.96136096e-04, 5.65006761e-06,\n",
       "        4.91659647e-04, 4.90308874e-04, 4.01556091e-04, 4.90291947e-04,\n",
       "        2.78041453e-06, 1.07049677e-06, 4.00114187e-04, 3.99947234e-04,\n",
       "        1.90734863e-07, 4.89979242e-04, 4.90135007e-04, 3.97213035e-04,\n",
       "        4.90475869e-04, 4.89820001e-04, 4.01332894e-04, 4.90389835e-04,\n",
       "        3.99828000e-04, 2.13248060e-07, 1.78416128e-07, 4.00185795e-04,\n",
       "        3.16297988e-07, 3.87384339e-07, 4.90485572e-04, 4.00161772e-04,\n",
       "        3.99732661e-04, 4.89726280e-04, 3.99995230e-04, 4.00305093e-04,\n",
       "        1.24891289e-06, 6.64157308e-07, 4.12402121e-06, 4.90524111e-04,\n",
       "        3.86914497e-06, 4.36664132e-06, 4.01937369e-04, 4.91382498e-04,\n",
       "        4.89923052e-04, 4.91853555e-04, 4.85595698e-04, 3.97354015e-04,\n",
       "        3.98208992e-06, 5.99375014e-06, 7.74827370e-06, 4.03206435e-04,\n",
       "        7.44843452e-07, 5.28106571e-06, 4.91110546e-04, 1.98675791e-06,\n",
       "        4.90037810e-04, 4.90313228e-04, 4.87635244e-04, 4.89745710e-04]),\n",
       " 'param_learning_rate': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_loss': masked_array(data=['squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'squared_error', 'squared_error', 'squared_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'absolute_error', 'absolute_error', 'absolute_error',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'huber', 'huber', 'huber', 'huber', 'huber', 'huber',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile',\n",
       "                    'quantile', 'quantile', 'quantile', 'quantile'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500,\n",
       "                    10, 10, 10, 50, 50, 50, 100, 100, 100, 500, 500, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_random_state': masked_array(data=[50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_subsample': masked_array(data=[0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0,\n",
       "                    0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7,\n",
       "                    1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5, 0.7, 1.0, 0.5,\n",
       "                    0.7, 1.0, 0.5, 0.7, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.0001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.001,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.01,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 0.1,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'squared_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'absolute_error',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'huber',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 3,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 7,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 10,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 50,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 100,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.5},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 0.7},\n",
       "  {'learning_rate': 1.0,\n",
       "   'loss': 'quantile',\n",
       "   'max_depth': 9,\n",
       "   'n_estimators': 500,\n",
       "   'random_state': 50,\n",
       "   'subsample': 1.0}],\n",
       " 'split0_test_score': array([ -0.34758196,  -0.34774987,  -0.34795931,  -0.34566382,\n",
       "         -0.34590931,  -0.34673715,  -0.34351755,  -0.34409845,\n",
       "         -0.34525667,  -0.32696471,  -0.32730102,  -0.33521982,\n",
       "         -0.34738039,  -0.34793411,  -0.34914889,  -0.3446837 ,\n",
       "         -0.34634319,  -0.35261907,  -0.34221518,  -0.34542527,\n",
       "         -0.35709225,  -0.32556039,  -0.33483765,  -0.39464682,\n",
       "         -0.34732197,  -0.34800265,  -0.34913971,  -0.34446755,\n",
       "         -0.34678339,  -0.35263671,  -0.34207129,  -0.34587952,\n",
       "         -0.35703679,  -0.32513061,  -0.33529136,  -0.39430933,\n",
       "         -0.2649428 ,  -0.26474242,  -0.26421284,  -0.2641933 ,\n",
       "         -0.26308187,  -0.25990429,  -0.26212535,  -0.2606618 ,\n",
       "         -0.25458361,  -0.24713676,  -0.24020288,  -0.22550278,\n",
       "         -0.26491041,  -0.26495898,  -0.26429198,  -0.26394055,\n",
       "         -0.26430967,  -0.26030072,  -0.26234545,  -0.26286558,\n",
       "         -0.25537834,  -0.2529467 ,  -0.25388809,  -0.24963516,\n",
       "         -0.26491041,  -0.26506926,  -0.26510833,  -0.26399126,\n",
       "         -0.26447315,  -0.26438813,  -0.26217463,  -0.26295951,\n",
       "         -0.26356696,  -0.25302096,  -0.25475553,  -0.27387415,\n",
       "         -0.2647421 ,  -0.26476454,  -0.2650671 ,  -0.26312981,\n",
       "         -0.26329337,  -0.26371083,  -0.26147006,  -0.26193003,\n",
       "         -0.26202658,  -0.24791453,  -0.24800226,  -0.25044585,\n",
       "         -0.26453323,  -0.26494336,  -0.26610023,  -0.26255135,\n",
       "         -0.26375544,  -0.26941286,  -0.26077122,  -0.26279778,\n",
       "         -0.27358826,  -0.24664663,  -0.25518046,  -0.30957844,\n",
       "         -0.264472  ,  -0.2648476 ,  -0.26614373,  -0.26192967,\n",
       "         -0.26372537,  -0.26958978,  -0.26000083,  -0.26265879,\n",
       "         -0.2739409 ,  -0.24598257,  -0.25461733,  -0.31141445,\n",
       "         -1.10833432,  -1.10821513,  -1.10849024,  -1.10509386,\n",
       "         -1.10500807,  -1.10470835,  -1.10076557,  -1.09984625,\n",
       "         -1.10003793,  -1.06680463,  -1.06403796,  -1.06381025,\n",
       "         -1.10816883,  -1.10811719,  -1.10838379,  -1.10448162,\n",
       "         -1.10423305,  -1.10421413,  -1.0992998 ,  -1.09875316,\n",
       "         -1.09903411,  -1.06266062,  -1.05892948,  -1.05905354,\n",
       "         -1.10816883,  -1.10811719,  -1.10838379,  -1.10448162,\n",
       "         -1.1042276 ,  -1.10421413,  -1.0992998 ,  -1.09874777,\n",
       "         -1.09903411,  -1.06266083,  -1.05890619,  -1.05905354,\n",
       "         -0.34145195,  -0.34289932,  -0.34525536,  -0.32327238,\n",
       "         -0.32533575,  -0.33521504,  -0.30920415,  -0.30877259,\n",
       "         -0.32299989,  -0.21786153,  -0.2064875 ,  -0.26769504,\n",
       "         -0.33853185,  -0.34493962,  -0.35682252,  -0.31195454,\n",
       "         -0.32889281,  -0.39374572,  -0.29493152,  -0.32165174,\n",
       "         -0.44529891,  -0.22236406,  -0.30281235,  -0.94842495,\n",
       "         -0.33943807,  -0.34571296,  -0.35717047,  -0.31204608,\n",
       "         -0.33368311,  -0.39444451,  -0.29476482,  -0.32713649,\n",
       "         -0.44544778,  -0.21631611,  -0.31526551,  -0.95785828,\n",
       "         -0.2618367 ,  -0.25983028,  -0.25457885,  -0.2558496 ,\n",
       "         -0.24625061,  -0.22523988,  -0.23822594,  -0.22852604,\n",
       "         -0.19935151,  -0.21038854,  -0.20581192,  -0.0891804 ,\n",
       "         -0.26151534,  -0.26200746,  -0.25537394,  -0.25366173,\n",
       "         -0.25749556,  -0.2497333 ,  -0.24039953,  -0.25331846,\n",
       "         -0.26472993,  -0.22956635,  -0.28382988,  -0.4446977 ,\n",
       "         -0.26151561,  -0.26311478,  -0.26356626,  -0.25414749,\n",
       "         -0.25886866,  -0.2732938 ,  -0.24059187,  -0.25303614,\n",
       "         -0.29862472,  -0.22582132,  -0.29008226,  -0.54793239,\n",
       "         -0.25980986,  -0.25974924,  -0.26214015,  -0.24411112,\n",
       "         -0.24552512,  -0.25047837,  -0.23184162,  -0.23455823,\n",
       "         -0.24038692,  -0.16702895,  -0.16442749,  -0.23714197,\n",
       "         -0.25787615,  -0.26180944,  -0.2734628 ,  -0.23897406,\n",
       "         -0.25126255,  -0.30953101,  -0.22298109,  -0.24134082,\n",
       "         -0.36025704,  -0.16248159,  -0.24083131,  -0.88362637,\n",
       "         -0.25628635,  -0.26077983,  -0.27397327,  -0.23478035,\n",
       "         -0.25245207,  -0.31125423,  -0.22132518,  -0.24353835,\n",
       "         -0.3625107 ,  -0.15827787,  -0.24542744,  -0.71832711,\n",
       "         -1.09849431,  -1.0972944 ,  -1.1000539 ,  -1.06727085,\n",
       "         -1.06602729,  -1.06370576,  -1.02722028,  -1.01702142,\n",
       "         -1.01476617,  -0.60697734,  -0.56968769,  -0.5670952 ,\n",
       "         -1.09684992,  -1.0963355 ,  -1.09900543,  -1.06152294,\n",
       "         -1.05879999,  -1.05909622,  -1.01371655,  -1.00789475,\n",
       "         -1.00425251,  -0.58111484,  -0.51606984,  -0.47270823,\n",
       "         -1.09684992,  -1.0963355 ,  -1.09900543,  -1.06152294,\n",
       "         -1.05874817,  -1.05909622,  -1.01371655,  -1.00784746,\n",
       "         -1.00425251,  -0.5810456 ,  -0.51832691,  -0.47270823,\n",
       "         -0.29569395,  -0.30229075,  -0.32357088,  -0.19236777,\n",
       "         -0.17479558,  -0.27431272,  -0.16652423,  -0.10766294,\n",
       "         -0.21560859,  -0.18395135,  -0.03620201,   0.01369094,\n",
       "         -0.27397781,  -0.31413441,  -0.44470275,  -0.14165993,\n",
       "         -0.27122757,  -0.95226043,  -0.11490587,  -0.24113363,\n",
       "         -1.58941833,  -0.31903923,  -0.35099196,  -2.68847714,\n",
       "         -0.27368721,  -0.31661253,  -0.44487098,  -0.14304641,\n",
       "         -0.2704457 ,  -0.96404138,  -0.10961745,  -0.23479961,\n",
       "         -1.60050344,  -0.26826437,  -0.30392657,  -2.93793482,\n",
       "         -0.23789336,  -0.21932566,  -0.19524232,  -0.22670368,\n",
       "         -0.20504892,  -0.12266622,  -0.36308875,  -0.25188062,\n",
       "         -0.19640013,  -0.67355731,  -0.62728786,  -0.8148703 ,\n",
       "         -0.23389424,  -0.22754248,  -0.25587927,  -0.20826359,\n",
       "         -0.23690612,  -0.49115784,  -0.24191206,  -0.3229624 ,\n",
       "         -0.61315763,  -0.59441421,  -0.72170226,  -0.94153882,\n",
       "         -0.23391515,  -0.22762395,  -0.29437422,  -0.21384961,\n",
       "         -0.24138817,  -0.49136852,  -0.26448325,  -0.25758124,\n",
       "         -0.60351658,  -0.5761226 ,  -0.57785774,  -0.91680296,\n",
       "         -0.21520034,  -0.21470061,  -0.24090314,  -0.13858457,\n",
       "         -0.12925258,  -0.24063028,  -0.15172073,  -0.09419309,\n",
       "         -0.20676836,  -0.21195083,  -0.04826078,  -0.07059479,\n",
       "         -0.19617045,  -0.2307726 ,  -0.35847435,  -0.10452167,\n",
       "         -0.22500602,  -0.8811764 ,  -0.09744125,  -0.24284774,\n",
       "         -1.54546119,  -0.30584771,  -0.41562788,  -2.54497999,\n",
       "         -0.20010961,  -0.21847822,  -0.36481453,  -0.10705822,\n",
       "         -0.20688409,  -0.6482974 ,  -0.10435582,  -0.25397064,\n",
       "         -0.93692019,  -0.31417281,  -0.42975782,  -2.0706302 ,\n",
       "         -1.00544609,  -0.99272927,  -1.02112479,  -0.62531195,\n",
       "         -0.54792789,  -0.55519648,  -0.24520412,  -0.17669897,\n",
       "         -0.38193516,   0.18554921,   0.31195151,   0.22286876,\n",
       "         -0.99000974,  -0.98510773,  -1.01153582,  -0.58074111,\n",
       "         -0.49380658,  -0.475022  ,  -0.25560319,  -0.14325574,\n",
       "         -0.12620482,  -0.03843244,   0.12366761,   0.2395652 ,\n",
       "         -0.99000974,  -0.98510773,  -1.01153582,  -0.58074111,\n",
       "         -0.49709621,  -0.475022  ,  -0.25491416,  -0.1446932 ,\n",
       "         -0.12623355,   0.02448083,   0.09675354,   0.21184871,\n",
       "         -0.08272875,  -0.09758826,  -0.14235787,  -0.06348717,\n",
       "         -0.16117629,   0.01440176,  -0.10008764,  -0.26048349,\n",
       "         -0.05501512,  -0.22426619,  -0.34474619,  -0.12827106,\n",
       "         -0.02431677,  -0.15164087,  -1.60584889,  -0.23099343,\n",
       "         -0.49108871,  -2.65203671,  -0.2823424 ,  -0.52589145,\n",
       "         -2.65801873,  -0.30540864,  -0.51757249,  -2.65811844,\n",
       "          0.06523142,  -0.31709391,  -1.69604209,  -0.17890945,\n",
       "         -0.58255018,  -3.01749442,  -0.26901414,  -0.58445578,\n",
       "         -3.03317951,  -0.29751121,  -0.58781692,  -3.03318765,\n",
       "         -0.20147292,  -0.28048514,   0.01265378,  -0.70873064,\n",
       "         -1.0680126 ,  -0.47782979,  -0.57849228,  -0.96566159,\n",
       "         -0.45842692,  -0.52251968,  -0.69673739,  -0.47613307,\n",
       "         -0.08255683,  -0.43144783,  -0.50787513,  -0.32440974,\n",
       "         -0.85520723,  -0.78355302,  -0.40585662,  -0.94844274,\n",
       "         -0.78912447,  -0.41032588,  -0.88703575,  -0.76577716,\n",
       "         -0.08255683,  -0.38565402,  -0.46712385,  -0.54291035,\n",
       "         -0.72640455,  -0.83117709,  -0.60193464,  -0.77323243,\n",
       "         -0.83553764,  -0.61294478,  -0.7964032 ,  -0.81864348,\n",
       "         -0.20242529,  -0.11625046,  -0.25093064,  -0.30402593,\n",
       "         -0.03755014,  -0.17873565,  -0.35320267,  -0.10426329,\n",
       "         -0.22950597,  -0.41072558,  -0.1754513 ,  -0.30243764,\n",
       "         -0.13578205,  -0.33248164,  -1.61169423,  -0.41152364,\n",
       "         -0.55742658,  -2.59923521,  -0.46969124,  -0.59726054,\n",
       "         -2.60482031,  -0.48356529,  -0.59498195,  -2.60584065,\n",
       "         -0.1033606 ,  -0.13041146,  -1.29942224,  -0.49658181,\n",
       "         -0.51538481,  -2.42800999,  -0.53949258,  -0.54249818,\n",
       "         -2.45577801,  -0.55304331,  -0.54082334,  -2.4558924 ,\n",
       "         -0.14958855,  -0.1053546 ,  -0.28539866,  -0.05375064,\n",
       "          0.0652402 ,   0.30928373,  -0.2553469 ,   0.08177149,\n",
       "          0.22513109,  -0.50852486,  -0.19082667,  -0.50669753,\n",
       "         -0.06481207,  -0.12864364,  -0.05570506,   0.22419917,\n",
       "          0.10689404,   0.21635145,   0.15786807,   0.0363917 ,\n",
       "          0.12046687,  -0.01795957,  -0.07916234,  -0.24644828,\n",
       "         -0.06481207,  -0.12885012,  -0.05352089,   0.22259112,\n",
       "          0.10933523,   0.24572907,  -0.03021046,  -0.01490615,\n",
       "         -0.15642546,  -0.19693571,  -0.16142885,  -0.78107369,\n",
       "         -1.58000067,   0.30682881,  -1.19223725,  -4.05497347,\n",
       "          0.38036558,  -1.289226  ,  -4.49187151,   0.4468335 ,\n",
       "         -1.2890329 ,  -6.70156097,   0.31927289,  -1.28903285,\n",
       "         -2.97772038,  -0.439295  ,  -2.76840076,  -3.12303256,\n",
       "         -0.39260786,  -2.76840076,  -3.58802072,  -0.36050379,\n",
       "         -2.76840076,  -4.48333703,  -0.95410019,  -2.76840076,\n",
       "         -2.31966745,  -0.42419376,  -3.1122944 ,  -2.98897403,\n",
       "         -0.49480889,  -3.1122944 ,  -4.38120613,  -0.48343087,\n",
       "         -3.1122944 ,  -5.62596362,  -1.0623309 ,  -3.1122944 ,\n",
       "         -2.08309873,  -1.47013396,  -1.5949766 ,  -1.55613992,\n",
       "         -1.43939896,  -1.5228638 ,  -1.8367463 ,  -1.46478427,\n",
       "         -1.5228638 ,  -2.20047063,  -1.31061371,  -1.5228638 ,\n",
       "         -2.92122514,  -1.31977798,  -1.30002581,  -5.1377255 ,\n",
       "         -1.17634137,  -1.30002581,  -6.25935774,  -1.17634137,\n",
       "         -1.30002581,  -6.29451743,  -1.17634137,  -1.30002581,\n",
       "         -2.94774881,  -2.77239306,  -3.17333557,  -2.94026286,\n",
       "         -2.94014272,  -3.17333557,  -4.34792132,  -2.94014272,\n",
       "         -3.17333557,  -4.10769604,  -2.94014272,  -3.17333557,\n",
       "         -0.65916559,  -0.49129174,  -0.96147716,  -1.56358422,\n",
       "         -0.52981804,  -0.97652369,  -1.25020932,  -0.53092062,\n",
       "         -0.97660295,  -1.46171857,  -0.99494936,  -0.97660288,\n",
       "         -6.13596878,  -0.05620403,  -3.37636569,  -6.85558222,\n",
       "         -0.12917649,  -3.37636569,  -6.99041032,  -0.12917649,\n",
       "         -3.37636569,  -6.85274231,  -0.12917649,  -3.37636569,\n",
       "         -4.63748062,  -0.16326584,  -2.94457657,  -5.54838538,\n",
       "         -0.08038367,  -2.94457657,  -5.54838538,  -0.08038367,\n",
       "         -2.94457657,  -5.54838538,  -0.08038367,  -2.94457657,\n",
       "          0.28334783,  -0.70871951,   0.12469525,   0.28334783,\n",
       "         -0.70871951,   0.12469525,   0.28334783,  -0.70871951,\n",
       "          0.12469525,   0.43883186,  -0.81469051,   0.12469525,\n",
       "         -1.93711529,  -3.91534373,  -0.29397292,  -1.93711529,\n",
       "        -10.280063  ,  -0.29397292,  -1.93711529, -10.280063  ,\n",
       "         -0.29397292,  -1.93711529, -10.280063  ,  -0.29397292,\n",
       "         -2.6264585 ,  -3.91534373,  -0.29397292,  -2.6264585 ,\n",
       "        -10.280063  ,  -0.29397292,  -2.6264585 , -10.280063  ,\n",
       "         -0.29397292,  -2.6264585 , -10.280063  ,  -0.29397292]),\n",
       " 'split1_test_score': array([-2.64066777e-01, -2.63867872e-01, -2.63805114e-01, -2.58626424e-01,\n",
       "        -2.58356045e-01, -2.57356906e-01, -2.52079232e-01, -2.51718446e-01,\n",
       "        -2.49399118e-01, -2.01659120e-01, -2.00144660e-01, -1.90063412e-01,\n",
       "        -2.64041020e-01, -2.63618243e-01, -2.63981530e-01, -2.58084011e-01,\n",
       "        -2.57593657e-01, -2.57890951e-01, -2.51136678e-01, -2.50290193e-01,\n",
       "        -2.50440612e-01, -1.97176871e-01, -1.94641816e-01, -1.94895466e-01,\n",
       "        -2.64009842e-01, -2.63690445e-01, -2.64099160e-01, -2.58148443e-01,\n",
       "        -2.57838086e-01, -2.58448684e-01, -2.51347401e-01, -2.50506683e-01,\n",
       "        -2.51601804e-01, -1.97167509e-01, -1.95306392e-01, -2.00545011e-01,\n",
       "        -2.71011876e-01, -2.70848535e-01, -2.70880940e-01, -2.67824262e-01,\n",
       "        -2.67376873e-01, -2.67222595e-01, -2.63574126e-01, -2.63512264e-01,\n",
       "        -2.62690550e-01, -2.27698557e-01, -2.28014159e-01, -2.22283701e-01,\n",
       "        -2.70985957e-01, -2.70729462e-01, -2.70978273e-01, -2.67469338e-01,\n",
       "        -2.67069996e-01, -2.67657493e-01, -2.63071921e-01, -2.62899454e-01,\n",
       "        -2.62979332e-01, -2.24224898e-01, -2.24600193e-01, -2.27142654e-01,\n",
       "        -2.70985957e-01, -2.70729462e-01, -2.70978273e-01, -2.67469329e-01,\n",
       "        -2.67069996e-01, -2.67657493e-01, -2.63071893e-01, -2.62859963e-01,\n",
       "        -2.62853434e-01, -2.24298854e-01, -2.24460271e-01, -2.26689215e-01,\n",
       "        -2.70283700e-01, -2.70301249e-01, -2.70273203e-01, -2.64930024e-01,\n",
       "        -2.64721973e-01, -2.64070744e-01, -2.58107196e-01, -2.57963691e-01,\n",
       "        -2.56289155e-01, -2.07497081e-01, -2.06705567e-01, -1.96866023e-01,\n",
       "        -2.70324366e-01, -2.70085922e-01, -2.70263605e-01, -2.64364631e-01,\n",
       "        -2.64039486e-01, -2.64163654e-01, -2.57139755e-01, -2.56868052e-01,\n",
       "        -2.56625388e-01, -2.02539451e-01, -2.00632420e-01, -2.00794288e-01,\n",
       "        -2.70341522e-01, -2.70097121e-01, -2.70263809e-01, -2.64421154e-01,\n",
       "        -2.64096075e-01, -2.64369582e-01, -2.57150043e-01, -2.56714338e-01,\n",
       "        -2.57068591e-01, -2.02947599e-01, -2.00955614e-01, -2.03002915e-01,\n",
       "        -2.92886424e+00, -2.92883111e+00, -2.92952338e+00, -2.92639849e+00,\n",
       "        -2.92617648e+00, -2.92820700e+00, -2.92264707e+00, -2.92225478e+00,\n",
       "        -2.92659117e+00, -2.89412707e+00, -2.89335821e+00, -2.91397309e+00,\n",
       "        -2.92883264e+00, -2.92879828e+00, -2.92947388e+00, -2.92604237e+00,\n",
       "        -2.92586497e+00, -2.92795878e+00, -2.92182908e+00, -2.92167192e+00,\n",
       "        -2.92608320e+00, -2.88980336e+00, -2.89017173e+00, -2.91140973e+00,\n",
       "        -2.92883264e+00, -2.92879828e+00, -2.92947388e+00, -2.92604244e+00,\n",
       "        -2.92586163e+00, -2.92795878e+00, -2.92182954e+00, -2.92165113e+00,\n",
       "        -2.92608320e+00, -2.88972290e+00, -2.89002757e+00, -2.91140973e+00,\n",
       "        -2.51714582e-01, -2.50046782e-01, -2.49268553e-01, -1.99236579e-01,\n",
       "        -1.98592653e-01, -1.89807055e-01, -1.40458579e-01, -1.40864868e-01,\n",
       "        -1.26252559e-01,  1.67835407e-01,  1.60814783e-01,  1.28105734e-01,\n",
       "        -2.51399921e-01, -2.47635853e-01, -2.49793422e-01, -1.94987939e-01,\n",
       "        -1.91779680e-01, -1.94850675e-01, -1.35349019e-01, -1.29497287e-01,\n",
       "        -1.34222946e-01,  1.88620844e-01,  1.74992529e-01,  7.43529202e-02,\n",
       "        -2.51649365e-01, -2.48940544e-01, -2.51099928e-01, -1.94477515e-01,\n",
       "        -1.93815119e-01, -2.00384259e-01, -1.34086593e-01, -1.31932558e-01,\n",
       "        -1.44325690e-01,  1.87079022e-01,  1.73174718e-01,  6.16512563e-02,\n",
       "        -2.63969121e-01, -2.62354353e-01, -2.62688644e-01, -2.31488376e-01,\n",
       "        -2.27187427e-01, -2.22345240e-01, -1.88227375e-01, -1.84864363e-01,\n",
       "        -1.76822544e-01,  8.26890760e-02,  9.14015138e-02,  7.97669607e-02,\n",
       "        -2.63713706e-01, -2.61171807e-01, -2.62945180e-01, -2.27513609e-01,\n",
       "        -2.24556169e-01, -2.27141889e-01, -1.79433004e-01, -1.79829272e-01,\n",
       "        -1.86061885e-01,  1.05704910e-01,  1.03394574e-01,  6.92552118e-02,\n",
       "        -2.63713706e-01, -2.61171807e-01, -2.62945810e-01, -2.27391729e-01,\n",
       "        -2.25399340e-01, -2.25698825e-01, -1.79319133e-01, -1.80767058e-01,\n",
       "        -1.82989519e-01,  1.07003973e-01,  1.03081999e-01,  8.80557721e-02,\n",
       "        -2.56959848e-01, -2.56855833e-01, -2.56417898e-01, -2.05165719e-01,\n",
       "        -2.04686752e-01, -1.96947609e-01, -1.45158467e-01, -1.46762585e-01,\n",
       "        -1.32845692e-01,  1.63432198e-01,  1.47655561e-01,  1.11989480e-01,\n",
       "        -2.57847129e-01, -2.55718283e-01, -2.56856987e-01, -2.02480449e-01,\n",
       "        -1.99349661e-01, -2.01346166e-01, -1.41373043e-01, -1.37898693e-01,\n",
       "        -1.40416700e-01,  1.84343031e-01,  1.70803901e-01,  7.34592455e-02,\n",
       "        -2.57187498e-01, -2.55816850e-01, -2.57227503e-01, -2.01525068e-01,\n",
       "        -2.00384977e-01, -2.03646971e-01, -1.39453604e-01, -1.39714431e-01,\n",
       "        -1.45547245e-01,  1.84023848e-01,  1.66002713e-01,  7.44470768e-02,\n",
       "        -2.92002810e+00, -2.91970194e+00, -2.92662294e+00, -2.89589006e+00,\n",
       "        -2.89350628e+00, -2.91392770e+00, -2.85980643e+00, -2.85558033e+00,\n",
       "        -2.89917608e+00, -2.48560740e+00, -2.43050224e+00, -2.29806711e+00,\n",
       "        -2.91972213e+00, -2.91938389e+00, -2.92612873e+00, -2.89248719e+00,\n",
       "        -2.89066057e+00, -2.91146933e+00, -2.85252379e+00, -2.85077121e+00,\n",
       "        -2.89419890e+00, -2.44145023e+00, -2.38331889e+00, -2.26549585e+00,\n",
       "        -2.91972213e+00, -2.91938389e+00, -2.92612873e+00, -2.89249385e+00,\n",
       "        -2.89063010e+00, -2.91146933e+00, -2.85256775e+00, -2.85058038e+00,\n",
       "        -2.89419890e+00, -2.43784596e+00, -2.38289823e+00, -2.27912309e+00,\n",
       "        -1.40282048e-01, -1.26953334e-01, -1.23873096e-01,  1.61261128e-01,\n",
       "         1.78879370e-01,  1.29398831e-01,  3.13782098e-01,  3.07980023e-01,\n",
       "         2.58257892e-01,  3.45846818e-01,  2.81349658e-01,  1.98392487e-01,\n",
       "        -1.34661407e-01, -1.13285215e-01, -1.32023061e-01,  1.93414681e-01,\n",
       "         1.82340294e-01,  7.23821640e-02,  3.57625466e-01,  2.87136563e-01,\n",
       "         3.83830748e-02,  3.31195290e-01,  1.95037015e-01, -3.29920595e-01,\n",
       "        -1.30518198e-01, -1.15497373e-01, -1.43998254e-01,  1.96738259e-01,\n",
       "         1.76566943e-01,  5.95475501e-02,  3.68526422e-01,  2.72556760e-01,\n",
       "         1.04139232e-02,  3.46874833e-01,  1.65476292e-01, -4.12961052e-01,\n",
       "        -1.83101983e-01, -1.97256171e-01, -1.79807918e-01,  7.94657596e-02,\n",
       "         6.81374442e-02,  7.46857977e-02,  2.63305675e-01,  2.39282259e-01,\n",
       "         2.42293836e-01,  4.26458661e-01,  3.98473168e-01,  2.96042565e-01,\n",
       "        -1.80792448e-01, -1.69134765e-01, -1.94466260e-01,  9.25879059e-02,\n",
       "         9.34131178e-02,  7.01411256e-02,  3.04321555e-01,  2.71225774e-01,\n",
       "         2.57819852e-01,  4.62625569e-01,  3.63897926e-01,  3.35446779e-01,\n",
       "        -1.80792448e-01, -1.69134765e-01, -1.94524850e-01,  9.12353241e-02,\n",
       "         9.91621907e-02,  8.08162102e-02,  3.02233314e-01,  3.09803932e-01,\n",
       "         2.52768169e-01,  4.53302119e-01,  3.97545385e-01,  2.90174479e-01,\n",
       "        -1.39425546e-01, -1.34963707e-01, -1.32718017e-01,  1.51245718e-01,\n",
       "         1.56902058e-01,  1.13899964e-01,  3.22345197e-01,  2.97823072e-01,\n",
       "         2.24590477e-01,  3.40831148e-01,  2.93122972e-01,  1.97101888e-01,\n",
       "        -1.37307858e-01, -1.26696030e-01, -1.41131665e-01,  1.82001935e-01,\n",
       "         1.71193034e-01,  6.88382422e-02,  3.33654917e-01,  2.67464278e-01,\n",
       "         2.28088087e-02,  3.27278933e-01,  1.39907778e-01, -3.46701307e-01,\n",
       "        -1.35442651e-01, -1.24478651e-01, -1.42405410e-01,  1.89718341e-01,\n",
       "         1.69989432e-01,  6.78286388e-02,  3.59422965e-01,  2.48351830e-01,\n",
       "         1.90058834e-02,  3.51671853e-01,  1.20786740e-01, -3.84513312e-01,\n",
       "        -2.83386307e+00, -2.83045647e+00, -2.89943272e+00, -2.53556862e+00,\n",
       "        -2.49227655e+00, -2.31658327e+00, -2.14756261e+00, -1.99945997e+00,\n",
       "        -1.74467861e+00, -5.94731732e-01, -5.47863029e-01, -6.81959319e-01,\n",
       "        -2.82828503e+00, -2.82823909e+00, -2.89457521e+00, -2.45990302e+00,\n",
       "        -2.38336414e+00, -2.27492860e+00, -2.07629123e+00, -1.90476831e+00,\n",
       "        -1.75420390e+00, -3.38034715e-01, -4.22899291e-01, -5.67426038e-01,\n",
       "        -2.82828503e+00, -2.82823909e+00, -2.89457521e+00, -2.46020589e+00,\n",
       "        -2.38405488e+00, -2.29094340e+00, -2.06819120e+00, -1.92838567e+00,\n",
       "        -1.74800470e+00, -4.15197195e-01, -4.06146035e-01, -4.36952985e-01,\n",
       "         3.08775275e-01,  3.43506592e-01,  2.54308139e-01,  3.11627389e-01,\n",
       "         2.29086293e-01,  1.90742198e-01,  2.64532079e-01,  2.02922102e-01,\n",
       "         1.31384621e-01,  2.47938635e-01,  1.59198251e-01,  7.40167305e-02,\n",
       "         3.29268963e-01,  2.91218181e-01,  7.05999358e-02,  3.10709468e-01,\n",
       "         1.48513571e-01, -3.22960052e-01,  3.05473314e-01,  1.42417110e-01,\n",
       "        -3.25696783e-01,  2.98277646e-01,  1.41321780e-01, -3.25866446e-01,\n",
       "         3.45678056e-01,  3.73401337e-01,  5.07385247e-03,  2.33664157e-01,\n",
       "         2.76638303e-01, -4.13801216e-01,  2.55754087e-01,  2.87530717e-01,\n",
       "        -4.21730181e-01,  2.51930869e-01,  2.84472052e-01, -4.21752292e-01,\n",
       "         2.12968326e-01,  2.54973664e-01,  2.49995457e-01,  5.16292983e-01,\n",
       "         3.38139397e-01,  2.97385461e-01,  4.61053648e-01,  3.71404284e-01,\n",
       "         2.89646076e-01,  4.33527093e-01,  2.84748825e-01,  2.14349211e-01,\n",
       "         3.10538663e-01,  3.07950329e-01,  2.06620259e-01,  4.83229077e-01,\n",
       "         4.21700258e-01,  2.00687513e-01,  4.59894039e-01,  3.90381817e-01,\n",
       "         1.94098602e-01,  4.54959262e-01,  3.62211327e-01,  1.90974432e-01,\n",
       "         3.10768611e-01,  3.03771438e-01,  2.29673836e-01,  3.75000906e-01,\n",
       "         3.46438626e-01,  2.90630420e-01,  3.63054710e-01,  3.46881120e-01,\n",
       "         2.69097732e-01,  3.61039299e-01,  3.24729802e-01,  2.60688304e-01,\n",
       "         3.91899330e-01,  3.17832863e-01,  1.84221138e-01,  4.04300082e-01,\n",
       "         2.72556679e-01,  1.34027387e-01,  3.23337549e-01,  2.35636709e-01,\n",
       "         8.52202825e-02,  3.45720668e-01,  2.14316924e-01,  5.18843059e-02,\n",
       "         3.42684009e-01,  3.20182061e-01,  1.24631651e-02,  3.66537360e-01,\n",
       "         8.19456567e-02, -3.77188454e-01,  3.55766054e-01,  5.90483984e-02,\n",
       "        -3.72641085e-01,  3.44561014e-01,  5.50219479e-02, -3.75285730e-01,\n",
       "         2.52594724e-01,  3.52191752e-01, -6.19032774e-03,  2.64552180e-01,\n",
       "         1.86381348e-01, -3.94418331e-01,  2.59629103e-01,  1.87036899e-01,\n",
       "        -3.99886029e-01,  2.32470463e-01,  1.95250598e-01, -3.99936368e-01,\n",
       "        -1.70320184e+00, -2.11939147e+00, -1.92461461e+00, -7.32300818e-01,\n",
       "        -8.26711004e-01, -6.74109403e-01, -3.26791440e-01, -3.59473484e-01,\n",
       "        -6.25112563e-01, -6.44321751e-02, -2.63371662e-01, -3.40771307e-01,\n",
       "        -1.68752520e+00, -2.02420038e+00, -1.70082719e+00, -5.41116311e-01,\n",
       "        -6.00502202e-01, -3.94771815e-01, -1.52500639e-01, -3.06348432e-01,\n",
       "        -1.65384098e-01,  2.14158247e-01,  2.22076825e-01,  4.96538961e-02,\n",
       "        -1.68752520e+00, -2.02420038e+00, -1.72045521e+00, -4.23548822e-01,\n",
       "        -5.44120261e-01, -5.21449096e-01,  3.80238016e-02, -3.08050934e-01,\n",
       "        -2.83509467e-01,  4.02453876e-01,  4.32506911e-02,  2.04728014e-02,\n",
       "         2.95592977e-01,  1.13140207e-02, -1.04826716e-01,  1.66068121e-01,\n",
       "         9.13096050e-02, -8.30932227e-02,  2.49581970e-01,  8.38986826e-02,\n",
       "        -8.31090690e-02, -5.50663822e-01, -1.72156484e-01, -8.31091073e-02,\n",
       "        -4.66144458e-01, -3.41059872e-02, -4.46370499e-01, -6.79778952e-01,\n",
       "        -8.05454953e-02, -4.46370499e-01, -6.75044987e-01, -3.54200064e-02,\n",
       "        -4.46370499e-01, -2.48261502e+00, -1.54966755e-01, -4.46370499e-01,\n",
       "        -9.04591682e-03, -6.79278668e-01, -5.09222651e-01, -4.26145388e-01,\n",
       "        -6.51997339e-01, -5.09222651e-01, -6.51711264e-01, -6.86207233e-01,\n",
       "        -5.09222651e-01, -2.78177086e+00, -8.02024148e-01, -5.09222651e-01,\n",
       "        -2.23054957e-03, -3.01583918e-01, -5.37831025e-02, -2.76453973e-01,\n",
       "        -3.52170880e-01, -6.14382398e-03, -5.59330176e-01, -5.06876859e-01,\n",
       "        -6.14382398e-03, -4.89388382e-01, -2.93666470e-01, -6.14382398e-03,\n",
       "        -1.39228236e+00, -7.53010723e-01, -1.90627984e-01, -4.03104267e+00,\n",
       "        -6.08186866e-01, -1.90627984e-01, -2.04496104e+00, -6.08186866e-01,\n",
       "        -1.90627984e-01, -2.04496104e+00, -6.08186866e-01, -1.90627984e-01,\n",
       "        -3.03301055e-01, -6.95570159e-01, -5.09100952e-01, -9.23859347e-01,\n",
       "        -5.69356021e-01, -5.09100952e-01, -9.23859347e-01, -5.69356021e-01,\n",
       "        -5.09100952e-01, -9.23859347e-01, -5.69356021e-01, -5.09100952e-01,\n",
       "        -3.98195988e-01, -4.21203489e-01, -4.59859691e-01, -1.43958271e-01,\n",
       "        -3.21973456e-01, -3.97749779e-01, -1.11486790e-01, -3.19709674e-01,\n",
       "        -3.98013860e-01, -6.34385607e-01, -3.12564486e-01, -3.98013923e-01,\n",
       "        -2.59437751e+00, -6.05292934e-01, -7.44533019e-01, -2.47560870e+00,\n",
       "        -6.04139071e-01, -7.44533019e-01, -1.60423818e+00, -6.04139071e-01,\n",
       "        -7.44533019e-01, -1.60423819e+00, -6.04139071e-01, -7.44533019e-01,\n",
       "        -1.31917631e+00,  5.25534693e-01, -8.18616356e-01, -1.28631226e+00,\n",
       "         5.25534693e-01, -8.18616356e-01, -1.28631226e+00,  5.25534693e-01,\n",
       "        -8.18616356e-01, -1.28631226e+00,  5.25534693e-01, -8.18616356e-01,\n",
       "        -4.88614210e-01, -1.43131704e+00, -1.85311561e+00, -5.49727368e-01,\n",
       "        -1.11119182e+00, -1.85311561e+00, -5.49727368e-01, -1.11119182e+00,\n",
       "        -1.85311561e+00, -5.49727368e-01, -1.11119182e+00, -1.85311561e+00,\n",
       "        -2.51243740e+00, -2.53680556e+00, -2.29701644e+00, -2.40135997e+00,\n",
       "        -2.53680556e+00, -2.29701644e+00, -2.40135997e+00, -2.53680556e+00,\n",
       "        -2.29701644e+00, -2.40135997e+00, -2.53680556e+00, -2.29701644e+00,\n",
       "        -2.51243740e+00, -2.53680556e+00, -2.29701644e+00, -2.40135997e+00,\n",
       "        -2.53680556e+00, -2.29701644e+00, -2.40135997e+00, -2.53680556e+00,\n",
       "        -2.29701644e+00, -2.40135997e+00, -2.53680556e+00, -2.29701644e+00]),\n",
       " 'split2_test_score': array([-7.28141798e-02, -7.27278657e-02, -7.27210583e-02, -6.92478120e-02,\n",
       "        -6.90263200e-02, -6.87899070e-02, -6.44651319e-02, -6.41132670e-02,\n",
       "        -6.39186285e-02, -2.92733321e-02, -2.89668680e-02, -2.66011658e-02,\n",
       "        -7.25928693e-02, -7.25451507e-02, -7.26317691e-02, -6.88294018e-02,\n",
       "        -6.83189438e-02, -6.81743914e-02, -6.36403098e-02, -6.26508213e-02,\n",
       "        -6.25720334e-02, -2.44817311e-02, -2.20289161e-02, -2.12464981e-02,\n",
       "        -7.26175070e-02, -7.25199130e-02, -7.25132068e-02, -6.88869444e-02,\n",
       "        -6.83146645e-02, -6.81618617e-02, -6.36020431e-02, -6.26355536e-02,\n",
       "        -6.26798490e-02, -2.42418715e-02, -2.21553186e-02, -1.99569692e-02,\n",
       "        -4.61642758e-02, -4.60689437e-02, -4.62224888e-02, -4.25769078e-02,\n",
       "        -4.22725005e-02, -4.30670781e-02, -3.81487371e-02, -3.76051394e-02,\n",
       "        -3.91595680e-02, -2.32810650e-03,  8.72586791e-04, -2.69991809e-03,\n",
       "        -4.60800133e-02, -4.60602988e-02, -4.62182470e-02, -4.25135300e-02,\n",
       "        -4.19365709e-02, -4.31244007e-02, -3.78596567e-02, -3.70922088e-02,\n",
       "        -3.93009443e-02, -5.45092655e-04,  2.66795503e-03, -4.23432949e-04,\n",
       "        -4.61024838e-02, -4.60787394e-02, -4.62225982e-02, -4.25309669e-02,\n",
       "        -4.18928058e-02, -4.31418069e-02, -3.78352805e-02, -3.69330227e-02,\n",
       "        -3.93333170e-02, -3.61755157e-04,  2.92127667e-03, -5.09214769e-04,\n",
       "        -4.60403782e-02, -4.60243616e-02, -4.58778774e-02, -4.24955957e-02,\n",
       "        -4.23935640e-02, -4.13487239e-02, -3.76988128e-02, -3.76393546e-02,\n",
       "        -3.57345538e-02, -2.61417407e-03, -2.57688584e-03,  7.34784185e-03,\n",
       "        -4.58501804e-02, -4.59573561e-02, -4.57678435e-02, -4.22570134e-02,\n",
       "        -4.18877141e-02, -4.09458943e-02, -3.71956866e-02, -3.65260060e-02,\n",
       "        -3.48145356e-02,  1.14443676e-03,  2.91578443e-03,  1.17284582e-02,\n",
       "        -4.59274945e-02, -4.59728518e-02, -4.57229694e-02, -4.23290262e-02,\n",
       "        -4.18779876e-02, -4.06330830e-02, -3.73265836e-02, -3.65470638e-02,\n",
       "        -3.44893086e-02,  6.94057746e-04,  2.40569572e-03,  1.37949650e-02,\n",
       "        -6.59772671e-01, -6.59773445e-01, -6.59964711e-01, -6.57565289e-01,\n",
       "        -6.57922652e-01, -6.59302810e-01, -6.55039687e-01, -6.55763791e-01,\n",
       "        -6.58480267e-01, -6.34218676e-01, -6.37381256e-01, -6.52364672e-01,\n",
       "        -6.59767885e-01, -6.59707589e-01, -6.59853818e-01, -6.57505637e-01,\n",
       "        -6.57631594e-01, -6.58750268e-01, -6.54789636e-01, -6.55060642e-01,\n",
       "        -6.57379966e-01, -6.32003801e-01, -6.33485280e-01, -6.46729448e-01,\n",
       "        -6.59767885e-01, -6.59707589e-01, -6.59853818e-01, -6.57505637e-01,\n",
       "        -6.57631594e-01, -6.58750268e-01, -6.54789636e-01, -6.55060646e-01,\n",
       "        -6.57379966e-01, -6.32003801e-01, -6.33486545e-01, -6.46729448e-01,\n",
       "        -6.47486529e-02, -6.36534575e-02, -6.39142626e-02, -3.03897645e-02,\n",
       "        -2.83482364e-02, -2.65809637e-02,  1.39197560e-02,  1.50409045e-02,\n",
       "         1.56780024e-02,  2.49315607e-01,  2.53666734e-01,  2.45867237e-01,\n",
       "        -6.25322537e-02, -6.20762222e-02, -6.29011812e-02, -2.64263635e-02,\n",
       "        -2.18224360e-02, -2.05103826e-02,  2.14524795e-02,  2.85607185e-02,\n",
       "         2.82661683e-02,  2.69720458e-01,  2.76973671e-01,  2.63611501e-01,\n",
       "        -6.24107216e-02, -6.22800639e-02, -6.21619641e-02, -2.63030546e-02,\n",
       "        -2.23391679e-02, -2.09958827e-02,  2.22122010e-02,  2.98001722e-02,\n",
       "         2.47461455e-02,  2.73153816e-01,  2.78295751e-01,  2.72056109e-01,\n",
       "        -3.85655585e-02, -3.76119864e-02, -3.91560672e-02, -1.80328011e-03,\n",
       "        -5.22091461e-04, -2.68925155e-03,  3.95048527e-02,  4.60490962e-02,\n",
       "         4.03784685e-02,  2.95918581e-01,  3.15773631e-01,  3.02586910e-01,\n",
       "        -3.77268765e-02, -3.75274541e-02, -3.91161582e-02, -8.73475798e-04,\n",
       "         3.10859843e-03,  1.37254206e-05,  4.35232375e-02,  5.09500265e-02,\n",
       "         4.64934897e-02,  3.06687938e-01,  3.26347650e-01,  3.20289266e-01,\n",
       "        -3.79500768e-02, -3.77112334e-02, -3.91596139e-02, -1.12682072e-03,\n",
       "         4.21099824e-03, -8.50640634e-05,  4.35416155e-02,  5.21671970e-02,\n",
       "         4.63957960e-02,  3.08787721e-01,  3.25910489e-01,  3.24943389e-01,\n",
       "        -3.72430197e-02, -3.70458159e-02, -3.57296598e-02, -3.32174327e-03,\n",
       "        -2.33444130e-03,  7.37051183e-03,  4.01676239e-02,  4.10966077e-02,\n",
       "         5.46032995e-02,  2.74365402e-01,  2.76739997e-01,  2.75527768e-01,\n",
       "        -3.59105033e-02, -3.57961775e-02, -3.47915470e-02, -9.46608362e-04,\n",
       "         3.26049089e-03,  1.19187262e-02,  4.50190335e-02,  5.01224288e-02,\n",
       "         6.56953175e-02,  2.86416888e-01,  2.89884029e-01,  3.33035268e-01,\n",
       "        -3.62024718e-02, -3.62168143e-02, -3.48948705e-02, -3.86994064e-04,\n",
       "         3.79611351e-03,  1.31657826e-02,  4.59499628e-02,  5.03748058e-02,\n",
       "         6.86094305e-02,  2.88498758e-01,  2.93508150e-01,  3.54987848e-01,\n",
       "        -6.56568805e-01, -6.56569964e-01, -6.58479529e-01, -6.35007341e-01,\n",
       "        -6.39226047e-01, -6.52348178e-01, -6.11221989e-01, -6.21398680e-01,\n",
       "        -6.51215471e-01, -3.44183836e-01, -3.34986593e-01, -5.82560849e-01,\n",
       "        -6.56521358e-01, -6.55918506e-01, -6.57378737e-01, -6.34455106e-01,\n",
       "        -6.36556146e-01, -6.46725721e-01, -6.08987053e-01, -6.13145899e-01,\n",
       "        -6.33221288e-01, -3.35250303e-01, -3.18413605e-01, -4.38944835e-01,\n",
       "        -6.56521358e-01, -6.55918506e-01, -6.57378737e-01, -6.34455106e-01,\n",
       "        -6.36556146e-01, -6.46725721e-01, -6.08987053e-01, -6.13253435e-01,\n",
       "        -6.33221288e-01, -3.34780815e-01, -3.18816446e-01, -4.38944835e-01,\n",
       "         5.52720643e-03,  1.65869287e-02,  1.72017946e-02,  2.40963699e-01,\n",
       "         2.50171332e-01,  2.48160976e-01,  4.05409363e-01,  4.14852818e-01,\n",
       "         3.72458057e-01,  5.90840277e-01,  5.73735752e-01,  4.35316809e-01,\n",
       "         2.27950460e-02,  2.62143651e-02,  2.96835960e-02,  2.62728999e-01,\n",
       "         2.77630419e-01,  2.79700155e-01,  4.24702207e-01,  4.23500952e-01,\n",
       "         3.92157769e-01,  5.31280656e-01,  4.84040472e-01,  4.08119492e-01,\n",
       "         2.68261377e-02,  3.43142174e-02,  3.38464369e-02,  2.70744321e-01,\n",
       "         2.88485664e-01,  2.65741626e-01,  4.25263101e-01,  4.44074498e-01,\n",
       "         3.68583032e-01,  5.20321592e-01,  5.03033907e-01,  3.97437686e-01,\n",
       "         3.41113018e-02,  4.43103568e-02,  3.90486614e-02,  2.91858197e-01,\n",
       "         3.01846522e-01,  3.04429129e-01,  4.63239327e-01,  4.72344242e-01,\n",
       "         5.12838195e-01,  6.20788176e-01,  6.02803274e-01,  6.28822692e-01,\n",
       "         4.31291375e-02,  4.70423262e-02,  4.60418311e-02,  3.00622549e-01,\n",
       "         3.17836223e-01,  3.26737833e-01,  4.76147883e-01,  4.81889449e-01,\n",
       "         4.96036046e-01,  5.74072889e-01,  5.62753787e-01,  5.66123732e-01,\n",
       "         4.10769873e-02,  4.52253671e-02,  4.58907143e-02,  2.97288101e-01,\n",
       "         3.25524534e-01,  3.24519160e-01,  4.70497478e-01,  4.85227523e-01,\n",
       "         5.04114654e-01,  5.84869101e-01,  5.79084724e-01,  5.70804933e-01,\n",
       "         4.38602030e-02,  4.58855437e-02,  5.45654958e-02,  2.65957108e-01,\n",
       "         2.75400592e-01,  2.79758175e-01,  4.29159226e-01,  4.38024049e-01,\n",
       "         4.02343650e-01,  5.92419026e-01,  5.68434751e-01,  5.22452399e-01,\n",
       "         5.01238188e-02,  5.85196559e-02,  6.39429128e-02,  2.73148829e-01,\n",
       "         2.98253498e-01,  3.40833124e-01,  4.31951092e-01,  4.38510338e-01,\n",
       "         4.75502979e-01,  5.25670141e-01,  4.85980147e-01,  5.32034037e-01,\n",
       "         5.75895218e-02,  4.77137001e-02,  6.49819289e-02,  2.88745914e-01,\n",
       "         2.95087759e-01,  3.53988173e-01,  4.37368582e-01,  4.32518275e-01,\n",
       "         4.70793733e-01,  5.16774996e-01,  5.03691288e-01,  5.30868855e-01,\n",
       "        -6.26304059e-01, -6.27985127e-01, -6.50936982e-01, -3.34052082e-01,\n",
       "        -3.41302852e-01, -5.76031804e-01, -8.20408770e-02, -7.89207619e-02,\n",
       "        -2.34476613e-01,  3.81360042e-01,  4.23965128e-01,  4.16140376e-01,\n",
       "        -6.25869849e-01, -6.19335448e-01, -6.33149855e-01, -3.36704386e-01,\n",
       "        -3.17351616e-01, -4.39515842e-01, -6.88308670e-02, -3.10355564e-02,\n",
       "        -8.47404441e-02,  3.88236411e-01,  4.17542859e-01,  4.95894946e-01,\n",
       "        -6.25869849e-01, -6.19335448e-01, -6.33149855e-01, -3.36704386e-01,\n",
       "        -3.18040211e-01, -4.39515842e-01, -7.06594468e-02, -2.36407891e-02,\n",
       "        -8.47551823e-02,  3.86794046e-01,  4.25264583e-01,  4.94502316e-01,\n",
       "         4.13369878e-01,  3.99798722e-01,  4.11470594e-01,  6.09658966e-01,\n",
       "         5.53748365e-01,  4.48568826e-01,  6.12662013e-01,  4.96491744e-01,\n",
       "         3.89799754e-01,  5.74536999e-01,  4.31866127e-01,  3.78530935e-01,\n",
       "         3.50512140e-01,  3.96398075e-01,  3.44982880e-01,  4.61302276e-01,\n",
       "         4.92800088e-01,  4.01228941e-01,  4.59826893e-01,  4.73882278e-01,\n",
       "         4.04332528e-01,  4.43179336e-01,  4.72832073e-01,  4.04193883e-01,\n",
       "         3.88989984e-01,  4.49123280e-01,  3.40724300e-01,  5.51030224e-01,\n",
       "         5.47749908e-01,  3.27291746e-01,  5.48554590e-01,  5.38481267e-01,\n",
       "         3.26614457e-01,  5.42430843e-01,  5.35445997e-01,  3.26616245e-01,\n",
       "         4.32715933e-01,  4.86623523e-01,  4.62145419e-01,  6.24948162e-01,\n",
       "         5.92688907e-01,  6.23019058e-01,  6.44633091e-01,  6.29379209e-01,\n",
       "         6.11819585e-01,  6.04010779e-01,  6.11701130e-01,  6.17209562e-01,\n",
       "         4.10142708e-01,  4.96687075e-01,  4.96768649e-01,  5.94431936e-01,\n",
       "         5.92880660e-01,  5.19858044e-01,  5.19732785e-01,  5.92237905e-01,\n",
       "         5.24276536e-01,  5.00503149e-01,  6.00224266e-01,  5.15180629e-01,\n",
       "         4.59282677e-01,  4.50867491e-01,  4.93307438e-01,  6.20371494e-01,\n",
       "         5.31838538e-01,  5.24982697e-01,  5.99865112e-01,  5.39336751e-01,\n",
       "         5.21243475e-01,  6.02228217e-01,  5.48365170e-01,  5.24098434e-01,\n",
       "         3.51981205e-01,  4.66889459e-01,  4.09818975e-01,  5.02824583e-01,\n",
       "         6.14597053e-01,  4.99044449e-01,  4.96197511e-01,  5.73308197e-01,\n",
       "         4.10242785e-01,  4.67907825e-01,  4.97147121e-01,  3.70060244e-01,\n",
       "         3.89472906e-01,  5.06169039e-01,  4.25254486e-01,  5.81147809e-01,\n",
       "         5.79010609e-01,  4.86574652e-01,  5.81054432e-01,  5.68534237e-01,\n",
       "         4.86654809e-01,  5.66717216e-01,  5.71593570e-01,  4.85283812e-01,\n",
       "         3.57697967e-01,  4.28573063e-01,  4.79050623e-01,  5.47327677e-01,\n",
       "         4.88350896e-01,  5.32327309e-01,  5.52493233e-01,  4.80803154e-01,\n",
       "         5.31769545e-01,  5.46448106e-01,  4.89123776e-01,  5.31768273e-01,\n",
       "        -2.62650585e-01, -5.15138477e-02, -1.98827226e-01,  3.47039650e-01,\n",
       "         3.65468509e-01,  4.89934916e-01,  4.27180023e-01,  4.89172498e-01,\n",
       "         5.74213033e-01,  5.29348882e-01,  5.91773181e-01,  5.91418369e-01,\n",
       "        -1.94726952e-01, -1.24301342e-01, -1.20427799e-01,  3.15675695e-01,\n",
       "         3.31358592e-01,  4.55176635e-01,  4.80530712e-01,  5.19609818e-01,\n",
       "         5.52744900e-01,  4.21186773e-01,  5.36454860e-01,  5.63656063e-01,\n",
       "        -1.94657400e-01, -1.24301342e-01, -1.20427799e-01,  2.96211945e-01,\n",
       "         3.27863670e-01,  4.72772177e-01,  4.68395799e-01,  4.42698309e-01,\n",
       "         5.37428412e-01,  4.34878781e-01,  4.80902310e-01,  5.45766206e-01,\n",
       "         3.85419519e-01,  4.10253502e-01,  4.63877667e-01,  5.00255771e-01,\n",
       "         3.36983887e-01,  4.61013573e-01,  4.12661067e-01,  3.79703178e-01,\n",
       "         4.60973320e-01,  3.28332264e-01,  2.45865938e-01,  4.60973363e-01,\n",
       "         5.61793511e-03,  8.57114891e-02,  6.25131215e-01, -1.24244959e-01,\n",
       "         9.53063910e-02,  6.25131215e-01, -1.63670525e-01,  8.75075056e-02,\n",
       "         6.25131215e-01, -5.08653452e-01, -3.66249155e-02,  6.25131215e-01,\n",
       "        -2.57745257e-01,  1.89418138e-01,  1.66509408e-01, -7.96175266e-01,\n",
       "         1.91175154e-01,  1.66509408e-01, -9.96149610e-01,  2.18124939e-01,\n",
       "         1.66509408e-01, -1.26626742e+00,  2.16821445e-01,  1.66509408e-01,\n",
       "        -9.54996296e-02,  5.17740543e-02,  2.73229901e-01, -1.34906705e-01,\n",
       "        -2.52424400e-01,  7.72510101e-02, -1.68441715e-01, -2.71924575e-01,\n",
       "         7.72510101e-02, -1.41166646e+00, -4.00974908e-01,  7.72510101e-02,\n",
       "         3.84995627e-01,  2.70493366e-01,  8.93689928e-02, -1.46578742e+00,\n",
       "         3.47683578e-01,  8.93689928e-02, -1.79232090e+00,  3.47683578e-01,\n",
       "         8.93689928e-02, -1.69602339e+00,  3.47683578e-01,  8.93689928e-02,\n",
       "         4.31058873e-01, -7.46573547e-01,  1.48225098e-01,  9.40712593e-02,\n",
       "        -7.46573547e-01,  1.48225098e-01, -4.85463822e-02, -7.46573547e-01,\n",
       "         1.48225098e-01, -2.26943940e-01, -7.46573547e-01,  1.48225098e-01,\n",
       "         1.57488574e-01,  4.12028504e-01,  3.20240419e-01,  7.03462450e-02,\n",
       "         3.76758813e-01,  2.80248005e-01,  7.08272910e-03,  3.57683989e-01,\n",
       "         2.80722797e-01, -2.41406263e-01,  3.69636680e-01,  2.80722790e-01,\n",
       "        -1.18643199e-01, -1.98746436e-01,  5.74154884e-01,  5.36251387e-02,\n",
       "        -1.98746436e-01,  5.74154884e-01,  5.36206692e-02, -1.98746436e-01,\n",
       "         5.74154884e-01,  5.36206692e-02, -1.98746436e-01,  5.74154884e-01,\n",
       "        -1.86412943e-01,  2.75795715e-01,  5.72374813e-01, -1.15970043e+01,\n",
       "         2.75795715e-01,  5.72374813e-01, -1.15970043e+01,  2.75795715e-01,\n",
       "         5.72374813e-01, -1.15970043e+01,  2.75795715e-01,  5.72374813e-01,\n",
       "         4.53880392e-02,  1.56694900e-01, -2.73642254e-02, -2.52002349e-02,\n",
       "         1.56694900e-01, -2.73642254e-02, -2.18048781e-02,  1.56694900e-01,\n",
       "        -2.73642254e-02, -2.18048781e-02,  2.10669645e-01, -2.73642254e-02,\n",
       "        -9.40698528e-01,  6.23995352e-02, -4.50767708e-01, -9.40698528e-01,\n",
       "         6.23995352e-02, -4.50767708e-01, -9.40698528e-01,  6.23995352e-02,\n",
       "        -4.50767708e-01, -9.40698528e-01,  6.23995352e-02, -4.50767708e-01,\n",
       "        -9.40698528e-01,  6.23995352e-02, -4.50767708e-01, -9.40698528e-01,\n",
       "         6.23995352e-02, -4.50767708e-01, -9.40698528e-01,  6.23995352e-02,\n",
       "        -4.50767708e-01, -9.40698528e-01,  6.23995352e-02, -4.50767708e-01]),\n",
       " 'split3_test_score': array([-4.09524505e-02, -4.08475408e-02, -4.09324298e-02, -3.58649383e-02,\n",
       "        -3.59548619e-02, -3.60422554e-02, -2.95576015e-02, -2.94850849e-02,\n",
       "        -2.99950403e-02,  1.98365139e-02,  1.92696956e-02,  1.60789046e-02,\n",
       "        -4.08746720e-02, -4.07072938e-02, -4.09403778e-02, -3.51316720e-02,\n",
       "        -3.51353295e-02, -3.61164462e-02, -2.79966899e-02, -2.81143417e-02,\n",
       "        -3.01874290e-02,  2.53337874e-02,  2.51977512e-02,  1.26713169e-02,\n",
       "        -4.08633903e-02, -4.07095136e-02, -4.10033815e-02, -3.52303921e-02,\n",
       "        -3.52411001e-02, -3.64349034e-02, -2.83244756e-02, -2.82947468e-02,\n",
       "        -3.08045401e-02,  2.51601808e-02,  2.44205929e-02,  9.92692294e-03,\n",
       "        -3.61321407e-02, -3.62229593e-02, -3.63507715e-02, -3.12060322e-02,\n",
       "        -3.10104346e-02, -3.15320767e-02, -2.49521822e-02, -2.42718635e-02,\n",
       "        -2.55501209e-02,  2.56226980e-02,  2.70076050e-02,  1.94215209e-02,\n",
       "        -3.61555058e-02, -3.62226761e-02, -3.64334551e-02, -3.12850576e-02,\n",
       "        -3.09933087e-02, -3.18985313e-02, -2.48749472e-02, -2.43483237e-02,\n",
       "        -2.63977905e-02,  2.58016588e-02,  2.69537334e-02,  1.49985260e-02,\n",
       "        -3.61426246e-02, -3.62272014e-02, -3.64519261e-02, -3.12704344e-02,\n",
       "        -3.09888233e-02, -3.19609560e-02, -2.48640495e-02, -2.43214628e-02,\n",
       "        -2.64089589e-02,  2.57719321e-02,  2.68535865e-02,  1.48457617e-02,\n",
       "        -3.63397900e-02, -3.62341932e-02, -3.63404216e-02, -3.12432886e-02,\n",
       "        -3.14582170e-02, -3.14942345e-02, -2.47360764e-02, -2.49998612e-02,\n",
       "        -2.55019263e-02,  2.45577487e-02,  2.35541824e-02,  2.01534023e-02,\n",
       "        -3.61276535e-02, -3.61194415e-02, -3.63603665e-02, -3.05099722e-02,\n",
       "        -3.06830686e-02, -3.16583994e-02, -2.36746017e-02, -2.37912090e-02,\n",
       "        -2.58681792e-02,  2.93204576e-02,  2.83332941e-02,  1.59291953e-02,\n",
       "        -3.61654768e-02, -3.61867296e-02, -3.64217682e-02, -3.05295737e-02,\n",
       "        -3.07544743e-02, -3.19420820e-02, -2.36275761e-02, -2.38088349e-02,\n",
       "        -2.64717200e-02,  2.94139527e-02,  2.90098269e-02,  1.33582233e-02,\n",
       "        -1.76567645e+00, -1.76547776e+00, -1.76685823e+00, -1.76254303e+00,\n",
       "        -1.76236690e+00, -1.76611874e+00, -1.75881102e+00, -1.75816158e+00,\n",
       "        -1.76520152e+00, -1.72557842e+00, -1.72680295e+00, -1.75668636e+00,\n",
       "        -1.76575679e+00, -1.76529548e+00, -1.76685464e+00, -1.76170637e+00,\n",
       "        -1.76133697e+00, -1.76610133e+00, -1.75653788e+00, -1.75578048e+00,\n",
       "        -1.76516719e+00, -1.71560122e+00, -1.71144685e+00, -1.75700352e+00,\n",
       "        -1.76575679e+00, -1.76529548e+00, -1.76685464e+00, -1.76170637e+00,\n",
       "        -1.76133697e+00, -1.76610133e+00, -1.75653788e+00, -1.75578048e+00,\n",
       "        -1.76516719e+00, -1.71544454e+00, -1.71145042e+00, -1.75700352e+00,\n",
       "        -3.01866328e-02, -2.91355904e-02, -3.00074769e-02,  1.81709604e-02,\n",
       "         1.63312467e-02,  1.61446600e-02,  7.27023874e-02,  7.20185230e-02,\n",
       "         6.86042570e-02,  3.59886331e-01,  3.54724692e-01,  3.34412658e-01,\n",
       "        -3.04498320e-02, -2.75570109e-02, -3.01069379e-02,  2.09727744e-02,\n",
       "         2.24899650e-02,  1.26192847e-02,  7.83672742e-02,  8.23314022e-02,\n",
       "         5.51732912e-02,  3.67407256e-01,  3.49291569e-01,  1.64306747e-01,\n",
       "        -2.93203636e-02, -2.74291869e-02, -3.09522659e-02,  2.34097643e-02,\n",
       "         2.40775612e-02,  9.68716278e-03,  8.09794631e-02,  8.24733508e-02,\n",
       "         5.07498282e-02,  3.68622153e-01,  3.42450398e-01,  1.27497627e-01,\n",
       "        -2.34327273e-02, -2.43383789e-02, -2.56171707e-02,  2.29911755e-02,\n",
       "         2.46420187e-02,  1.93354035e-02,  7.73703152e-02,  8.22083071e-02,\n",
       "         6.95150441e-02,  3.57825158e-01,  3.53817631e-01,  3.40071662e-01,\n",
       "        -2.36698329e-02, -2.43435838e-02, -2.64625280e-02,  2.20868854e-02,\n",
       "         2.46535880e-02,  1.53731677e-02,  7.93289655e-02,  8.19222682e-02,\n",
       "         6.00114556e-02,  3.55226629e-01,  3.40797698e-01,  2.75112530e-01,\n",
       "        -2.35451284e-02, -2.43866920e-02, -2.66396937e-02,  2.22281138e-02,\n",
       "         2.47004811e-02,  1.48042903e-02,  7.95866856e-02,  8.18479931e-02,\n",
       "         6.00051761e-02,  3.54096732e-01,  3.41335268e-01,  2.78850825e-01,\n",
       "        -2.53581258e-02, -2.44292476e-02, -2.55135817e-02,  2.22215423e-02,\n",
       "         2.04092233e-02,  2.02214452e-02,  7.81070876e-02,  7.77448459e-02,\n",
       "         7.22070236e-02,  3.64658063e-01,  3.59129967e-01,  3.26168706e-01,\n",
       "        -2.38968673e-02, -2.36640804e-02, -2.58622485e-02,  2.95638020e-02,\n",
       "         2.78167787e-02,  1.58532057e-02,  8.67448481e-02,  8.49625351e-02,\n",
       "         5.74014304e-02,  3.69484884e-01,  3.46337706e-01,  1.66123141e-01,\n",
       "        -2.36498775e-02, -2.29644994e-02, -2.64227500e-02,  2.97058582e-02,\n",
       "         2.58488901e-02,  1.39002721e-02,  8.83219102e-02,  8.33616275e-02,\n",
       "         5.42298070e-02,  3.70417769e-01,  3.38256462e-01,  1.27805043e-01,\n",
       "        -1.75341849e+00, -1.75143352e+00, -1.76519978e+00, -1.72291693e+00,\n",
       "        -1.72092719e+00, -1.75651400e+00, -1.68746548e+00, -1.68555765e+00,\n",
       "        -1.74551999e+00, -1.32638578e+00, -1.42079307e+00, -1.44897843e+00,\n",
       "        -1.75422097e+00, -1.74962646e+00, -1.76516461e+00, -1.71487432e+00,\n",
       "        -1.71119409e+00, -1.75683910e+00, -1.66608259e+00, -1.66881226e+00,\n",
       "        -1.74653362e+00, -1.18826260e+00, -1.25033957e+00, -1.48432123e+00,\n",
       "        -1.75422097e+00, -1.74962646e+00, -1.76516461e+00, -1.71487432e+00,\n",
       "        -1.71119409e+00, -1.75683910e+00, -1.66608259e+00, -1.66881226e+00,\n",
       "        -1.74653362e+00, -1.18693717e+00, -1.24990984e+00, -1.48432123e+00,\n",
       "         6.27332199e-02,  7.74680576e-02,  6.81205344e-02,  3.51824887e-01,\n",
       "         3.51121198e-01,  3.35845965e-01,  4.77488558e-01,  4.72475304e-01,\n",
       "         4.25947215e-01,  4.73771627e-01,  4.08872802e-01,  2.77126939e-01,\n",
       "         7.40658256e-02,  8.00476265e-02,  5.57929528e-02,  3.50208443e-01,\n",
       "         3.39147742e-01,  1.65200669e-01,  4.28482168e-01,  3.93458457e-01,\n",
       "         3.06673710e-02,  2.07958243e-01,  1.30573959e-01, -3.73946326e-01,\n",
       "         6.94588465e-02,  8.01192964e-02,  5.02912870e-02,  3.46705616e-01,\n",
       "         3.39171953e-01,  1.23722980e-01,  4.38302639e-01,  3.97452866e-01,\n",
       "        -5.78740331e-02,  1.92250132e-01,  1.17038717e-01, -6.81171482e-01,\n",
       "         9.03778374e-02,  8.20301762e-02,  6.92509375e-02,  3.42026315e-01,\n",
       "         3.59229366e-01,  3.39796854e-01,  4.66984050e-01,  4.65822412e-01,\n",
       "         4.21431421e-01,  4.66257564e-01,  4.99206414e-01,  4.42471259e-01,\n",
       "         8.78998847e-02,  8.12104144e-02,  5.90633373e-02,  3.42975473e-01,\n",
       "         3.42450582e-01,  2.77087160e-01,  4.27067864e-01,  4.06936528e-01,\n",
       "         3.09177075e-01,  3.53562305e-01,  2.72466520e-01,  2.63608657e-01,\n",
       "         8.87783213e-02,  8.09731695e-02,  5.80369446e-02,  3.44035540e-01,\n",
       "         3.44302077e-01,  2.77649617e-01,  4.35340071e-01,  3.99489808e-01,\n",
       "         3.01269080e-01,  3.33109572e-01,  2.73129872e-01,  2.34076457e-01,\n",
       "         7.42987902e-02,  8.30708763e-02,  7.17176286e-02,  3.38452372e-01,\n",
       "         3.62611452e-01,  3.18231010e-01,  4.74616693e-01,  4.86047592e-01,\n",
       "         4.37068463e-01,  4.59440821e-01,  4.26599323e-01,  2.93575670e-01,\n",
       "         8.48635208e-02,  9.20679628e-02,  5.67092687e-02,  3.46513320e-01,\n",
       "         3.48831848e-01,  1.65671380e-01,  4.29558801e-01,  3.98997871e-01,\n",
       "         2.65277042e-02,  1.95139974e-01,  1.40009655e-01, -4.09637235e-01,\n",
       "         8.28470381e-02,  9.09228662e-02,  5.46152100e-02,  3.35675617e-01,\n",
       "         3.28301806e-01,  1.29245573e-01,  4.25320996e-01,  3.67582136e-01,\n",
       "        -4.60819248e-02,  2.06081869e-01,  1.11913033e-01, -6.82847220e-01,\n",
       "        -1.61433561e+00, -1.61677305e+00, -1.74544445e+00, -1.23794081e+00,\n",
       "        -1.34406310e+00, -1.48471425e+00, -9.01482924e-01, -9.94383478e-01,\n",
       "        -1.13477878e+00,  7.69399296e-02, -1.33250165e-01, -2.08482343e-01,\n",
       "        -1.62042630e+00, -1.60034528e+00, -1.74618288e+00, -1.19509481e+00,\n",
       "        -1.18995450e+00, -1.50771346e+00, -8.67207650e-01, -8.92150393e-01,\n",
       "        -1.24804243e+00,  2.99888158e-01,  2.10269144e-01, -1.28805158e-01,\n",
       "        -1.62042630e+00, -1.60034528e+00, -1.74618288e+00, -1.19496899e+00,\n",
       "        -1.18995450e+00, -1.50771346e+00, -8.75661055e-01, -8.89852028e-01,\n",
       "        -1.24818931e+00,  3.13513889e-01,  2.01414597e-01, -1.32933960e-01,\n",
       "         4.31713547e-01,  4.92604286e-01,  4.01951885e-01,  4.25784127e-01,\n",
       "         4.49766649e-01,  2.40128994e-01,  3.87528032e-01,  3.58655735e-01,\n",
       "         1.04167905e-01,  1.84396895e-01,  1.32997650e-01, -4.39930864e-02,\n",
       "         3.93151915e-01,  4.04326734e-01, -9.51419996e-03,  1.07334372e-01,\n",
       "         1.53674469e-01, -4.13482725e-01,  4.71825016e-04,  1.25150357e-01,\n",
       "        -4.19806001e-01, -4.43761653e-02,  1.23004895e-01, -4.19932630e-01,\n",
       "         3.68545709e-01,  4.05240793e-01, -8.84602489e-02,  1.59328045e-01,\n",
       "         8.10429770e-02, -7.06251691e-01,  7.47671307e-02,  6.07161379e-02,\n",
       "        -7.18157180e-01,  5.93829451e-02,  5.62419632e-02, -7.18226642e-01,\n",
       "         3.95852784e-01,  5.14092117e-01,  4.38286632e-01,  3.09596955e-01,\n",
       "         4.82603525e-01,  3.07545425e-01,  4.05765609e-01,  4.50520985e-01,\n",
       "         2.21631185e-01,  2.47007404e-01,  3.70512677e-01,  2.36509821e-01,\n",
       "         4.77263277e-01,  5.24351211e-01,  2.77475728e-01,  3.19495679e-01,\n",
       "         3.01826657e-01,  2.64315762e-01,  2.63361180e-01,  2.42791971e-01,\n",
       "         2.48709236e-01,  2.32842206e-01,  2.17293502e-01,  2.23649949e-01,\n",
       "         4.85589545e-01,  5.02172916e-01,  2.56964892e-01,  4.20356269e-01,\n",
       "         3.03398224e-01,  1.27452951e-01,  3.41145237e-01,  2.64449557e-01,\n",
       "         1.08995773e-01,  3.05069971e-01,  2.61650520e-01,  1.00502827e-01,\n",
       "         4.43964137e-01,  4.87648919e-01,  4.05115033e-01,  4.45144273e-01,\n",
       "         4.11181665e-01,  3.08680313e-01,  3.60255302e-01,  2.68936747e-01,\n",
       "         2.28746234e-01,  9.40297914e-02,  7.84026007e-02,  8.68404904e-02,\n",
       "         3.84117521e-01,  4.14232500e-01,  1.89272614e-02,  1.31460566e-01,\n",
       "         2.20730505e-01, -4.21916958e-01,  1.51419748e-02,  1.80971601e-01,\n",
       "        -4.28514952e-01,  9.42292227e-03,  1.63844018e-01, -4.28727941e-01,\n",
       "         4.04046798e-01,  4.14529266e-01, -7.86675238e-02,  1.11249624e-01,\n",
       "         1.56268028e-01, -7.12952312e-01,  2.60412907e-02,  1.42593455e-01,\n",
       "        -7.23127750e-01,  1.17911954e-02,  1.34128165e-01, -7.23165553e-01,\n",
       "        -7.72317233e-01, -9.60230917e-01, -1.07469954e+00,  6.60941176e-02,\n",
       "        -3.08322163e-01, -2.08305491e-01,  1.37299318e-01, -3.15795368e-01,\n",
       "        -1.99863786e-01,  2.43535236e-01, -3.95271924e-01, -1.99760207e-01,\n",
       "        -6.26939372e-01, -5.38135393e-01, -1.15969414e+00,  1.57226027e-01,\n",
       "         1.20873075e-01, -7.77570844e-02,  6.46353036e-02,  1.44720428e-01,\n",
       "        -1.47182237e-02,  1.49194674e-01,  1.92770925e-01,  3.44792717e-03,\n",
       "        -6.91053852e-01, -5.38005451e-01, -1.15969414e+00,  2.54170867e-01,\n",
       "         1.94810535e-01, -1.72793110e-01,  2.54385394e-01,  1.65901165e-01,\n",
       "        -1.49238325e-01,  9.95276533e-02,  2.36031072e-02,  1.92058740e-01,\n",
       "        -5.64530584e-01, -9.18308394e-01, -1.70016254e-01, -1.97924319e+00,\n",
       "        -9.66813089e-01, -1.66715598e-01, -1.85270627e+00, -9.97281567e-01,\n",
       "        -1.66540316e-01, -3.19878739e+00, -1.06077812e+00, -1.66540282e-01,\n",
       "        -2.63990625e+00, -3.35037012e+00, -7.83768977e-01, -2.74166683e+00,\n",
       "        -3.27816175e+00, -7.83768977e-01, -3.10054969e+00, -4.34655232e+00,\n",
       "        -7.83768977e-01, -8.34717693e+00, -5.68901626e+00, -7.83768977e-01,\n",
       "        -2.00622466e+00, -2.11457043e+00, -8.17790401e-01, -1.74822311e+00,\n",
       "        -2.07296854e+00, -8.17790401e-01, -2.43059457e+00, -2.28871762e+00,\n",
       "        -8.17790401e-01, -2.59083145e+00, -2.60768136e+00, -8.17790401e-01,\n",
       "        -4.83529057e-01,  1.12652640e-01,  7.42998436e-02, -4.84310638e+00,\n",
       "        -4.03361567e-01, -2.42506221e-01, -7.33638209e+00, -3.35596821e-01,\n",
       "        -2.42506221e-01, -9.75946653e+00, -3.74534023e-01, -2.42506221e-01,\n",
       "        -3.75540059e+00, -7.78811723e-01, -7.69258173e-01, -1.28689158e+00,\n",
       "        -8.53636884e-01, -7.69258173e-01, -2.01297596e+00, -8.53636884e-01,\n",
       "        -7.69258173e-01, -3.78723829e+00, -8.53636884e-01, -7.69258173e-01,\n",
       "        -1.60965084e+00, -1.30863148e+00, -5.96256109e-01, -4.04890300e+00,\n",
       "        -1.29690534e+00, -5.96256109e-01, -4.80187788e+00, -1.29690534e+00,\n",
       "        -5.96256109e-01, -4.69785806e+00, -1.29690534e+00, -5.96256109e-01,\n",
       "        -2.17762473e+00, -3.27925789e-01, -1.40457464e-01, -2.95394475e+00,\n",
       "        -6.31343287e-01, -2.65099260e-01, -2.86827319e+00, -6.55514451e-01,\n",
       "        -2.65153824e-01, -2.93496054e+00, -5.60659846e-01, -2.65153828e-01,\n",
       "        -2.17813387e+00, -1.38705455e+00, -7.54431333e-01, -2.67420781e+00,\n",
       "        -1.38705455e+00, -7.54431333e-01, -2.67420781e+00, -1.38705455e+00,\n",
       "        -7.54431333e-01, -2.67420781e+00, -1.38705455e+00, -7.54431333e-01,\n",
       "        -1.51671475e+00, -2.17399276e+00, -8.39196792e-01, -3.02273646e+01,\n",
       "        -2.17399276e+00, -8.39196792e-01, -3.02484875e+01, -2.17399276e+00,\n",
       "        -8.39196792e-01, -3.02484875e+01, -2.17399276e+00, -8.39196792e-01,\n",
       "        -4.02793204e-01, -3.36853174e-01, -6.48627526e-01, -1.04343139e+00,\n",
       "        -2.52831012e-01, -6.48627526e-01, -1.04343139e+00,  3.66182977e-02,\n",
       "        -6.48627526e-01, -1.19575439e+00,  3.66182977e-02, -6.48627526e-01,\n",
       "        -1.78629991e+00, -5.51507708e-02, -1.40957394e+00, -1.78629991e+00,\n",
       "        -5.51507708e-02, -1.40957394e+00, -1.78629991e+00, -5.51507708e-02,\n",
       "        -1.40957394e+00, -1.78629991e+00, -5.51507708e-02, -1.40957394e+00,\n",
       "        -1.78629991e+00, -5.51507708e-02, -1.40957394e+00, -1.78629991e+00,\n",
       "        -5.51507708e-02, -1.40957394e+00, -1.78629991e+00, -5.51507708e-02,\n",
       "        -1.40957394e+00, -1.78629991e+00, -5.51507708e-02, -1.40957394e+00]),\n",
       " 'split4_test_score': array([ -0.28716297,  -0.2873504 ,  -0.28698701,  -0.28729988,\n",
       "         -0.28771957,  -0.28626087,  -0.28738432,  -0.28771426,\n",
       "         -0.2856544 ,  -0.28928047,  -0.29010242,  -0.28318562,\n",
       "         -0.28715977,  -0.28733531,  -0.28697728,  -0.28749525,\n",
       "         -0.28798174,  -0.28623682,  -0.28769568,  -0.28847922,\n",
       "         -0.28526686,  -0.28948853,  -0.29165338,  -0.27977693,\n",
       "         -0.28709199,  -0.28736963,  -0.2870014 ,  -0.28746096,\n",
       "         -0.28771274,  -0.28629881,  -0.28771736,  -0.2882263 ,\n",
       "         -0.28552612,  -0.28932483,  -0.29115189,  -0.28134361,\n",
       "         -0.40957565,  -0.40966978,  -0.4097195 ,  -0.40964317,\n",
       "         -0.4105174 ,  -0.4106576 ,  -0.40958402,  -0.41086646,\n",
       "         -0.41182933,  -0.40854927,  -0.41421489,  -0.41973622,\n",
       "         -0.40957911,  -0.40960943,  -0.4088884 ,  -0.40934734,\n",
       "         -0.40985902,  -0.40651826,  -0.40917225,  -0.40956227,\n",
       "         -0.40359089,  -0.40665086,  -0.40812407,  -0.3804253 ,\n",
       "         -0.40957911,  -0.40960943,  -0.4088884 ,  -0.40934734,\n",
       "         -0.40985902,  -0.40651826,  -0.40917225,  -0.40956227,\n",
       "         -0.40359089,  -0.40661344,  -0.40809488,  -0.3804253 ,\n",
       "         -0.40937927,  -0.40955145,  -0.40921494,  -0.40905788,\n",
       "         -0.40954297,  -0.40805287,  -0.40857781,  -0.40897231,\n",
       "         -0.40696224,  -0.40620153,  -0.40702487,  -0.40051089,\n",
       "         -0.40942199,  -0.40955004,  -0.4092041 ,  -0.4091807 ,\n",
       "         -0.40983809,  -0.40795116,  -0.40896128,  -0.40934045,\n",
       "         -0.40648118,  -0.40606528,  -0.40734806,  -0.39606816,\n",
       "         -0.40943086,  -0.409562  ,  -0.40922465,  -0.40924302,\n",
       "         -0.4099433 ,  -0.40819673,  -0.40914662,  -0.4094367 ,\n",
       "         -0.40698217,  -0.40592714,  -0.40681651,  -0.39854604,\n",
       "         -3.24954778,  -3.24950214,  -3.24987888,  -3.24775909,\n",
       "         -3.24844368,  -3.24919987,  -3.245814  ,  -3.24726458,\n",
       "         -3.24835868,  -3.23008605,  -3.23246207,  -3.24350606,\n",
       "         -3.2492754 ,  -3.24909027,  -3.24908024,  -3.24673168,\n",
       "         -3.24694619,  -3.24564582,  -3.24380112,  -3.24346529,\n",
       "         -3.24126494,  -3.21985031,  -3.21326649,  -3.2081967 ,\n",
       "         -3.2492754 ,  -3.24904951,  -3.24908024,  -3.24673169,\n",
       "         -3.24690543,  -3.24564582,  -3.24380228,  -3.24341688,\n",
       "         -3.24126494,  -3.21985191,  -3.21312113,  -3.2081967 ,\n",
       "         -0.28671046,  -0.28899644,  -0.28572069,  -0.29028879,\n",
       "         -0.29409105,  -0.2832505 ,  -0.29526236,  -0.29886946,\n",
       "         -0.28307822,  -0.37333713,  -0.39308744,  -0.34136915,\n",
       "         -0.28661863,  -0.28918861,  -0.28531321,  -0.29249102,\n",
       "         -0.29645167,  -0.28005053,  -0.29920198,  -0.30432005,\n",
       "         -0.27669189,  -0.38149883,  -0.41406327,  -0.35042696,\n",
       "         -0.2867117 ,  -0.28920434,  -0.28552347,  -0.29086329,\n",
       "         -0.29472452,  -0.2806225 ,  -0.29609002,  -0.30304943,\n",
       "         -0.27910404,  -0.37605162,  -0.40943779,  -0.36935581,\n",
       "         -0.41020175,  -0.41113848,  -0.41164145,  -0.41001565,\n",
       "         -0.4192303 ,  -0.41955804,  -0.40986633,  -0.42289925,\n",
       "         -0.43057851,  -0.43706875,  -0.48065802,  -0.54704557,\n",
       "         -0.41023838,  -0.41053741,  -0.40339934,  -0.40908818,\n",
       "         -0.41150723,  -0.38015445,  -0.40757767,  -0.4054698 ,\n",
       "         -0.3553242 ,  -0.43189426,  -0.43828408,  -0.31183044,\n",
       "         -0.41023838,  -0.41053741,  -0.40339934,  -0.40908818,\n",
       "         -0.41150723,  -0.38015445,  -0.40757767,  -0.40559426,\n",
       "         -0.3553242 ,  -0.43227348,  -0.44397467,  -0.3125228 ,\n",
       "         -0.40825763,  -0.40997296,  -0.4069975 ,  -0.40736224,\n",
       "         -0.41256175,  -0.40054453,  -0.40660603,  -0.41116667,\n",
       "         -0.39552823,  -0.4471924 ,  -0.46707676,  -0.44649716,\n",
       "         -0.40908953,  -0.40978076,  -0.40641318,  -0.40840382,\n",
       "         -0.41369006,  -0.39607498,  -0.40869736,  -0.41276464,\n",
       "         -0.3867195 ,  -0.44897419,  -0.47654642,  -0.41699498,\n",
       "         -0.40867707,  -0.41004959,  -0.40673945,  -0.40985033,\n",
       "         -0.41430783,  -0.39853696,  -0.41024763,  -0.41475465,\n",
       "         -0.39170787,  -0.45229534,  -0.48210443,  -0.44517212,\n",
       "         -3.24504941,  -3.24459998,  -3.24835793,  -3.2276051 ,\n",
       "         -3.23416059,  -3.24347588,  -3.2091743 ,  -3.21856617,\n",
       "         -3.25990896,  -3.0536089 ,  -3.08668243,  -3.37006403,\n",
       "         -3.24233475,  -3.24049569,  -3.24041357,  -3.21752259,\n",
       "         -3.21962344,  -3.20756914,  -3.18978488,  -3.18644509,\n",
       "         -3.19253334,  -2.94178225,  -2.90723205,  -2.88620898,\n",
       "         -3.24233476,  -3.24008869,  -3.24041357,  -3.21752404,\n",
       "         -3.21921728,  -3.20756914,  -3.18980046,  -3.18597207,\n",
       "         -3.19253334,  -2.94185874,  -2.90330433,  -2.88620898,\n",
       "         -0.28932661,  -0.31664054,  -0.28521841,  -0.38895948,\n",
       "         -0.44802893,  -0.34405132,  -0.5200726 ,  -0.61984391,\n",
       "         -0.53549016,  -1.04749386,  -1.19299182,  -1.18675645,\n",
       "         -0.2980352 ,  -0.30972835,  -0.27706573,  -0.4117902 ,\n",
       "         -0.46392514,  -0.34993661,  -0.59001145,  -0.64422313,\n",
       "         -0.53642676,  -1.17966842,  -1.14922393,  -1.021193  ,\n",
       "         -0.28926169,  -0.31482877,  -0.27926315,  -0.39009158,\n",
       "         -0.45530111,  -0.37062042,  -0.56795785,  -0.62648676,\n",
       "         -0.56563822,  -1.1255524 ,  -1.18168387,  -1.02910855,\n",
       "         -0.41941996,  -0.41672873,  -0.43038543,  -0.44333539,\n",
       "         -0.48867375,  -0.54606183,  -0.5301052 ,  -0.59538946,\n",
       "         -0.66060632,  -0.75739525,  -0.75539137,  -0.77561734,\n",
       "         -0.41169986,  -0.40217675,  -0.35511922,  -0.45489787,\n",
       "         -0.43863665,  -0.31618403,  -0.5555413 ,  -0.56861327,\n",
       "         -0.39878282,  -0.95449193,  -0.8979749 ,  -0.58301366,\n",
       "         -0.41169986,  -0.40217675,  -0.35511922,  -0.45980418,\n",
       "         -0.43803163,  -0.31627862,  -0.56698478,  -0.55940516,\n",
       "         -0.39065074,  -0.98538551,  -0.94317359,  -0.71478306,\n",
       "         -0.40179959,  -0.42604077,  -0.39750043,  -0.47769767,\n",
       "         -0.51458685,  -0.44625591,  -0.59662609,  -0.65693712,\n",
       "         -0.62998251,  -1.08720799,  -1.18191715,  -1.04858539,\n",
       "         -0.41962942,  -0.4205738 ,  -0.38650783,  -0.48849401,\n",
       "         -0.54876568,  -0.41797534,  -0.64158497,  -0.69887249,\n",
       "         -0.55007568,  -1.18912836,  -1.16883428,  -0.97493446,\n",
       "         -0.4198609 ,  -0.42463186,  -0.38983188,  -0.4937561 ,\n",
       "         -0.5403931 ,  -0.44381334,  -0.64037223,  -0.68448284,\n",
       "         -0.60159463,  -1.21825706,  -1.16614426,  -0.9986911 ,\n",
       "         -3.20060507,  -3.19786633,  -3.26068213,  -3.06260633,\n",
       "         -3.05651999,  -3.37093939,  -2.8316611 ,  -2.79650437,\n",
       "         -3.44983561,  -1.97554081,  -2.1620806 ,  -2.43864857,\n",
       "         -3.16463285,  -3.15820804,  -3.1837454 ,  -2.93579297,\n",
       "         -2.9496435 ,  -2.87910756,  -2.57283676,  -2.6289583 ,\n",
       "         -2.43394205,  -1.54564878,  -1.54207189,  -1.91213867,\n",
       "         -3.16463289,  -3.15420152,  -3.1837454 ,  -2.93602111,\n",
       "         -2.94588175,  -2.87910756,  -2.56842807,  -2.62592258,\n",
       "         -2.43459551,  -1.48230814,  -1.54883142,  -1.81671741,\n",
       "         -0.64033805,  -0.65585501,  -0.5314315 ,  -1.14063062,\n",
       "         -1.21445023,  -1.23438574,  -1.30604087,  -1.27796449,\n",
       "         -1.33690135,  -1.44010051,  -1.38813448,  -1.4285583 ,\n",
       "         -0.63067676,  -0.66739949,  -0.53802657,  -1.13674276,\n",
       "         -1.20618583,  -0.99919504,  -1.18141367,  -1.22302942,\n",
       "         -1.00815758,  -1.26081027,  -1.26254412,  -1.00829238,\n",
       "         -0.67830959,  -0.74860442,  -0.59248389,  -1.13469404,\n",
       "         -1.33424139,  -1.03861888,  -1.12675766,  -1.37127153,\n",
       "         -1.04484268,  -1.12291581,  -1.37764109,  -1.04486836,\n",
       "         -0.69134315,  -0.60323862,  -0.66258839,  -0.90214695,\n",
       "         -0.96344332,  -0.62249622,  -1.04936487,  -1.07507879,\n",
       "         -0.7041552 ,  -1.31305724,  -1.13196803,  -0.91423124,\n",
       "         -0.61575846,  -0.52091816,  -0.3975327 ,  -1.02271595,\n",
       "         -0.86843038,  -0.52490616,  -1.22237219,  -1.02099783,\n",
       "         -0.69951593,  -1.2474248 ,  -1.0516201 ,  -0.72949561,\n",
       "         -0.61164665,  -0.51687439,  -0.41506093,  -1.06780212,\n",
       "         -0.95823711,  -0.6767799 ,  -1.19854952,  -1.07348738,\n",
       "         -0.70595714,  -1.24315125,  -1.11892967,  -0.73118267,\n",
       "         -0.71425039,  -0.66761345,  -0.62794911,  -0.97865952,\n",
       "         -1.08142653,  -1.13736501,  -1.05392067,  -1.05216366,\n",
       "         -1.2087548 ,  -1.09235875,  -1.1018861 ,  -1.28136488,\n",
       "         -0.66570353,  -0.65127143,  -0.56224815,  -1.12842693,\n",
       "         -1.16370456,  -1.00134128,  -1.15403825,  -1.20354162,\n",
       "         -1.00894131,  -1.17018375,  -1.19760003,  -1.00880247,\n",
       "         -0.80041188,  -0.76219077,  -0.62967575,  -1.13450754,\n",
       "         -1.27631972,  -1.00112543,  -1.14702852,  -1.28124386,\n",
       "         -0.99974644,  -1.15857965,  -1.31335778,  -0.99968232,\n",
       "         -2.793133  ,  -2.66441903,  -3.29216594,  -2.06690826,\n",
       "         -2.20413299,  -2.38310864,  -1.87238768,  -1.87683609,\n",
       "         -2.28151281,  -1.54155957,  -1.67489238,  -2.04588942,\n",
       "         -2.51138817,  -2.40031049,  -2.45475318,  -1.74140336,\n",
       "         -1.55242381,  -1.83932139,  -1.51638755,  -1.3256549 ,\n",
       "         -1.73313916,  -1.29100419,  -1.26665846,  -1.27613183,\n",
       "         -2.51136172,  -2.37598096,  -2.45475318,  -1.65873713,\n",
       "         -1.41344961,  -1.8378363 ,  -1.37180103,  -1.28745029,\n",
       "         -1.64935007,  -1.13902925,  -1.17939167,  -1.23600035,\n",
       "         -2.33942979,  -1.83656218,  -1.25807929,  -2.3372757 ,\n",
       "         -2.35750723,  -1.32917141,  -2.10383152,  -1.9910316 ,\n",
       "         -1.32961991,  -4.32666938,  -1.84759304,  -1.32962003,\n",
       "         -2.20272858,  -1.18918559,  -0.8765679 ,  -2.30553895,\n",
       "         -1.41400504,  -0.8765679 ,  -2.37351725,  -1.46821765,\n",
       "         -0.8765679 ,  -6.98496487,  -2.11392343,  -0.8765679 ,\n",
       "         -2.673736  ,  -2.04116705,  -1.11119337,  -3.95364028,\n",
       "         -2.21300779,  -1.11119337,  -3.78939615,  -2.16256244,\n",
       "         -1.11119337,  -7.41472135,  -2.62687347,  -1.11119337,\n",
       "         -1.24636565,  -1.48721323,  -1.11624033,  -2.13409695,\n",
       "         -1.53913493,  -1.16532803,  -2.98143145,  -1.53045092,\n",
       "         -1.16532803,  -2.62736941,  -1.48867151,  -1.16532803,\n",
       "         -2.78436835,  -3.80554298,  -1.54145929,  -4.73217594,\n",
       "         -3.95376531,  -1.54145929,  -4.94967006,  -3.95376531,\n",
       "         -1.54145929,  -5.91002367,  -3.95376531,  -1.54145929,\n",
       "         -2.30854627,  -2.53529928,  -1.22250435,  -2.79214302,\n",
       "         -3.61266046,  -1.22250435,  -3.44437425,  -3.61266046,\n",
       "         -1.22250435,  -3.86587154,  -3.61266046,  -1.22250435,\n",
       "         -1.50352998,  -1.04019694,  -0.8625676 ,  -1.67003936,\n",
       "         -0.95715452,  -1.09678356,  -1.8160376 ,  -1.1257315 ,\n",
       "         -1.09677357,  -1.74893907,  -1.29267668,  -1.09677355,\n",
       "        -11.34877492,  -0.62080112,  -0.84150883,  -2.93035274,\n",
       "         -0.62080112,  -0.84150883,  -2.93035274,  -0.62080112,\n",
       "         -0.84150883,  -2.93035274,  -0.62080112,  -0.84150883,\n",
       "         -1.89200881,  -0.59565197,  -0.96785526,  -2.12380284,\n",
       "         -0.59565197,  -0.96785526,  -2.12380284,  -0.59565197,\n",
       "         -0.96785526,  -2.12380284,  -0.59565197,  -0.96785526,\n",
       "         -2.50443747,  -2.28635984,  -2.12668104,  -2.65674191,\n",
       "         -2.77177402,  -2.12668104,  -2.65674191,  -2.77177402,\n",
       "         -2.12668104,  -2.46795218,  -2.77177402,  -2.12668104,\n",
       "         -2.44948433,  -2.00869317,  -2.06555648,  -2.44948433,\n",
       "         -2.00869317,  -2.06555648,  -2.44948433,  -2.00869317,\n",
       "         -2.06555648,  -2.31253083,  -2.00869317,  -2.06555648,\n",
       "         -2.44948433,  -1.81928132,  -2.06555648,  -2.44948433,\n",
       "         -1.81928132,  -2.06555648,  -2.44948433,  -1.81928132,\n",
       "         -2.06555648,  -2.44948433,  -1.81928132,  -2.06555648]),\n",
       " 'mean_test_score': array([-2.02515668e-01, -2.02508709e-01, -2.02480983e-01, -1.99340576e-01,\n",
       "        -1.99393221e-01, -1.99037419e-01, -1.95400767e-01, -1.95425901e-01,\n",
       "        -1.94844772e-01, -1.65468224e-01, -1.65449054e-01, -1.63798221e-01,\n",
       "        -2.02409744e-01, -2.02428022e-01, -2.02735970e-01, -1.98844808e-01,\n",
       "        -1.99074572e-01, -2.00207536e-01, -1.94536908e-01, -1.94991968e-01,\n",
       "        -1.97111837e-01, -1.62274748e-01, -1.63592801e-01, -1.75578880e-01,\n",
       "        -2.02380940e-01, -2.02458429e-01, -2.02751371e-01, -1.98838858e-01,\n",
       "        -1.99177996e-01, -2.00396193e-01, -1.94612514e-01, -1.95108561e-01,\n",
       "        -1.97529819e-01, -1.62140929e-01, -1.63896875e-01, -1.77245600e-01,\n",
       "        -2.05565347e-01, -2.05510527e-01, -2.05477309e-01, -2.03088734e-01,\n",
       "        -2.02851815e-01, -2.02476727e-01, -1.99676883e-01, -1.99383505e-01,\n",
       "        -1.98762635e-01, -1.72017999e-01, -1.70910347e-01, -1.70160219e-01,\n",
       "        -2.05542200e-01, -2.05516169e-01, -2.05362069e-01, -2.02911163e-01,\n",
       "        -2.02833714e-01, -2.01899881e-01, -1.99464846e-01, -1.99353568e-01,\n",
       "        -1.97529460e-01, -1.71713178e-01, -1.71398131e-01, -1.68525605e-01,\n",
       "        -2.05544118e-01, -2.05542819e-01, -2.05529906e-01, -2.02921865e-01,\n",
       "        -2.02856759e-01, -2.02733330e-01, -1.99423622e-01, -1.99327247e-01,\n",
       "        -1.99150713e-01, -1.71704615e-01, -1.71507163e-01, -1.73330424e-01,\n",
       "        -2.05357047e-01, -2.05375159e-01, -2.05354710e-01, -2.02171321e-01,\n",
       "        -2.02282017e-01, -2.01735481e-01, -1.98117992e-01, -1.98301050e-01,\n",
       "        -1.97302890e-01, -1.67933913e-01, -1.68151080e-01, -1.64064304e-01,\n",
       "        -2.05251483e-01, -2.05331223e-01, -2.05539229e-01, -2.01772735e-01,\n",
       "        -2.02040760e-01, -2.02826392e-01, -1.97548509e-01, -1.97864699e-01,\n",
       "        -1.99475510e-01, -1.64957293e-01, -1.66382373e-01, -1.75756648e-01,\n",
       "        -2.05267471e-01, -2.05333261e-01, -2.05555386e-01, -2.01690489e-01,\n",
       "        -2.02079441e-01, -2.02946252e-01, -1.97450332e-01, -1.97833145e-01,\n",
       "        -1.99790538e-01, -1.64949859e-01, -1.66194786e-01, -1.77162044e-01,\n",
       "        -1.94243909e+00, -1.94235991e+00, -1.94294309e+00, -1.93987195e+00,\n",
       "        -1.93998356e+00, -1.94150735e+00, -1.93661547e+00, -1.93665819e+00,\n",
       "        -1.93973391e+00, -1.91016297e+00, -1.91080849e+00, -1.92606808e+00,\n",
       "        -1.94236031e+00, -1.94220176e+00, -1.94272927e+00, -1.93929354e+00,\n",
       "        -1.93920255e+00, -1.94053407e+00, -1.93525150e+00, -1.93494630e+00,\n",
       "        -1.93778588e+00, -1.90398386e+00, -1.90145997e+00, -1.91647859e+00,\n",
       "        -1.94236031e+00, -1.94219361e+00, -1.94272927e+00, -1.93929355e+00,\n",
       "        -1.93919264e+00, -1.94053407e+00, -1.93525183e+00, -1.93493138e+00,\n",
       "        -1.93778588e+00, -1.90393679e+00, -1.90139837e+00, -1.91647859e+00,\n",
       "        -1.94962454e-01, -1.94946319e-01, -1.94833269e-01, -1.65003310e-01,\n",
       "        -1.66007289e-01, -1.63741779e-01, -1.31660589e-01, -1.32289497e-01,\n",
       "        -1.29609682e-01,  3.71677366e-02,  3.39262540e-02,  1.98642867e-02,\n",
       "        -1.93906497e-01, -1.94279464e-01, -1.96987455e-01, -1.60977418e-01,\n",
       "        -1.63291326e-01, -1.75307606e-01, -1.25932554e-01, -1.28915391e-01,\n",
       "        -1.54554859e-01,  4.43771352e-02,  1.68764295e-02, -1.59316149e-01,\n",
       "        -1.93906044e-01, -1.94713419e-01, -1.97381621e-01, -1.60056035e-01,\n",
       "        -1.64096870e-01, -1.77351997e-01, -1.24349953e-01, -1.29968990e-01,\n",
       "        -1.58676308e-01,  4.72974527e-02,  1.38435148e-02, -1.73201819e-01,\n",
       "        -1.99601173e-01, -1.99054696e-01, -1.98736436e-01, -1.75233146e-01,\n",
       "        -1.73709682e-01, -1.70099402e-01, -1.43888896e-01, -1.41606450e-01,\n",
       "        -1.39371810e-01,  1.77951042e-02,  1.49045666e-02,  1.72399127e-02,\n",
       "        -1.99372826e-01, -1.99117544e-01, -1.97459429e-01, -1.73810022e-01,\n",
       "        -1.73159353e-01, -1.68328550e-01, -1.40911602e-01, -1.41149047e-01,\n",
       "        -1.39922213e-01,  2.12317722e-02,  9.68519063e-03, -1.83742256e-02,\n",
       "        -1.99392579e-01, -1.99384385e-01, -1.99142144e-01, -1.73905220e-01,\n",
       "        -1.73372749e-01, -1.72885569e-01, -1.40872075e-01, -1.41076455e-01,\n",
       "        -1.46107493e-01,  2.23587251e-02,  7.25416473e-03, -3.37210413e-02,\n",
       "        -1.97525696e-01, -1.97610619e-01, -1.97359758e-01, -1.67547856e-01,\n",
       "        -1.68939767e-01, -1.64075709e-01, -1.33066280e-01, -1.34729206e-01,\n",
       "        -1.28390104e-01,  3.76468637e-02,  3.04042559e-02,  6.00936462e-03,\n",
       "        -1.96924037e-01, -1.97353748e-01, -1.99477353e-01, -1.64248227e-01,\n",
       "        -1.66645000e-01, -1.75836044e-01, -1.28257521e-01, -1.31383837e-01,\n",
       "        -1.52859300e-01,  4.57578051e-02,  1.79295813e-02, -1.45600738e-01,\n",
       "        -1.96400653e-01, -1.97165517e-01, -1.99851569e-01, -1.63367377e-01,\n",
       "        -1.67499974e-01, -1.77274421e-01, -1.27350908e-01, -1.32854200e-01,\n",
       "        -1.55385317e-01,  4.64734317e-02,  1.40470909e-02, -1.21251851e-01,\n",
       "        -1.93471182e+00, -1.93391996e+00, -1.93974281e+00, -1.90973806e+00,\n",
       "        -1.91076948e+00, -1.92599430e+00, -1.87897770e+00, -1.87962485e+00,\n",
       "        -1.91411733e+00, -1.56335265e+00, -1.56853040e+00, -1.65335312e+00,\n",
       "        -1.93392982e+00, -1.93235201e+00, -1.93761822e+00, -1.90417243e+00,\n",
       "        -1.90336685e+00, -1.91633990e+00, -1.86621897e+00, -1.86541384e+00,\n",
       "        -1.89414793e+00, -1.49757204e+00, -1.47507479e+00, -1.50953582e+00,\n",
       "        -1.93392983e+00, -1.93227061e+00, -1.93761822e+00, -1.90417405e+00,\n",
       "        -1.90326916e+00, -1.91633990e+00, -1.86623088e+00, -1.86529312e+00,\n",
       "        -1.89414793e+00, -1.49649366e+00, -1.47465115e+00, -1.51226127e+00,\n",
       "        -1.31408438e-01, -1.30365927e-01, -1.29468011e-01,  3.45444935e-02,\n",
       "         3.14694782e-02,  1.90083465e-02,  1.02016639e-01,  9.35602599e-02,\n",
       "         6.11128838e-02,  3.58027013e-02,  6.95287620e-03, -5.24458544e-02,\n",
       "        -1.21962709e-01, -1.26177196e-01, -1.53662997e-01,  5.05803989e-02,\n",
       "         1.27931496e-02, -1.56982811e-01,  1.01178504e-01,  4.37478429e-02,\n",
       "        -3.32927374e-01, -8.56546914e-02, -1.38112887e-01, -8.01083514e-01,\n",
       "        -1.19436421e-01, -1.26501030e-01, -1.56798933e-01,  5.62100421e-02,\n",
       "         1.56955510e-02, -1.77129928e-01,  1.10903372e-01,  5.05595521e-02,\n",
       "        -3.69003747e-01, -6.68740417e-02, -1.40012305e-01, -9.32747645e-01,\n",
       "        -1.43185234e-01, -1.41394005e-01, -1.39427214e-01,  8.66223958e-03,\n",
       "         7.09813238e-03,  1.00367458e-02,  6.00670188e-02,  6.60357673e-02,\n",
       "         6.39114001e-02,  1.65103694e-02,  2.35607256e-02, -4.46302250e-02,\n",
       "        -1.39071505e-01, -1.34120252e-01, -1.40071915e-01,  1.46048923e-02,\n",
       "         1.56314311e-02, -2.66751497e-02,  8.20167883e-02,  5.36952164e-02,\n",
       "         1.02185049e-02, -3.17290748e-02, -8.41117853e-02, -7.18746625e-02,\n",
       "        -1.39310430e-01, -1.34547386e-01, -1.48018126e-01,  1.17810344e-02,\n",
       "         1.79138019e-02, -2.49324312e-02,  7.53205661e-02,  7.55069711e-02,\n",
       "         1.27969167e-02, -3.80454634e-02, -5.42542697e-02, -1.07306030e-01,\n",
       "        -1.27653298e-01, -1.29349733e-01, -1.28967691e-01,  2.78745915e-02,\n",
       "         3.02149329e-02,  5.00059111e-03,  9.55548596e-02,  9.41529023e-02,\n",
       "         4.54503432e-02,  1.87064352e-02,  1.15958219e-02, -2.12100440e-02,\n",
       "        -1.23624078e-01, -1.25490962e-01, -1.53092333e-01,  4.17296816e-02,\n",
       "         8.90133656e-03, -1.44761798e-01,  9.12277163e-02,  3.26504508e-02,\n",
       "        -3.14139475e-01, -8.93774041e-02, -1.63712915e-01, -7.48843790e-01,\n",
       "        -1.22995321e-01, -1.25790432e-01, -1.55490936e-01,  4.26651108e-02,\n",
       "         9.22036132e-03, -1.08209672e-01,  9.54768987e-02,  2.19997535e-02,\n",
       "        -2.18959424e-01, -9.15802301e-02, -1.71902205e-01, -7.21162596e-01,\n",
       "        -1.85611078e+00, -1.85316205e+00, -1.91552421e+00, -1.55909596e+00,\n",
       "        -1.55641807e+00, -1.66069304e+00, -1.24159032e+00, -1.20919351e+00,\n",
       "        -1.38914095e+00, -3.85284672e-01, -4.21455432e-01, -5.38016220e-01,\n",
       "        -1.84584475e+00, -1.83824712e+00, -1.89383783e+00, -1.50164726e+00,\n",
       "        -1.46682407e+00, -1.51525749e+00, -1.16815394e+00, -1.12003366e+00,\n",
       "        -1.12942673e+00, -2.46798274e-01, -2.42698313e-01, -3.74581944e-01,\n",
       "        -1.84584476e+00, -1.83744581e+00, -1.89383783e+00, -1.50172830e+00,\n",
       "        -1.46700551e+00, -1.51846045e+00, -1.16757079e+00, -1.12249885e+00,\n",
       "        -1.12835565e+00, -2.34543312e-01, -2.46308946e-01, -3.36050667e-01,\n",
       "         8.61583806e-02,  9.64932653e-02,  7.87882488e-02,  2.85905392e-02,\n",
       "        -2.86050411e-02, -6.81087924e-02, -2.82812775e-02, -9.60756814e-02,\n",
       "        -1.53312839e-01, -1.31498834e-01, -2.01763728e-01, -2.29654957e-01,\n",
       "         8.35878976e-02,  5.45805270e-02, -3.47561367e-01, -9.76780148e-02,\n",
       "        -1.80457282e-01, -7.97289119e-01, -1.39596807e-01, -2.01494226e-01,\n",
       "        -8.01469313e-01, -1.73827619e-01, -2.08591573e-01, -8.01603203e-01,\n",
       "         9.80271163e-02,  3.24134166e-02, -4.06237614e-01, -7.39162129e-02,\n",
       "        -2.02272075e-01, -9.69774892e-01, -1.03339198e-01, -2.13799837e-01,\n",
       "        -9.78259017e-01, -1.13336472e-01, -2.17859600e-01, -9.78283740e-01,\n",
       "         2.97441947e-02,  7.43931083e-02,  1.00098581e-01, -3.20078979e-02,\n",
       "        -1.23604819e-01,  2.55247873e-02, -2.32809592e-02, -1.17887180e-01,\n",
       "        -7.89705531e-03, -1.10206330e-01, -1.12348557e-01, -6.44591438e-02,\n",
       "         9.99258706e-02,  7.53245260e-02,  1.50913601e-02,  1.00062006e-02,\n",
       "        -8.14460068e-02, -6.47195717e-02, -7.70481606e-02, -1.48805776e-01,\n",
       "        -1.04311205e-01, -9.38892130e-02, -1.51785350e-01, -1.13093552e-01,\n",
       "         1.12287470e-01,  7.08566872e-02,  1.95522753e-02, -3.89967610e-02,\n",
       "        -1.00593256e-01, -1.12978183e-01, -9.92838206e-02, -1.39210477e-01,\n",
       "        -1.28431561e-01, -1.17551708e-01, -1.56117477e-01, -1.32907317e-01,\n",
       "         5.42337990e-02,  9.77014665e-02,  2.40550784e-02,  1.39166972e-02,\n",
       "         3.58717450e-02, -7.48697018e-02, -4.54665960e-02, -1.57090604e-02,\n",
       "        -1.42810293e-01, -1.19085210e-01, -9.74941511e-02, -2.15003496e-01,\n",
       "         6.29577709e-02,  5.13661060e-02, -3.43459494e-01, -9.21609678e-02,\n",
       "        -1.67888875e-01, -7.82621450e-01, -1.34353406e-01, -1.98449586e-01,\n",
       "        -7.85652570e-01, -1.46609577e-01, -2.00424489e-01, -7.86674596e-01,\n",
       "         2.21134015e-02,  6.05383693e-02, -3.06981044e-01, -1.41591975e-01,\n",
       "        -1.92140852e-01, -8.00835751e-01, -1.69671495e-01, -2.02661706e-01,\n",
       "        -8.09353735e-01, -1.84182640e-01, -2.07135716e-01, -8.09381673e-01,\n",
       "        -1.13617824e+00, -1.18018197e+00, -1.35514120e+00, -4.87965191e-01,\n",
       "        -5.81691489e-01, -4.93260977e-01, -3.78009335e-01, -3.96232191e-01,\n",
       "        -4.61429008e-01, -2.68326499e-01, -3.86517891e-01, -5.00340018e-01,\n",
       "        -1.01707835e+00, -1.04311825e+00, -1.09828147e+00, -3.17083756e-01,\n",
       "        -3.18760059e-01, -3.28064440e-01, -1.93170821e-01, -1.86256278e-01,\n",
       "        -2.48005941e-01, -1.04884813e-01, -7.89036391e-02, -1.81164444e-01,\n",
       "        -1.02988205e+00, -1.03826765e+00, -1.10177025e+00, -2.61862404e-01,\n",
       "        -2.65112088e-01, -3.62715451e-01, -1.28241298e-01, -2.00361580e-01,\n",
       "        -3.40218981e-01, -7.98209300e-02, -1.58612881e-01, -2.51755257e-01,\n",
       "        -7.60589709e-01, -4.05294847e-01, -4.52256369e-01, -1.54103369e+00,\n",
       "        -5.03132248e-01, -4.81438532e-01, -1.55723325e+00, -4.15575562e-01,\n",
       "        -4.81465774e-01, -2.88986986e+00, -5.03077762e-01, -4.81465781e-01,\n",
       "        -1.65617635e+00, -9.85449040e-01, -8.49995383e-01, -1.79485245e+00,\n",
       "        -1.01400275e+00, -8.49995383e-01, -1.98016063e+00, -1.22463725e+00,\n",
       "        -8.49995383e-01, -4.56134946e+00, -1.78972631e+00, -8.49995383e-01,\n",
       "        -1.45328386e+00, -1.01395835e+00, -1.07679828e+00, -1.98263162e+00,\n",
       "        -1.04832148e+00, -1.07679828e+00, -2.44981155e+00, -1.08055864e+00,\n",
       "        -1.07679828e+00, -3.93591094e+00, -1.37641769e+00, -1.07679828e+00,\n",
       "        -7.82144722e-01, -6.18900883e-01, -4.83494058e-01, -1.78894079e+00,\n",
       "        -7.97298149e-01, -5.71918172e-01, -2.57646635e+00, -8.21926690e-01,\n",
       "        -5.71918172e-01, -3.29767228e+00, -7.73692124e-01, -5.71918172e-01,\n",
       "        -2.09365616e+00, -1.27733001e+00, -7.42400453e-01, -3.33072462e+00,\n",
       "        -1.24884937e+00, -7.42400453e-01, -3.41185714e+00, -1.24884937e+00,\n",
       "        -7.42400453e-01, -3.94655276e+00, -1.24884937e+00, -7.42400453e-01,\n",
       "        -1.34763762e+00, -1.61169351e+00, -1.07059438e+00, -2.12221939e+00,\n",
       "        -1.83312762e+00, -1.07059438e+00, -2.71331584e+00, -1.83312762e+00,\n",
       "        -1.07059438e+00, -2.76444578e+00, -1.83312762e+00, -1.07059438e+00,\n",
       "        -9.16205544e-01, -3.73717891e-01, -4.20824298e-01, -1.25223607e+00,\n",
       "        -4.12706099e-01, -4.91181658e-01, -1.20778484e+00, -4.54838451e-01,\n",
       "        -4.91164281e-01, -1.40428201e+00, -5.58242738e-01, -4.91164277e-01,\n",
       "        -4.47517966e+00, -5.73619814e-01, -1.02853680e+00, -2.97642527e+00,\n",
       "        -5.87983532e-01, -1.02853680e+00, -2.82911768e+00, -5.87983532e-01,\n",
       "        -1.02853680e+00, -2.80158408e+00, -5.87983532e-01, -1.02853680e+00,\n",
       "        -1.91035869e+00, -4.26316032e-01, -9.99574032e-01, -1.01565739e+01,\n",
       "        -4.09739599e-01, -9.99574032e-01, -1.01607985e+01, -4.09739599e-01,\n",
       "        -9.99574032e-01, -1.01607985e+01, -4.09739599e-01, -9.99574032e-01,\n",
       "        -6.13421803e-01, -9.21310930e-01, -9.06218631e-01, -7.98350613e-01,\n",
       "        -9.37564290e-01, -9.06218631e-01, -7.97671542e-01, -8.79674428e-01,\n",
       "        -9.06218631e-01, -7.59281390e-01, -8.90073680e-01, -9.06218631e-01,\n",
       "        -1.92520709e+00, -1.69071874e+00, -1.30337750e+00, -1.90299161e+00,\n",
       "        -2.96366259e+00, -1.30337750e+00, -1.90299161e+00, -2.96366259e+00,\n",
       "        -1.30337750e+00, -1.87560091e+00, -2.96366259e+00, -1.30337750e+00,\n",
       "        -2.06307574e+00, -1.65283637e+00, -1.30337750e+00, -2.04086025e+00,\n",
       "        -2.92578022e+00, -1.30337750e+00, -2.04086025e+00, -2.92578022e+00,\n",
       "        -1.30337750e+00, -2.04086025e+00, -2.92578022e+00, -1.30337750e+00]),\n",
       " 'std_test_score': array([ 0.12241167,  0.12250343,  0.12247566,  0.12353992,  0.12365496,\n",
       "         0.12357681,  0.12510803,  0.1253546 ,  0.12503071,  0.13828034,\n",
       "         0.13833778,  0.13820809,  0.12242832,  0.12259581,  0.12279192,\n",
       "         0.12356631,  0.12408878,  0.12515297,  0.12534794,  0.12632518,\n",
       "         0.12819806,  0.14017673,  0.14308777,  0.15393359,  0.12239972,\n",
       "         0.12262881,  0.12281332,  0.1234783 ,  0.12415094,  0.12513652,\n",
       "         0.12525726,  0.12637064,  0.12813386,  0.14004721,  0.14290599,\n",
       "         0.15379804,  0.14390653,  0.14390161,  0.14381116,  0.14553137,\n",
       "         0.14576098,  0.14523274,  0.14742031,  0.14793843,  0.14702101,\n",
       "         0.16281689,  0.16488802,  0.16251278,  0.14391565,  0.14389353,\n",
       "         0.14357216,  0.14538993,  0.1457272 ,  0.14402327,  0.14736003,\n",
       "         0.14779387,  0.14450869,  0.16301086,  0.16450938,  0.15287619,\n",
       "         0.1439137 ,  0.1438975 ,  0.14363421,  0.14539379,  0.14575173,\n",
       "         0.14434502,  0.1473534 ,  0.14783988,  0.14517892,  0.16304356,\n",
       "         0.16460883,  0.15564186,  0.14374683,  0.14382753,  0.14376207,\n",
       "         0.1450306 ,  0.14513671,  0.14491341,  0.14676589,  0.14685785,\n",
       "         0.1464551 ,  0.16069677,  0.16067923,  0.15985181,  0.14383744,\n",
       "         0.14386435,  0.14386427,  0.1451937 ,  0.14549552,  0.14544825,\n",
       "         0.14709776,  0.14747979,  0.14755535,  0.16220497,  0.1635229 ,\n",
       "         0.1667132 ,  0.14381044,  0.14384157,  0.1438693 ,  0.14514412,\n",
       "         0.14551317,  0.14555407,  0.14706863,  0.14747412,  0.14769653,\n",
       "         0.16204652,  0.16337481,  0.1676096 ,  1.00536328,  1.00537148,\n",
       "         1.00546236,  1.00562555,  1.00568944,  1.0058507 ,  1.00587823,\n",
       "         1.00617051,  1.00633472,  1.00833932,  1.0084312 ,  1.01055413,\n",
       "         1.00531212,  1.00529742,  1.00529115,  1.00543484,  1.00547805,\n",
       "         1.00510216,  1.0055818 ,  1.00551241,  1.00484131,  1.00643834,\n",
       "         1.00519747,  1.00312477,  1.00531212,  1.00528682,  1.00529115,\n",
       "         1.00543486,  1.00546769,  1.00510216,  1.0055822 ,  1.00549663,\n",
       "         1.00484131,  1.00642883,  1.00513462,  1.00312477,  0.12426132,\n",
       "         0.12530171,  0.12502628,  0.13681454,  0.13787688,  0.13822954,\n",
       "         0.15577068,  0.15647154,  0.15579532,  0.28276571,  0.28546022,\n",
       "         0.27382994,  0.12393127,  0.12635443,  0.12803546,  0.13598271,\n",
       "         0.14177112,  0.15384857,  0.15628972,  0.16573374,  0.18829359,\n",
       "         0.29273043,  0.31338827,  0.44663561,  0.12450399,  0.12664327,\n",
       "         0.12819532,  0.13634215,  0.1429763 ,  0.15346356,  0.15637859,\n",
       "         0.16702913,  0.18678852,  0.29069437,  0.31329976,  0.44676457,\n",
       "         0.14787622,  0.14783071,  0.146951  ,  0.16380311,  0.16597576,\n",
       "         0.16242624,  0.18121715,  0.18647228,  0.18206886,  0.30204411,\n",
       "         0.31797298,  0.32227202,  0.14796397,  0.14775759,  0.14447578,\n",
       "         0.16303302,  0.16538699,  0.15299765,  0.18166709,  0.1847195 ,\n",
       "         0.16663021,  0.30607071,  0.31796843,  0.30866636,  0.14794488,\n",
       "         0.14780206,  0.14511642,  0.16305294,  0.16582722,  0.15551138,\n",
       "         0.18174959,  0.18499644,  0.17200777,  0.30579087,  0.32073398,\n",
       "         0.34155956,  0.14639118,  0.14713382,  0.14648395,  0.15985657,\n",
       "         0.16131039,  0.15988917,  0.17851104,  0.18032651,  0.17756785,\n",
       "         0.30191232,  0.30592042,  0.30008622,  0.14717659,  0.14761784,\n",
       "         0.14754818,  0.16179501,  0.16489042,  0.16674946,  0.18113081,\n",
       "         0.184964  ,  0.19485119,  0.30660896,  0.32144841,  0.44555317,\n",
       "         0.14686654,  0.14768674,  0.14756482,  0.16195651,  0.16488695,\n",
       "         0.16758189,  0.18196491,  0.18551984,  0.19644756,  0.3076133 ,\n",
       "         0.32242645,  0.39712841,  1.00534664,  1.00543671,  1.00633834,\n",
       "         1.00785523,  1.00832414,  1.01056516,  1.0100317 ,  1.01095329,\n",
       "         1.02114392,  1.05153194,  1.05725346,  1.0706142 ,  1.00483634,\n",
       "         1.00469563,  1.00463461,  1.00595514,  1.00621203,  1.00297387,\n",
       "         1.00718131,  1.00580504,  1.00891491,  1.02598315,  1.01823801,\n",
       "         0.96835085,  1.00483634,  1.00458965,  1.00463461,  1.00595683,\n",
       "         1.00610849,  1.00297387,  1.00719401,  1.00562468,  1.00891491,\n",
       "         1.02554098,  1.01656099,  0.97049149,  0.14727456,  0.16065997,\n",
       "         0.15653669,  0.2793484 ,  0.29806087,  0.27674271,  0.3839219 ,\n",
       "         0.41041872,  0.374544  ,  0.60296503,  0.63242481,  0.58317838,\n",
       "         0.15076695,  0.16427843,  0.18862048,  0.28493106,  0.32041237,\n",
       "         0.45095458,  0.40049877,  0.41957348,  0.69512626,  0.61512861,\n",
       "         0.57234942,  1.04664373,  0.14820991,  0.16750182,  0.18835013,\n",
       "         0.27894411,  0.31895006,  0.44706423,  0.39543999,  0.41576274,\n",
       "         0.68411261,  0.5906759 ,  0.58049194,  1.10751277,  0.18593245,\n",
       "         0.18406934,  0.18155044,  0.30200784,  0.31792922,  0.3244823 ,\n",
       "         0.42350901,  0.42272555,  0.43677549,  0.60176492,  0.58867484,\n",
       "         0.6220091 ,  0.18426681,  0.1794409 ,  0.16547423,  0.3052271 ,\n",
       "         0.30800103,  0.32437794,  0.40871241,  0.42061576,  0.43418457,\n",
       "         0.62095697,  0.60252645,  0.58362327,  0.184082  ,  0.17902646,\n",
       "         0.17119181,  0.30709026,  0.31076638,  0.32474615,  0.41600265,\n",
       "         0.41031501,  0.43007159,  0.62516897,  0.59611404,  0.5930735 ,\n",
       "         0.17502045,  0.18502528,  0.1781081 ,  0.30055756,  0.31902726,\n",
       "         0.29981029,  0.41150293,  0.42729867,  0.40808151,  0.6169938 ,\n",
       "         0.63081076,  0.54776965,  0.18260602,  0.18939448,  0.19387039,\n",
       "         0.30615382,  0.34389567,  0.44603773,  0.41519675,  0.43933934,\n",
       "         0.69641116,  0.6146784 ,  0.57970063,  1.01927915,  0.18396341,\n",
       "         0.18706538,  0.19578363,  0.30925729,  0.33411253,  0.37558645,\n",
       "         0.41875091,  0.42765996,  0.49480317,  0.62835012,  0.57924973,\n",
       "         0.84622319,  1.00580064,  1.00604342,  1.02035132,  1.06675846,\n",
       "         1.0644049 ,  1.07561292,  1.07723018,  1.05202487,  1.16492771,\n",
       "         0.86023597,  0.93610243,  1.02284664,  0.99759679,  0.99925679,\n",
       "         1.00547798,  1.02705038,  1.03764212,  0.96711469,  0.99294269,\n",
       "         1.00842499,  0.91695148,  0.69869709,  0.70656436,  0.84812528,\n",
       "         0.9975968 ,  0.99819904,  1.00547798,  1.02717813,  1.03592056,\n",
       "         0.96964852,  0.98943278,  1.01266519,  0.91629016,  0.68455434,\n",
       "         0.70587137,  0.80421684,  0.40800832,  0.42745977,  0.36529256,\n",
       "         0.62467104,  0.64158923,  0.59932963,  0.67930421,  0.64361597,\n",
       "         0.60874699,  0.70189652,  0.64370258,  0.62357435,  0.38710594,\n",
       "         0.4144735 ,  0.69113194,  0.56898118,  0.60361449,  1.02855195,\n",
       "         0.58013345,  0.60495832,  1.03106549,  0.60318878,  0.61693094,\n",
       "         1.03104934,  0.40569568,  0.48188471,  0.71082979,  0.57893692,\n",
       "         0.67795597,  1.1188233 ,  0.57627578,  0.68843951,  1.12313879,\n",
       "         0.57383861,  0.68947801,  1.12313708,  0.42506967,  0.4429451 ,\n",
       "         0.4140667 ,  0.64249741,  0.73362221,  0.48656764,  0.6672148 ,\n",
       "         0.74240666,  0.49254775,  0.71417174,  0.67767594,  0.55219139,\n",
       "         0.40705768,  0.45729852,  0.39529595,  0.6065684 ,  0.64386666,\n",
       "         0.49978583,  0.66033553,  0.69186069,  0.53516956,  0.66212791,\n",
       "         0.68062118,  0.53038276,  0.41501802,  0.43324843,  0.38747957,\n",
       "         0.65261553,  0.6148511 ,  0.54062615,  0.68667122,  0.65337738,\n",
       "         0.5422202 ,  0.69830226,  0.66918056,  0.54207414,  0.44966723,\n",
       "         0.44006794,  0.40506796,  0.57683315,  0.59758623,  0.57616152,\n",
       "         0.58407326,  0.56093124,  0.57265587,  0.57242272,  0.5471096 ,\n",
       "         0.57435352,  0.41435427,  0.45865419,  0.70784378,  0.61475119,\n",
       "         0.61881979,  1.02501379,  0.62073875,  0.62734201,  1.02724106,\n",
       "         0.62220142,  0.62353684,  1.02702977,  0.44808556,  0.46025338,\n",
       "         0.60819722,  0.60255976,  0.63366668,  0.96312349,  0.6059899 ,\n",
       "         0.63514071,  0.97166585,  0.6051999 ,  0.64780055,  0.97169684,\n",
       "         0.99356122,  1.05476047,  1.15207842,  0.86550773,  0.90362763,\n",
       "         1.02919212,  0.79557638,  0.80155326,  0.99517164,  0.72369594,\n",
       "         0.73082131,  0.85982674,  0.93997257,  0.97365178,  0.92195572,\n",
       "         0.77423749,  0.69259192,  0.80757123,  0.69226045,  0.62769674,\n",
       "         0.78023642,  0.60951314,  0.62511183,  0.60750835,  0.93499105,\n",
       "         0.96686888,  0.92504581,  0.74697176,  0.64805625,  0.81307966,\n",
       "         0.64592166,  0.59589042,  0.7152005 ,  0.57711504,  0.55225511,\n",
       "         0.65666367,  1.06126985,  0.85527905,  0.66881847,  1.68683025,\n",
       "         1.04818234,  0.70952338,  1.79684393,  0.94298842,  0.70958969,\n",
       "         2.55019693,  0.83251873,  0.70958972,  1.1992213 ,  1.26373122,\n",
       "         1.09765958,  1.1794608 ,  1.24695298,  1.09765958,  1.341505  ,\n",
       "         1.65459199,  1.09765958,  2.86209605,  2.08583989,  1.09765958,\n",
       "         1.10099984,  0.91372341,  1.10267572,  1.32535849,  0.93874349,\n",
       "         1.10267572,  1.47454664,  0.9827547 ,  1.10267572,  2.2461823 ,\n",
       "         1.09969444,  1.10267572,  0.78446177,  0.71613261,  0.73541525,\n",
       "         1.70438855,  0.56794841,  0.64907108,  2.57819322,  0.5574133 ,\n",
       "         0.64907108,  3.31214187,  0.5153931 ,  0.64907108,  1.45320254,\n",
       "         1.36476013,  0.62393199,  1.63554319,  1.44481047,  0.62393199,\n",
       "         1.8396358 ,  1.44481047,  0.62393199,  1.9012875 ,  1.44481047,\n",
       "         0.62393199,  1.2495401 ,  0.88088957,  1.13757547,  1.49519989,\n",
       "         1.22126756,  1.13757547,  1.89058846,  1.22126756,  1.13757547,\n",
       "         1.82111049,  1.22126756,  1.13757547,  0.82744799,  0.46470789,\n",
       "         0.47259896,  1.10858589,  0.44484352,  0.50144931,  1.07766093,\n",
       "         0.48468739,  0.5015936 ,  0.93915757,  0.5747051 ,  0.50159358,\n",
       "         3.94456026,  0.46328274,  1.28621091,  2.21690502,  0.44762974,\n",
       "         1.28621091,  2.33071286,  0.44762974,  1.28621091,  2.28168887,\n",
       "         0.44762974,  1.28621091,  1.4774283 ,  0.95406758,  1.12370198,\n",
       "        10.67102463,  0.95920015,  1.12370198, 10.67897091,  0.95920015,\n",
       "         1.12370198, 10.67897091,  0.95920015,  1.12370198,  0.98733052,\n",
       "         0.85701194,  0.92603067,  1.03376297,  1.01124721,  0.92603067,\n",
       "         1.03427161,  1.05607959,  0.92603067,  1.01324473,  1.06449668,\n",
       "         0.92603067,  0.56716844,  1.51752649,  0.81551872,  0.54549329,\n",
       "         3.80106252,  0.81551872,  0.54549329,  3.80106252,  0.81551872,\n",
       "         0.52021755,  3.80106252,  0.81551872,  0.63324136,  1.51146806,\n",
       "         0.81551872,  0.61887249,  3.81132124,  0.81551872,  0.61887249,\n",
       "         3.81132124,  0.81551872,  0.61887249,  3.81132124,  0.81551872]),\n",
       " 'rank_test_score': array([381, 380, 379, 344, 350, 336, 309, 310, 304, 249, 248, 239, 375,\n",
       "        376, 384, 335, 338, 359, 300, 307, 314, 233, 236, 283, 374, 377,\n",
       "        385, 334, 342, 361, 301, 308, 324, 232, 240, 288, 411, 403, 402,\n",
       "        393, 388, 378, 356, 347, 333, 271, 265, 264, 407, 404, 400, 390,\n",
       "        387, 368, 352, 345, 323, 269, 266, 260, 409, 408, 405, 391, 389,\n",
       "        383, 351, 343, 341, 268, 267, 275, 399, 401, 398, 371, 373, 365,\n",
       "        329, 330, 316, 257, 258, 241, 394, 396, 406, 367, 369, 386, 325,\n",
       "        328, 353, 246, 252, 284, 395, 397, 410, 364, 370, 392, 320, 327,\n",
       "        357, 245, 251, 287, 685, 682, 688, 675, 676, 679, 663, 664, 673,\n",
       "        640, 643, 652, 683, 681, 686, 671, 670, 677, 661, 660, 667, 636,\n",
       "        630, 648, 684, 680, 686, 672, 669, 677, 662, 659, 667, 635, 629,\n",
       "        648, 306, 305, 303, 247, 250, 238, 181, 182, 175,  46,  50,  66,\n",
       "        298, 299, 313, 231, 234, 282, 162, 171, 221,  41,  74, 229, 297,\n",
       "        302, 319, 230, 243, 290, 159, 176, 228,  37,  83, 274, 355, 337,\n",
       "        332, 281, 277, 263, 209, 206, 194,  72,  79,  73, 346, 339, 321,\n",
       "        278, 273, 259, 201, 203, 197,  65,  91, 102, 349, 348, 340, 280,\n",
       "        276, 272, 200, 202, 212,  62,  95, 111, 322, 326, 318, 255, 261,\n",
       "        242, 185, 189, 169,  45,  54,  98, 312, 317, 354, 244, 253, 285,\n",
       "        168, 178, 217,  39,  70, 211, 311, 315, 358, 235, 254, 289, 165,\n",
       "        183, 222,  38,  81, 154, 658, 655, 674, 639, 642, 651, 623, 624,\n",
       "        644, 598, 599, 602, 656, 654, 665, 637, 634, 646, 620, 619, 627,\n",
       "        587, 585, 590, 657, 653, 665, 638, 633, 646, 621, 618, 627, 586,\n",
       "        584, 591, 179, 177, 174,  49,  53,  68,   3,  13,  27,  48,  97,\n",
       "        116, 155, 163, 220,  35,  85, 226,   4,  42, 433, 130, 190, 501,\n",
       "        153, 164, 225,  30,  76, 286,   2,  36, 439, 120, 198, 519, 208,\n",
       "        204, 195,  94,  96,  89,  29,  24,  25,  75,  61, 114, 191, 186,\n",
       "        199,  80,  77, 106,  17,  33,  88, 109, 129, 122, 193, 188, 214,\n",
       "         86,  71, 105,  21,  19,  84, 112, 117, 143, 166, 173, 172,  58,\n",
       "         55,  99,  10,  12,  40,  69,  87, 103, 158, 160, 218,  44,  93,\n",
       "        210,  14,  51, 429, 131, 237, 488, 156, 161, 223,  43,  92, 144,\n",
       "         11,  64, 417, 132, 270, 483, 617, 616, 645, 597, 595, 604, 562,\n",
       "        560, 579, 443, 454, 471, 614, 613, 625, 588, 582, 592, 557, 551,\n",
       "        554, 422, 420, 441, 615, 612, 625, 589, 583, 593, 556, 552, 553,\n",
       "        419, 421, 434,  15,   9,  18,  57, 108, 121, 107, 135, 219, 180,\n",
       "        366, 418,  16,  31, 437, 137, 291, 496, 196, 363, 502, 279, 413,\n",
       "        503,   7,  52, 447, 123, 372, 521, 140, 414, 522, 149, 416, 523,\n",
       "         56,  22,   5, 110, 157,  59, 104, 151, 100, 145, 146, 118,   6,\n",
       "         20,  78,  90, 128, 119, 125, 215, 141, 134, 216, 148,   1,  23,\n",
       "         67, 113, 139, 147, 138, 192, 170, 150, 224, 184,  32,   8,  60,\n",
       "         82,  47, 124, 115, 101, 207, 152, 136, 415,  26,  34, 436, 133,\n",
       "        256, 493, 187, 331, 494, 213, 362, 495,  63,  28, 428, 205, 295,\n",
       "        500, 262, 382, 504, 293, 412, 505, 555, 558, 577, 463, 477, 467,\n",
       "        442, 445, 458, 427, 444, 468, 531, 538, 549, 430, 431, 432, 296,\n",
       "        294, 423, 142, 126, 292, 536, 537, 550, 425, 426, 438, 167, 360,\n",
       "        435, 127, 227, 424, 490, 446, 456, 594, 470, 459, 596, 452, 460,\n",
       "        703, 469, 461, 603, 524, 507, 608, 530, 507, 689, 561, 507, 717,\n",
       "        607, 507, 581, 529, 545, 690, 539, 545, 697, 548, 545, 714, 578,\n",
       "        544, 492, 482, 462, 606, 497, 473, 698, 506, 473, 711, 491, 473,\n",
       "        695, 567, 484, 712, 563, 484, 713, 563, 484, 715, 563, 484, 576,\n",
       "        600, 540, 696, 609, 540, 699, 609, 540, 700, 609, 540, 517, 440,\n",
       "        453, 566, 451, 466, 559, 457, 465, 580, 472, 464, 716, 476, 532,\n",
       "        710, 478, 532, 702, 478, 532, 701, 478, 532, 641, 455, 525, 718,\n",
       "        448, 525, 720, 448, 525, 719, 450, 525, 481, 518, 513, 499, 520,\n",
       "        513, 498, 511, 513, 489, 512, 513, 650, 605, 568, 631, 707, 568,\n",
       "        631, 707, 568, 622, 707, 568, 694, 601, 568, 691, 704, 568, 691,\n",
       "        704, 568, 691, 704, 568])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'loss': 'absolute_error',\n",
       " 'max_depth': 9,\n",
       " 'n_estimators': 10,\n",
       " 'random_state': 50,\n",
       " 'subsample': 0.5}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9700341932735869"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9700341932735869"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, reg.predict(X_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
